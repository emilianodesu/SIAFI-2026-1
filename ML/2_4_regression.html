<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2. Machine Learning ‚Äì SIAFI Data Science and Machine Learning Handbook 2026-1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-299fde5381b5a602aab895950093955a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">SIAFI Data Science and Machine Learning Handbook 2026-1</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-introduction-to-data-science" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Introduction to Data Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-introduction-to-data-science">    
        <li>
    <a class="dropdown-item" href="../IDS/1_1_fundamentals.html">
 <span class="dropdown-text">Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../IDS/1_3_end_to_end_ml_project.html">
 <span class="dropdown-text">End to End Machine Learning Project</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../IDS/1_4_dimensionality_reduction.html">
 <span class="dropdown-text">Dimensionality Reduction</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-machine-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Machine Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-machine-learning">    
        <li>
    <a class="dropdown-item" href="../ML/2_1_intro.html">
 <span class="dropdown-text">Introduction to Machine Learning</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/2_2_slf.html">
 <span class="dropdown-text">Supervised Learning Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/2_3_classification.html">
 <span class="dropdown-text">Classification Fundamentals</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/2_3_2_classification_algorithms.html">
 <span class="dropdown-text">Classification Algorithms</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../ML/2_4_regression.html">
 <span class="dropdown-text">Regression</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-deep-learning" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Deep Learning</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-deep-learning">    
        <li>
    <a class="dropdown-item" href="../DL/3_1_fundamentals.html">
 <span class="dropdown-text">Fundamentals</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#regression" id="toc-regression" class="nav-link active" data-scroll-target="#regression">2.3 Regression</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">2.3.1 Linear Regression</a></li>
  <li><a href="#loss" id="toc-loss" class="nav-link" data-scroll-target="#loss">2.3.2 Loss</a></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">2.3.3 Polynomial Regression</a></li>
  <li><a href="#regularized-linear-models" id="toc-regularized-linear-models" class="nav-link" data-scroll-target="#regularized-linear-models">2.3.4 Regularized Linear Models</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">2.3.5 Logistic Regression</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">2. Machine Learning</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="regression" class="level2">
<h2 class="anchored" data-anchor-id="regression">2.3 Regression</h2>
<section id="linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="linear-regression">2.3.1 Linear Regression</h3>
<p>Linear regression is a statistical technique used to find the relationship between variables. In an ML context, linear regression finds the relationship between features and a label.</p>
<p>For example, suppose we want to predict a car‚Äôs fuel efficiency in miles per gallon based on how heavy the car is, and we have the following dataset:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Pounds in 1000s (feature)</th>
<th>Miles per gallon (label)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3.5</td>
<td>18</td>
</tr>
<tr class="even">
<td>3.69</td>
<td>15</td>
</tr>
<tr class="odd">
<td>3.44</td>
<td>18</td>
</tr>
<tr class="even">
<td>3.43</td>
<td>16</td>
</tr>
<tr class="odd">
<td>4.34</td>
<td>15</td>
</tr>
<tr class="even">
<td>4.42</td>
<td>14</td>
</tr>
<tr class="odd">
<td>2.37</td>
<td>24</td>
</tr>
</tbody>
</table>
<p>If we plotted these points, we‚Äôd get the following graph:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/car-data-points.png" class="img-fluid figure-img"></p>
<figcaption>Car heaviness (in pounds) versus miles per gallon rating. As a car gets heavier, its miles per gallon rating generally decreases.</figcaption>
</figure>
</div>
<p>We could create our own model by drawing a best fit line through the points:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/car-data-points-with-model.png" class="img-fluid figure-img"></p>
<figcaption>A best fit line drawn through the data from the previous figure</figcaption>
</figure>
</div>
<p><strong>Linear regression equation</strong>:</p>
<p>In algebraic terms, the model would be defined as <span class="math inline">\(y = mx + b\)</span>, where</p>
<ul>
<li><span class="math inline">\(y\)</span> is miles per gallon‚Äîthe value we want to predict.</li>
<li><span class="math inline">\(m\)</span> is the slope of the line.</li>
<li><span class="math inline">\(x\)</span> is pounds‚Äîour input value.</li>
<li><span class="math inline">\(b\)</span> is the y-intercept. In ML, we write the equation for a linear regression model as follows:</li>
</ul>
<p><span class="math display">\[y' = b + w_1x_1\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(y'\)</span> is the predicted label‚Äîthe output.</li>
<li><span class="math inline">\(b\)</span> is the bias of the model. Bias is the same concept as the y-intercept in the algebraic equation for a line. In ML, bias is sometimes referred to as . Bias is a parameter of the model and is calculated during training.</li>
<li><span class="math inline">\(w_1\)</span> is the weight of the feature. Weight is the same concept as the slope in the algebraic equation for a line. Weight is a parameter of the model and is calculated during training.</li>
<li><span class="math inline">\(x_1\)</span> is a feature‚Äîthe input.</li>
</ul>
<p>During training, the model calculates the weight and bias that produce the best model.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/equation.png" class="img-fluid figure-img"></p>
<figcaption>Mathematical representation of a linear model.</figcaption>
</figure>
</div>
<p>In our example, we‚Äôd calculate the weight and bias from the line we drew. The bias is 34 (where the line intersects the y-axis), and the weight is ‚Äì4.6 (the slope of the line). The model would be defined as <span class="math inline">\(y' = 34 -4.6x_1\)</span>, and we could use it to make predictions. For instance, using this model, a 4,000-pound car would have a predicted fuel efficiency of 15.6 miles per gallon.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/model-prediction.png" class="img-fluid figure-img"></p>
<figcaption>Using the model, a 4,000-pound car has a predicted fuel efficiency of 15.6 miles per gallon.</figcaption>
</figure>
</div>
<p><strong>Models with multiple features</strong>:</p>
<p>Although the example in this section uses only one feature‚Äîthe heaviness of the car‚Äîa more sophisticated model might rely on multiple features, each having a separate weight (<span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, etc.). For example, a model that relies on five features would be written as follows:</p>
<p><span class="math display">\[y' = b + w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + w_5x_5\]</span></p>
<p>For example, a model that predicts gas mileage could additionally use features such as the following:</p>
<ul>
<li>Engine displacement</li>
<li>Acceleration</li>
<li>Number of cylinders</li>
<li>Horsepower</li>
</ul>
<p>This model would be written as follows:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/equation-multiple-features.png" class="img-fluid figure-img"></p>
<figcaption>A model with five features to predict a car‚Äôs miles per gallon rating.</figcaption>
</figure>
</div>
<p>By graphing a couple of these additional features, we can see that they also have a linear relationship to the label, miles per gallon:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/displacement.png" class="img-fluid figure-img"></p>
<figcaption>A car‚Äôs displacement in cubic centimeters and its miles per gallon rating. As a car‚Äôs engine gets bigger, its miles per gallon rating generally decreases.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/acceleration.png" class="img-fluid figure-img"></p>
<figcaption>A car‚Äôs acceleration in seconds and its miles per gallon rating. As a car‚Äôs acceleration time increases, its miles per gallon rating generally increases.</figcaption>
</figure>
</div>
<p><strong>Alternative notation</strong>:</p>
<p><span class="math display">\[ \hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n \]</span></p>
<p>In this equation:</p>
<ul>
<li><p><span class="math inline">\(\hat{y}\)</span> is the predicted value.</p></li>
<li><p><span class="math inline">\(n\)</span> is the number of features.</p></li>
<li><p><span class="math inline">\(x_i\)</span> is the <span class="math inline">\(i\)</span> th feature value.</p></li>
<li><p><span class="math inline">\(\theta_j\)</span> is the <span class="math inline">\(j\)</span> th model parameter, including the bias term <span class="math inline">\(\theta_0\)</span> and the feature weights <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(\theta_2\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(\theta_n\)</span>.</p></li>
</ul>
<p><strong>Linear regression model prediction (vectorized form)</strong>:</p>
<p><span class="math display">\[ \hat{y} = h_\theta(\mathbf{x}) = ùõâ \cdot \mathbf{x} \]</span></p>
<p>In this equation:</p>
<ul>
<li><span class="math inline">\(h_\theta\)</span> is the hypothesis function, using the model parameters <span class="math inline">\(ùõâ\)</span>.</li>
<li><span class="math inline">\(ùõâ\)</span> is the model‚Äôs parameter vector, containing the bias term <span class="math inline">\(\theta_0\)</span> and the feature weights <span class="math inline">\(\theta_1\)</span> to <span class="math inline">\(\theta_n\)</span>.</li>
<li><span class="math inline">\(\mathbf{x}\)</span> is the instance‚Äôs feature vector, containing <span class="math inline">\(x_0\)</span> to <span class="math inline">\(x_n\)</span>, with <span class="math inline">\(x_0\)</span> always equal to 1.</li>
<li><span class="math inline">\(ùõâ \cdot \mathbf{x}\)</span> is the dot product of the vectors <span class="math inline">\(\theta\)</span> and <span class="math inline">\(x\)</span>, which is equal to <span class="math inline">\(\theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n\)</span>.</li>
</ul>
<p><strong>Note</strong>: In machine learning, vectors are often represented as <em>column vectors</em>, which are 2D arrays with a single column. If ùõâ and <span class="math inline">\(\mathbf{x}\)</span> are column vectors, then the prediction is <span class="math inline">\(\hat{y}=ùõâ^T\mathbf{x}\)</span>, where <span class="math inline">\(ùõâ^T\)</span> is the transpose of ùõâ (a row vector instead of a column vector) and <span class="math inline">\(ùõâ^T\mathbf{x}\)</span> is the matrix multiplication of <span class="math inline">\(ùõâ^T\)</span> and <span class="math inline">\(\mathbf{x}\)</span>. It is of course the same prediction, except that it is now represented as a single-cell matrix rather than a scalar value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/lin_reg.png" class="img-fluid figure-img"></p>
<figcaption>Assumptions of Linear Regression</figcaption>
</figure>
</div>
<p>OK, that‚Äôs the linear regression model‚Äîbut how do we train it? Well, recall that training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data. We previously saw that the most common performance measure of a regression model is the root mean squared error. Therefore, a way to train a linear regression model is to find the value of ùõâ that minimizes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE) than the RMSE, and it leads to the same result (because the value that minimizes a positive function also minimizes its square root).</p>
<section id="the-normal-equation" class="level4">
<h4 class="anchored" data-anchor-id="the-normal-equation">2.3.1.1 The Normal Equation</h4>
<p>To find the value of ùõâ that minimizes the MSE, there exists a closed-form solution‚Äîin other words, a mathematical equation that gives the result directly. This is called the normal equation.</p>
<p><span class="math display">\[ \hat{\mathbf{\theta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \]</span></p>
<p>In this equation:</p>
<ul>
<li><span class="math inline">\(\hat{\mathbf{\theta}}\)</span> is the value of ùõâ that minimizes the cost function (mean squared error).</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the design matrix, which contains the feature vectors of all training instances as rows.</li>
<li><span class="math inline">\(\mathbf{y}\)</span> is the vector of target values containing <span class="math inline">\(y^{(1)}\)</span> to <span class="math inline">\(y^{(m)}\)</span> for all <span class="math inline">\(m\)</span> training instances.</li>
</ul>
<p>Let‚Äôs generate some linear-looking data to test this equation on:</p>
<div id="a6e073bb" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">200</span>  <span class="co"># number of instances</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> rng.random((m, <span class="dv">1</span>))  <span class="co"># column vector</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">4</span> <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> X <span class="op">+</span> rng.standard_normal((m, <span class="dv">1</span>))  <span class="co"># column vector</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="04d4cc3a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.axis((<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>))</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now let‚Äôs compute <span class="math inline">\(\hat{\mathbf{\theta}}\)</span> using the normal equation. We will use the <code>inv()</code> function from NumPy‚Äôs linear algebra module (<code>np.linalg</code>) to compute the inverse of a matrix, and the @ operator for matrix multiplication:</p>
<div id="5166df5f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> add_dummy_feature</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>X_b <span class="op">=</span> add_dummy_feature(X)  <span class="co"># add x0 = 1 to each instance</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>theta_best <span class="op">=</span> np.linalg.inv(X_b.T <span class="op">@</span> X_b) <span class="op">@</span> X_b.T <span class="op">@</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The function that we used to generate the data is <span class="math inline">\(y = 4 + 3x_1 + \text{Gaussian noise}\)</span>. Let‚Äôs see what the equation found:</p>
<div id="7205580e" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>theta_best</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>array([[3.69084138],
       [3.32960458]])</code></pre>
</div>
</div>
<p>We would have hoped for <span class="math inline">\(\theta_0 = 4\)</span> and <span class="math inline">\(\theta_1 = 3\)</span> instead of <span class="math inline">\(\theta_0 = 3.6908\)</span> and <span class="math inline">\(\theta_1 = 3.3296\)</span>. Close enough, but the noise made it impossible to recover the exact parameters of the original function. The smaller and noisier the dataset, the harder it gets.</p>
<p>Now we can make predictions using <span class="math inline">\(\hat{\mathbf{\theta}}\)</span>:</p>
<div id="e62a0faa" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">2</span>]])</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>X_new_b <span class="op">=</span> add_dummy_feature(X_new)  <span class="co"># add x0 = 1 to each instance</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> X_new_b <span class="op">@</span> theta_best</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y_predict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>array([[ 3.69084138],
       [10.35005055]])</code></pre>
</div>
</div>
<p>Let‚Äôs plot this model‚Äôs predictions:</p>
<div id="82a304fc" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))  <span class="co"># extra code ‚Äì not needed, just formatting</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_predict, <span class="st">"r-"</span>, label<span class="op">=</span><span class="st">"Predictions"</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># extra code ‚Äì beautifies Figure 4‚Äì2</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.axis((<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">15</span>))</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Performing linear regression using Scikit-Learn is relatively straightforward:</p>
<div id="2c96fc4d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>lin_reg.fit(X, y)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>lin_reg.intercept_, lin_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(array([3.69084138]), array([[3.32960458]]))</code></pre>
</div>
</div>
<div id="288cf490" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>lin_reg.predict(X_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([[ 3.69084138],
       [10.35005055]])</code></pre>
</div>
</div>
<p>Notice that Scikit-Learn separates the bias term (<code>intercept_</code>) from the feature weights (<code>coef_</code>).</p>
<p><strong>Note:</strong> Scikit-learn uses a technique called <em>singular value decomposition</em> (SVD) to solve the normal equation. This approach is more efficient than computing the normal equation, plus it handles edge cases nicely: indeed, the normal equation may not work if the matrix <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> is not invertible (i.e., singular), such as if <span class="math inline">\(m &lt; n\)</span> or if some features are redundant, but the pseudoinverse is always defined.</p>
</section>
<section id="gradient-descent-regression" class="level4">
<h4 class="anchored" data-anchor-id="gradient-descent-regression">2.3.1.2 Gradient Descent Regression</h4>
<p>To perform linear regression using stochastic GD with Scikit-Learn, you can use the <code>SGDRegressor</code> class, which defaults to optimizing the MSE cost function. The following code runs for a maximum of 1,000 epochs (<code>max_iter</code>) or until the loss drops by less than <span class="math inline">\(10^{-5}\)</span> (<code>tol</code>) during 100 epochs (<code>n_iter_no_change</code>). It starts with a learning rate of 0.01 (<code>eta0</code>), using the default learning schedule. Lastly, it does not use any regularization (<code>penalty=None</code>; more details on this shortly):</p>
<div id="136bfb2d" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDRegressor</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-5</span>, penalty<span class="op">=</span><span class="va">None</span>, eta0<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                       n_iter_no_change<span class="op">=</span><span class="dv">100</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>sgd_reg.fit(X, y.ravel())  <span class="co"># y.ravel() because fit() expects 1D targets</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "‚ñ∏";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  display: none;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  display: block;
  width: 100%;
  overflow: visible;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "‚ñæ";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}

.estimator-table summary {
    padding: .5rem;
    font-family: monospace;
    cursor: pointer;
}

.estimator-table details[open] {
    padding-left: 0.1rem;
    padding-right: 0.1rem;
    padding-bottom: 0.3rem;
}

.estimator-table .parameters-table {
    margin-left: auto !important;
    margin-right: auto !important;
}

.estimator-table .parameters-table tr:nth-child(odd) {
    background-color: #fff;
}

.estimator-table .parameters-table tr:nth-child(even) {
    background-color: #f6f6f6;
}

.estimator-table .parameters-table tr:hover {
    background-color: #e0e0e0;
}

.estimator-table table td {
    border: 1px solid rgba(106, 105, 104, 0.232);
}

.user-set td {
    color:rgb(255, 94, 0);
    text-align: left;
}

.user-set td.value pre {
    color:rgb(255, 94, 0) !important;
    background-color: transparent !important;
}

.default td {
    color: black;
    text-align: left;
}

.user-set td i,
.default td i {
    color: black;
}

.copy-paste-icon {
    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);
    background-repeat: no-repeat;
    background-size: 14px 14px;
    background-position: 0;
    display: inline-block;
    width: 14px;
    height: 14px;
    cursor: pointer;
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SGDRegressor(n_iter_no_change=100, penalty=None, random_state=42, tol=1e-05)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>SGDRegressor</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.SGDRegressor.html">?<span>Documentation for SGDRegressor</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted" data-param-prefix="">
        <div class="estimator-table">
            <details>
                <summary>Parameters</summary>
                
<table class="parameters-table caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<tbody>
<tr class="default odd">
<td><em></em></td>
<td class="param">loss&nbsp;</td>
<td class="value">'squared_error'</td>
</tr>
<tr class="user-set even">
<td><em></em></td>
<td class="param">penalty&nbsp;</td>
<td class="value">None</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">alpha&nbsp;</td>
<td class="value">0.0001</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">l1_ratio&nbsp;</td>
<td class="value">0.15</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">fit_intercept&nbsp;</td>
<td class="value">True</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">max_iter&nbsp;</td>
<td class="value">1000</td>
</tr>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">tol&nbsp;</td>
<td class="value">1e-05</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">shuffle&nbsp;</td>
<td class="value">True</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">verbose&nbsp;</td>
<td class="value">0</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">epsilon&nbsp;</td>
<td class="value">0.1</td>
</tr>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">random_state&nbsp;</td>
<td class="value">42</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">learning_rate&nbsp;</td>
<td class="value">'invscaling'</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">eta0&nbsp;</td>
<td class="value">0.01</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">power_t&nbsp;</td>
<td class="value">0.25</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">early_stopping&nbsp;</td>
<td class="value">False</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">validation_fraction&nbsp;</td>
<td class="value">0.1</td>
</tr>
<tr class="user-set odd">
<td><em></em></td>
<td class="param">n_iter_no_change&nbsp;</td>
<td class="value">100</td>
</tr>
<tr class="default even">
<td><em></em></td>
<td class="param">warm_start&nbsp;</td>
<td class="value">False</td>
</tr>
<tr class="default odd">
<td><em></em></td>
<td class="param">average&nbsp;</td>
<td class="value">False</td>
</tr>
</tbody>
</table>

            </details>
        </div>
    </div></div></div></div></div><script>function copyToClipboard(text, element) {
    // Get the parameter prefix from the closest toggleable content
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;

    const originalStyle = element.style;
    const computedStyle = window.getComputedStyle(element);
    const originalWidth = computedStyle.width;
    const originalHTML = element.innerHTML.replace('Copied!', '');

    navigator.clipboard.writeText(fullParamName)
        .then(() => {
            element.style.width = originalWidth;
            element.style.color = 'green';
            element.innerHTML = "Copied!";

            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        })
        .catch(err => {
            console.error('Failed to copy:', err);
            element.style.color = 'red';
            element.innerHTML = "Failed!";
            setTimeout(() => {
                element.innerHTML = originalHTML;
                element.style = originalStyle;
            }, 2000);
        });
    return false;
}

document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {
    const toggleableContent = element.closest('.sk-toggleable__content');
    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';
    const paramName = element.parentElement.nextElementSibling.textContent.trim();
    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;

    element.setAttribute('title', fullParamName);
});
</script>
</div>
</div>
<p>Once again, you find a solution quite close to the one returned by the normal equation:</p>
<div id="8e831139" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>sgd_reg.intercept_, sgd_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>(array([3.68899733]), array([3.33054574]))</code></pre>
</div>
</div>
<p><strong>Tip:</strong> All Scikit-Learn estimators can be trained using the <code>fit()</code> method, but some estimators also have a <code>partial_fit()</code> method that you can call to run a single round of training on one or more instances (it ignores hyperparameters like <code>max_iter</code> or <code>tol</code>). Repeatedly calling <code>partial_fit()</code> will gradually train the model. This is useful when you need more control over the training process. Other models have a <code>warm_start</code> hyperparameter instead (and some have both): if you set <code>warm_start=True</code>, calling the <code>fit()</code> method on a trained model will not reset the model; it will just continue training where it left off, respecting hyperparameters like <code>max_iter</code> and <code>tol</code>. Note that <code>fit()</code> resets the iteration counter used by the learning schedule, while <code>partial_fit()</code> does not.</p>
</section>
</section>
<section id="loss" class="level3">
<h3 class="anchored" data-anchor-id="loss">2.3.2 Loss</h3>
<p>Loss is a numerical metric that describes how wrong a model‚Äôs predictions are. Loss measures the distance between the model‚Äôs predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.</p>
<p>In the following image, you can visualize loss as arrows drawn from the data points to the model. The arrows show how far the model‚Äôs predictions are from the actual values.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/loss-lines.png" class="img-fluid figure-img"></p>
<figcaption>Loss is measured from the actual value to the predicted value.</figcaption>
</figure>
</div>
<p><strong>Distance of loss</strong>:</p>
<p>In statistics and machine learning, loss measures the difference between the predicted and actual values. Loss focuses on the distance between the values, not the direction. For example, if a model predicts 2, but the actual value is 5, we don‚Äôt care that the loss is negative <span class="math inline">\((2 - 5 = -3)\)</span>. Instead, we care that the distance between the values is 3. Thus, all methods for calculating loss remove the sign.</p>
<p>The two most common methods to remove the sign are the following:</p>
<ul>
<li>Take the absolute value of the difference between the actual value and the prediction.</li>
<li>Square the difference between the actual value and the prediction.</li>
</ul>
<p><strong>Types of loss</strong>:</p>
<p>In linear regression, there are five main types of loss, which are outlined in the following table.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 36%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Loss Type</th>
<th>Definition</th>
<th>Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(L_1\)</span> loss</td>
<td>The sum of the absolute values of the difference between the predicted values and the actual values.</td>
<td>$| -| $</td>
</tr>
<tr class="even">
<td>Mean Absolute Error (MAE)</td>
<td>The average of <span class="math inline">\(L_1\)</span> losses across a set of <span class="math inline">\(N\)</span> examples.</td>
<td>$ | -| $</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(L_2\)</span> loss</td>
<td>The sum of the squared differences between the predicted values and the actual values.</td>
<td>$(-)^2 $</td>
</tr>
<tr class="even">
<td>Mean Squared Error (MSE)</td>
<td>The average of <span class="math inline">\(L_2\)</span> losses across a set of <span class="math inline">\(N\)</span> examples.</td>
<td>$ (-)^2 $</td>
</tr>
<tr class="odd">
<td>Root Mean Squared Error (RMSE)</td>
<td>The square root of the MSE.</td>
<td>$ $</td>
</tr>
</tbody>
</table>
<p>The functional difference between <span class="math inline">\(L_1\)</span> loss and <span class="math inline">\(L_2\)</span> loss (or between MAE/RMSE and MSE) is squaring. When the difference between the prediction and label is large, squaring makes the loss even larger. When the difference is small (less than 1), squaring makes the loss even smaller.</p>
<p>Loss metrics like MAE and RMSE may be preferable to <span class="math inline">\(L_2\)</span> loss or MSE in some use cases because they tend to be more human-interpretable, as they measure error using the same scale as the model‚Äôs predicted value.</p>
<p><strong>Note</strong>: MAE and RMSE can differ quite widely. MAE represents the average prediction error, whereas RMSE represents the ‚Äúspread‚Äù of the errors, and is more skewed by larger errors.</p>
<p>When processing multiple examples at once, we recommend averaging the losses across all the examples, whether using MAE, MSE, or RMSE.</p>
<p><strong>Choosing a loss</strong>:</p>
<p>Deciding whether to use MAE or MSE can depend on the dataset and the way you want to handle certain predictions. Most feature values in a dataset typically fall within a distinct range. For example, cars are normally between 2000 and 5000 pounds and get between 8 to 50 miles per gallon. An 8,000-pound car, or a car that gets 100 miles per gallon, is outside the typical range and would be considered an outlier.</p>
<p>An outlier can also refer to how far off a model‚Äôs predictions are from the real values. For instance, 3,000 pounds is within the typical car-weight range, and 40 miles per gallon is within the typical fuel-efficiency range. However, a 3,000-pound car that gets 40 miles per gallon would be an outlier in terms of the model‚Äôs prediction because the model would predict that a 3,000-pound car would get around 20 miles per gallon.</p>
<p>When choosing the best loss function, consider how you want the model to treat outliers. For instance, MSE moves the model more toward the outliers, while MAE doesn‚Äôt. L2 loss incurs a much higher penalty for an outlier than L1 loss. For example, the following images show a model trained using MAE and a model trained using MSE. The red line represents a fully trained model that will be used to make predictions. The outliers are closer to the model trained with MSE than to the model trained with MAE.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/model-mse.png" class="img-fluid figure-img"></p>
<figcaption>MSE loss moves the model closer to the outliers.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./img/model-mae.png" class="img-fluid figure-img"></p>
<figcaption>MAE loss keeps the model farther from the outliers.</figcaption>
</figure>
</div>
<p>Note the relationship between the model and the data:</p>
<ul>
<li><strong>MSE</strong>: The model is closer to the outliers but further away from most of the other data points.</li>
<li><strong>MAE</strong>: The model is further away from the outliers but closer to most of the other data points.</li>
</ul>
<p><strong>Choose MSE</strong>:</p>
<ul>
<li>If you want to heavily penalize large errors.</li>
<li>If you believe the outliers are important and indicative of true data variance that the model should account for.</li>
</ul>
<p><strong>Choose MAE</strong>:</p>
<ul>
<li>If your dataset has significant outliers that you don‚Äôt want to overly influence the model. MAE is more robust.</li>
<li>If you prefer a loss function that is more directly interpretable as the average error magnitude.</li>
</ul>
<p>In practice, your metric choice can also depend on the specific business problem and what kind of errors are more costly.</p>
</section>
<section id="polynomial-regression" class="level3">
<h3 class="anchored" data-anchor-id="polynomial-regression">2.3.3 Polynomial Regression</h3>
<p>What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called polynomial regression.</p>
<p>Let‚Äôs look at an example. First, we‚Äôll generate some nonlinear data, based on a simple quadratic equation‚Äîthat‚Äôs an equation of the form <span class="math inline">\(y = ax^2 + bx + c\)</span>‚Äîplus some noise:</p>
<div id="e4a3dd0a" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">200</span>  <span class="co"># number of instances</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">6</span> <span class="op">*</span> rng.random((m, <span class="dv">1</span>)) <span class="op">-</span> <span class="dv">3</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> X <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> X <span class="op">+</span> <span class="dv">2</span> <span class="op">+</span> rng.standard_normal((m, <span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="19820799" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.axis((<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Clearly, a straight line will never fit this data properly. So let‚Äôs use Scikit-Learn‚Äôs <code>PolynomialFeatures</code> class to transform our training data, adding the square (second-degree polynomial) of each feature in the training set as a new feature (in this case there is just one feature):</p>
<div id="832f89da" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>poly_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>X_poly <span class="op">=</span> poly_features.fit_transform(X)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>array([1.64373629])</code></pre>
</div>
</div>
<div id="d6c36c5a" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>X_poly[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>array([1.64373629, 2.701869  ])</code></pre>
</div>
</div>
<p><code>X_poly</code> now contains the original feature of <code>X</code> plus the square of this feature. Now we can fit a <code>LinearRegression</code> model to this extended training data :</p>
<div id="ab7e81cb" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>lin_reg.fit(X_poly, y)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>lin_reg.intercept_, lin_reg.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>(array([2.00540719]), array([[1.11022126, 0.50526985]]))</code></pre>
</div>
</div>
<div id="0b4e08d2" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>X_new_poly <span class="op">=</span> poly_features.transform(X_new)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>y_new <span class="op">=</span> lin_reg.predict(X_new_poly)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_new, <span class="st">"r-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">"Predictions"</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>plt.axis((<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Not bad: the model estimates <span class="math inline">\(\hat{y} = 0.56 x_1^2 + 0.93 x_1 + 1.78\)</span> when in fact the original function was <span class="math inline">\(y = 0.5 x_1^2 + 1.0 x_1 + 2.0 +\)</span> Gaussian noise.</p>
<p>Note that when there are multiple features, polynomial regression is capable of finding relationships between features, which is something a plain linear regression model cannot do. This is made possible by the fact that <code>PolynomialFeatures</code> also adds all combinations of features up to the given degree. For example, if there were two features <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, <code>PolynomialFeatures</code> with <code>degree=3</code> would not only add the features <span class="math inline">\(a^2\)</span>, <span class="math inline">\(a^3\)</span>, <span class="math inline">\(b^2\)</span>, and <span class="math inline">\(b^3\)</span>, but also the combinations <span class="math inline">\(ab\)</span>, <span class="math inline">\(a^2b\)</span>, and <span class="math inline">\(ab^2\)</span>.</p>
<p>If you perform high-degree polynomial regression, you will likely fit the training data much better than with plain linear regression. For example, in the next figure a 300-degree polynomial model is applied to the preceding training data, and compares the result with a pure linear model and a quadratic model (second-degree polynomial). Notice how the 300-degree polynomial model wiggles around to get as close as possible to the training instances.</p>
<div id="a02b661e" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> style, width, degree <span class="kw">in</span> ((<span class="st">"r-+"</span>, <span class="dv">2</span>, <span class="dv">1</span>), (<span class="st">"b--"</span>, <span class="dv">2</span>, <span class="dv">2</span>), (<span class="st">"g-"</span>, <span class="dv">1</span>, <span class="dv">300</span>)):</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    polybig_features <span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span>degree, include_bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    std_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    lin_reg <span class="op">=</span> LinearRegression()</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    polynomial_regression <span class="op">=</span> make_pipeline(polybig_features, std_scaler, lin_reg)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    polynomial_regression.fit(X, y)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    y_newbig <span class="op">=</span> polynomial_regression.predict(X_new)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss"> degree</span><span class="sc">{</span><span class="st">'s'</span> <span class="cf">if</span> degree <span class="op">&gt;</span> <span class="dv">1</span> <span class="cf">else</span> <span class="st">''</span><span class="sc">}</span><span class="ss">"</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(X_new, y_newbig, style, label<span class="op">=</span>label, linewidth<span class="op">=</span>width)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"b."</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>plt.axis((<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">10</span>))</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Warning</strong>: <code>PolynomialFeatures(degree=d)</code> transforms an array containing <span class="math inline">\(n\)</span> features into an array containing <span class="math inline">\(\frac{(n + d)!}{d! \, n!}\)</span> features, where <span class="math inline">\(n!\)</span> is the factorial of <span class="math inline">\(n\)</span>, equal to <span class="math inline">\(1 \times 2 \times 3 \times \cdots \times n\)</span>. Beware of the combinatorial explosion of the number of features!</p>
<p>This high-degree polynomial regression model is severely overfitting the training data, while the linear model is underfitting it. The model that will generalize best in this case is the quadratic model, which makes sense because the data was generated using a quadratic model. But in general you won‚Äôt know what function generated the data, so how can you decide how complex your model should be? How can you tell that your model is overfitting or underfitting the data?</p>
<p>We have used cross-validation to get an estimate of a model‚Äôs generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex.</p>
</section>
<section id="regularized-linear-models" class="level3">
<h3 class="anchored" data-anchor-id="regularized-linear-models">2.3.4 Regularized Linear Models</h3>
<p>A good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. A simple way to regularize a polynomial model is to reduce the number of polynomial degrees.</p>
<p>What about linear models? Can we regularize them too? You may wonder why we may want to do that: aren‚Äôt linear models constrained enough already? Well, linear regression makes a few assumptions, including the fact that the true relationship between the inputs and the outputs is linear, the noise has zero mean, constant variance, and is independent of the inputs, plus the input matrix has full rank, meaning that the inputs are not colinear‚Å† and there at least as many samples as parameters. In practice, some assumptions don‚Äôt hold perfectly. For example, some inputs may be close to colinear, which makes linear regression numerically unstable, meaning that very small differences in the training set can have a big impact on the trained model. Regularization can stabilize linear models and make them more accurate.</p>
<p>So how can we regularize a linear model? This is usually done by constraining its weights. In this section, we will discuss ridge regression, lasso regression, and elastic net regression, which implement three different ways to do that.</p>
<section id="ridge-regression" class="level4">
<h4 class="anchored" data-anchor-id="ridge-regression">2.3.4.1 Ridge Regression</h4>
<p>Ridge regression is a regularized version of linear regression: a regularization term equal to <span class="math display">\[ \frac{\alpha}{m} \sum_{i=1}^{n} \theta_i^2 \]</span> is added to the MSE. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. This constraint makes the model less flexible, preventing it from stretching itself too much to fit every data point: this reduces the risk of overfitting. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to use the unregularized MSE (or the RMSE) to evaluate the model‚Äôs performance.</p>
<p>The hyperparameter Œ± controls how much you want to regularize the model. If <span class="math inline">\(\alpha = 0\)</span>, then ridge regression is just linear regression. If <span class="math inline">\(\alpha\)</span> is very large, then all weights end up very close to zero and the result is a flat line going through the data‚Äôs mean.</p>
<p><strong>The ridge regression cost function</strong>:</p>
<p><span class="math display">\[ J(ùõâ) = MSE(ùõâ) + \frac{\alpha}{m} \sum_{i=1}^{n} \theta_i^2 \]</span></p>
<p>Note that the bias term $ _0 $ is not regularized (the sum starts at <span class="math inline">\(i = 1\)</span>, not <span class="math inline">\(0\)</span>).</p>
<p><strong>Ridge regression closed-form solution</strong>:</p>
<p><span class="math display">\[\hat{ùõâ} = (\mathbf{X}^\top \mathbf{X} + \alpha \mathbf{A})^{-1} \mathbf{X}^\top \mathbf{y}\]</span></p>
<p>In this equation, <span class="math inline">\(\mathbf{A}\)</span> is an identity matrix with a zero in the top-left corner to avoid regularizing the bias term.</p>
<p><strong>Warning</strong>: It is important to scale the data (e.g., using a <code>StandardScaler</code>) before performing ridge regression, as it is sensitive to the scale of the input features. This is true of most regularized models.</p>
<p>Let‚Äôs generate a very small and noisy linear dataset:</p>
<div id="d57feafb" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">20</span>  <span class="co"># number of instances</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">3</span> <span class="op">*</span> rng.random((m, <span class="dv">1</span>))</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> X <span class="op">+</span> rng.standard_normal((m, <span class="dv">1</span>)) <span class="op">/</span> <span class="fl">1.5</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="eb854b8f" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">4</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"."</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$  "</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>plt.axis((<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="fl">3.5</span>))</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-20-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here is how to perform ridge regression with Scikit-Learn using a closed-form solution:</p>
<div id="bf906fd8" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>ridge_reg <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>, solver<span class="op">=</span><span class="st">"cholesky"</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>ridge_reg.fit(X, y)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>ridge_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>array([1.84414523])</code></pre>
</div>
</div>
<p>The next figure shows several ridge models that were trained on some very noisy linear data using different <span class="math inline">\(\alpha\)</span> values. On the left, plain ridge models are used, leading to linear predictions. On the right, the data is first expanded using <code>PolynomialFeatures(degree=10)</code>, then it is scaled using a <code>StandardScaler</code>, and finally the ridge models are applied to the resulting features: this is polynomial regression with ridge regularization. Note how increasing Œ± leads to flatter (i.e., less extreme, more reasonable) predictions, thus reducing the model‚Äôs variance but increasing its bias.</p>
<div id="05a37ced" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_model(model_class, polynomial, alphas, <span class="op">**</span>model_kwargs):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    plt.plot(X, y, <span class="st">"b."</span>, linewidth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> alpha, style <span class="kw">in</span> <span class="bu">zip</span>(alphas, (<span class="st">"b:"</span>, <span class="st">"g--"</span>, <span class="st">"r-"</span>)):</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> alpha <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> model_class(alpha, <span class="op">**</span>model_kwargs)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> LinearRegression()</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> polynomial:</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> make_pipeline(</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>                PolynomialFeatures(degree<span class="op">=</span><span class="dv">10</span>, include_bias<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>                StandardScaler(),</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>                model)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        model.fit(X, y)</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        y_new_regul <span class="op">=</span> model.predict(X_new)</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        plt.plot(X_new, y_new_regul, style, linewidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>                 label<span class="op">=</span><span class="vs">fr"$\alpha = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="vs">$"</span>)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>    plt.legend(loc<span class="op">=</span><span class="st">"upper left"</span>)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"$x_1$"</span>)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>    plt.axis((<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">0</span>, <span class="fl">3.5</span>))</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="fl">3.5</span>))</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>plot_model(Ridge, polynomial<span class="op">=</span><span class="va">False</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">100</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$  "</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>plot_model(Ridge, polynomial<span class="op">=</span><span class="va">True</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">10</span><span class="op">**-</span><span class="dv">5</span>, <span class="dv">1</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>plt.gca().axes.yaxis.set_ticklabels([])</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As with linear regression, we can perform ridge regression either by computing a closed-form equation or by performing gradient descent.</p>
<div id="114ee1e5" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(penalty<span class="op">=</span><span class="st">"l2"</span>, alpha<span class="op">=</span><span class="fl">0.1</span> <span class="op">/</span> m, tol<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>                       max_iter<span class="op">=</span><span class="dv">1000</span>, eta0<span class="op">=</span><span class="fl">0.01</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>sgd_reg.fit(X, y.ravel())  <span class="co"># y.ravel() because fit() expects 1D targets</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>sgd_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>array([1.83659707])</code></pre>
</div>
</div>
<div id="aa1b9caa" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>ridge_reg <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>, solver<span class="op">=</span><span class="st">"sag"</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>ridge_reg.fit(X, y)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>ridge_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>array([1.84415109])</code></pre>
</div>
</div>
<p>The penalty hyperparameter sets the type of regularization term to use. Specifying <code>"l2"</code> indicates that you want SGD to add a regularization term to the MSE cost function equal to <code>alpha</code> times the square of the <span class="math inline">\(‚Ñì_2\)</span> norm of the weight vector. This is just like ridge regression, except there‚Äôs no division by <span class="math inline">\(m\)</span> in this case; that‚Äôs why we passed <code>alpha=0.1 / m</code>, to get the same result as <code>Ridge(alpha=0.1)</code>.</p>
<div id="c5f885f3" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extra code ‚Äì shows the closed form solution of Ridge regression,</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="co">#              compare with the next Ridge model's learned parameters below</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.array([[<span class="fl">0.</span>, <span class="fl">0.</span>], [<span class="fl">0.</span>, <span class="fl">1.</span>]])</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>X_b <span class="op">=</span> np.c_[np.ones(m), X]</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>np.linalg.inv(X_b.T <span class="op">@</span> X_b <span class="op">+</span> alpha <span class="op">*</span> I) <span class="op">@</span> X_b.T <span class="op">@</span> y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>array([[1.23749481],
       [0.40443361]])</code></pre>
</div>
</div>
<div id="a385b6bd" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>ridge_reg.intercept_, ridge_reg.coef_  <span class="co"># extra code</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>(array([1.23754037]), array([0.40440715]))</code></pre>
</div>
</div>
<p><strong>Tip</strong>: The <code>RidgeCV</code> class also performs ridge regression, but it automatically tunes hyperparameters using cross-validation. It‚Äôs roughly equivalent to using <code>GridSearchCV</code>, but it‚Äôs optimized for ridge regression and runs <em>much</em> faster. Several other estimators (mostly linear) also have efficient CV variants, such as <code>LassoCV</code> and <code>ElasticNetCV</code>.</p>
</section>
<section id="lasso-regression" class="level4">
<h4 class="anchored" data-anchor-id="lasso-regression">2.3.4.2 Lasso Regression</h4>
<p><em>Least absolute shrinkage and selection operator regression</em> (usually simply called <em>lasso regression</em>) is another regularized version of linear regression: just like ridge regression, it adds a regularization term to the cost function, but it uses the <span class="math inline">\(‚Ñì_1\)</span> norm of the weight vector instead of the square of the <span class="math inline">\(‚Ñì_2\)</span> norm. Notice that the <span class="math inline">\(‚Ñì_1\)</span> norm is multiplied by <span class="math inline">\(2\alpha\)</span>, whereas the <span class="math inline">\(‚Ñì_2\)</span> norm was multiplied by <span class="math inline">\(\alpha / m\)</span> in ridge regression. These factors were chosen to ensure that the optimal <span class="math inline">\(\alpha\)</span> value is independent from the training set size: different norms lead to different factors.</p>
<p><strong>The lasso regression cost function</strong>:</p>
<p><span class="math display">\[ J(ùõâ) = MSE(ùõâ) + 2\alpha \sum_{i=1}^{n} |\theta_i| \]</span></p>
<div id="96f825d7" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>lasso_reg <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>lasso_reg.fit(X, y)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>lasso_reg.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>array([1.87550211])</code></pre>
</div>
</div>
<p>Note that you could instead use <code>SGDRegressor(penalty="l1", alpha=0.1)</code>.</p>
<div id="9175a009" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="fl">3.5</span>))</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">121</span>)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>plot_model(Lasso, polynomial<span class="op">=</span><span class="va">False</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">1</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$  "</span>, rotation<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">122</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>plot_model(Lasso, polynomial<span class="op">=</span><span class="va">True</span>, alphas<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1e-2</span>, <span class="dv">1</span>), random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>plt.gca().axes.yaxis.set_ticklabels([])</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="2_4_regression_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>An important characteristic of lasso regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero). For example, the dashed line in the righthand plot (with <span class="math inline">\(\alpha = 0.01\)</span>) looks roughly cubic: all the weights for the high-degree polynomial features are equal to zero. In other words, lasso regression automatically performs feature selection and outputs a <em>sparse model</em> with few nonzero feature weights. Of course, there‚Äôs a trade-off: if you increase <span class="math inline">\(\alpha\)</span> too much, the model will be very sparse, but its performance will plummet.</p>
</section>
<section id="elastic-net-regression" class="level4">
<h4 class="anchored" data-anchor-id="elastic-net-regression">2.3.4.3 Elastic Net Regression</h4>
<p>Elastic net regression is a middle ground between ridge regression and lasso regression. The regularization term is a weighted sum of both ridge and lasso‚Äôs regularization terms, and you can control the mix ratio <span class="math inline">\(r\)</span>. When <span class="math inline">\(r = 0\)</span>, elastic net is equivalent to ridge regression, and when <span class="math inline">\(r = 1\)</span>, it is equivalent to lasso regression</p>
<p><strong>Elastic net cost function</strong>:</p>
<p><span class="math display">\[ J(ùõâ) = MSE(ùõâ) + r \left( 2\alpha \sum_{i=1}^{n} |\theta_i| \right) + \left(1 - r \right) \left( \frac{\alpha}{m} \sum_{i=1}^{n} \theta_i^2 \right) \]</span></p>
<p>So when should you use elastic net regression, or ridge, lasso, or plain linear regression (i.e., without any regularization)? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain linear regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer lasso or elastic net because they tend to reduce the useless features‚Äô weights down to zero, as discussed earlier. In general, elastic net is preferred over lasso because lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.</p>
<p>Here is a short example that uses Scikit-Learn‚Äôs <code>ElasticNet</code> (<code>l1_ratio</code> corresponds to the mix ratio <code>r</code>):</p>
<div id="cf1a7b26" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> ElasticNet</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>elastic_net <span class="op">=</span> ElasticNet(alpha<span class="op">=</span><span class="fl">0.1</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>elastic_net.fit(X, y)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>elastic_net.predict([[<span class="fl">1.5</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>array([5.006944])</code></pre>
</div>
</div>
</section>
</section>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">2.3.5 Logistic Regression</h3>
<p><strong>Logistic regression will be covered in Deep Learning course.</strong></p>
<hr>
<p><strong>References:</strong></p>
<p>Disclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.</p>
<ul>
<li><p>Chapter 4 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur√©lien G√©ron, <a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/">3rd edition</a></p></li>
<li><p><a href="https://developers.google.com/machine-learning/crash-course/linear-regression">Linear Regression</a></p></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/emilianodesu\.github\.io\/SIAFI-2026-1\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>