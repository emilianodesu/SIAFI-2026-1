[
  {
    "objectID": "IDS/1_2_end_to_end_ml_project.html",
    "href": "IDS/1_2_end_to_end_ml_project.html",
    "title": "1. Introduction to Data Science",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "IDS/1_2_end_to_end_ml_project.html#end-to-end-machine-learning-project",
    "href": "IDS/1_2_end_to_end_ml_project.html#end-to-end-machine-learning-project",
    "title": "1. Introduction to Data Science",
    "section": "1.3 End-to-End Machine Learning Project",
    "text": "1.3 End-to-End Machine Learning Project\nIn this section, you will work through an example project end to end, pretending to be a recently hired data scientist at a real estate company. The goal of this example is to illustrate the main steps of a machine learning project, not to learn everything about the real estate business. Here are the main steps we will go through:\n\nLook at the big picture: What is the business problem we are trying to solve? How can we measure success? What is the current solution?\nGet the data: Where does the data come from? How can we get it?\nExplore and visualize the data to gain insights: What is in the data? What are the main characteristics of the data? What are the relationships between different attributes?\nPrepare the data for machine learning algorithms: How can we clean the data? How can we transform the data into a format that is suitable for machine learning algorithms?\nSelect and train a model: What are the different types of machine learning algorithms? How can we select the best algorithm for our problem? How can we train the model?\nFine-tune the model: How can we improve the model’s performance? How can we select the best hyperparameters?\nPresent the solution: How can we communicate the results to stakeholders? How can we present the solution in a way that is easy to understand?\nLaunch, monitor, and maintain the system: How can we deploy the model in a production environment? How can we monitor the model’s performance over time? How can we update the model as new data becomes available?\n\n\n1.3.1 Working with Real Data\nWhen learning about machine learning, it is best to experiment with real-world data, not artificial datasets. Fortunately, there are thousands of open datasets to choose from, ranging across all sorts of domains. Here are a few places you can look to get data:\n\nPopular open data repositories:\n\nOpenML.org\nKaggle.com\nUCI Machine Learning Repository\nAmazon AWS Open Data Registry\nGoogle Dataset Search\nTensorFlow Datasets\nPyTorch Datasets\nScikit-learn Datasets\n\nMeta portals that list many datasets:\n\nDataPortals.org\nAwesome Public Datasets\n\nOther pages listing many popular data repositories:\n\nWikipedia: List of datasets for machine-learning research\nQuora.com\nThe datasets subreddit\n\n\nIn this example, we will use the California housing dataset, which contains information about various districts in California, including features such as median income, average house age, and average number of rooms. This dataset is available in the StatLib repository and is also included in the Scikit-learn library.\n\n\n\nCalifornia housing prices\n\n\n\n\n1.3.2 Look at the Big Picture\nOur first task is to use California census data to build a model of housing prices in the state. This model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.\nThe first question to ask is what exactly the business objective is. Building a model is probably not the end goal. How does the company expect to use and benefit from this model? Knowing the objective is important because it will determine how you frame the problem, which algorithms you will select, which performance measure you will use to evaluate your model, and how much effort you will spend tweaking it.\n\n1.3.2.1 Pipelines\nA sequence of data processing components is called a data pipeline. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\nComponents typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust.\nOn the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system’s performance drops.\n\n\n\nMachine learning real estate pipeline\n\n\n\n\n1.3.2.2 Frame the Problem\nFirst, determine what kind of training supervision the model will need: is it a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task? And is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\nThis is clearly a typical supervised learning task, since the model can be trained with labeled examples (each instance comes with the expected output, i.e., the district’s median housing price). It is a typical regression task, since the model will be asked to predict a value. More specifically, this is a multiple regression problem, since the system will use multiple features to make a prediction (the district’s population, the median income, etc.). It is also a univariate regression problem, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust changing data rapidly, and data is small enough to fit in memory, so plain batch learning should do just fine.\n\n\n1.3.2.3 Select a Performance Measure\nA typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.\n\\[ RMSE(\\textbf{X}, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} \\left( h(\\textbf{x}^{(i)}) - y^{(i)} \\right) ^2} \\]\nwhere:\n\n\\(\\textbf{x}^{(i)}\\) is a vector of all the feature values of the \\(i^{th}\\) instance in the dataset \\(\\textbf{X}\\), and \\(y^{(i)}\\) is the corresponding target value or label (the median housing price in this case).\n\\(\\textbf{X}\\) is a matrix containing all the feature values (excluding labels) of all instances in the dataset. There is one row per instance, and the \\(i^{th}\\) row contains is equal to the transpose of the feature vector \\(\\textbf{x}^{(i)}\\).\n\\(m\\) is the number of instances in the dataset \\(\\textbf{X}\\)\n\\(h\\) is your system’s prediction function, also called hypothesis. When a system is given instance’s feature vector \\(\\textbf{x}^{(i)}\\), it outputs a predicted value \\(\\hat{y}^{(i)} = h(\\textbf{x}^{(i)})\\) for that instance.\n\nAlthough the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, if there are many outliers in the data, you may want to use the Mean Absolute Error (MAE) instead, since it is less sensitive to outliers.\n\\[ MAE(\\textbf{X}, h) = \\frac{1}{m} \\sum_{i=1}^{m} | h(\\textbf{x}^{(i)}) - y^{(i)} | \\]\n\n\n\n1.3.3 Get the Data\nIn typical environments your data would be available in a relational database or some other common data store, and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations⁠ and familiarize yourself with the data schema. In this project, however, things are much simpler: We are going to download a single compressed file, housing.tgz, which contains a comma-separated values (CSV) file called housing.csv with all the data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_housing_data():\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n    with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path=\"datasets\", filter='data')\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n\n\n1.3.3.1 Take a Quick Look at the Data Structure\n\nhousing = load_housing_data()\nhousing.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThe info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of non-null val\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing['ocean_proximity'].value_counts()\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\nThe count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, the count of total_bedrooms is 20,433, not 20,640). The std row shows the standard deviation, which measures how dispersed the values are.⁠ The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of the districts have a housing_median_age lower than 18, while 50% are lower than 29 and 75% are lower than 37. These are often called the 25th percentile (or first quartile), the median, and the 75th percentile (or third quartile).\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have given a value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the entire DataFrame to plot a histogram for each numerical attribute. Here is the result:\n\nimport matplotlib.pyplot as plt\n\nhousing.hist(bins=50, figsize=(16, 8))\nplt.show()\n\n\n\n\n\n\n\n\nLooking at these histograms, you notice a few things:\n\nFirst, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in machine learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\nThe housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your machine learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:\n\nCollect proper labels for the districts whose labels were capped.\nRemove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\n\nThese attributes have very different scales. We will discuss this later in this chapter, when we explore feature scaling.\nFinally, many histograms are skewed right: they extend much farther to the right of the median than to the left. This may make it a bit harder for some machine learning algorithms to detect patterns in the data. Later, we’ll try to tranform these attributes to have a more symmetrical and bell-shaped distribution.\n\n\n\n1.3.3.2 Create a Test Set\nIt may seem strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which also means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of machine learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called data snooping bias.\nCreating a test set is theoretically simple; pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside.\nScikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(). There is a random_state parameter that allows you to set the random generator seed. Second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is useful when you have a separate set of labels). Here is how to use it:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\nHowever, we are considering a purely random sampling method. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When employees at a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population, with regard to the questions they want to ask. For example, the US population is 51.1% females and 48.9% males, so a well-conducted survey in the US would try to maintain this ratio in the sample: 511 females and 489 males (at least if it seems possible that the answers may vary across genders). This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population. If the people running the survey used purely random sampling, there would be about a 10.7% chance of sampling a skewed test set with less than 48.5% female or more than 53.5% female participants. Either way, the survey results would likely be quite biased.\nSuppose you’ve chatted with some experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely (back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of a stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with five categories (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n\nhousing['income_cat'] = pd.cut(housing['median_income'],\n                               bins=[0., 1.5, 3.0, 4.5, 6., float('inf')],\n                               labels=[1, 2, 3, 4, 5])\n\nhousing['income_cat'].value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.xlabel('Income Category')\nplt.ylabel('Number of Districts')\nplt.show()\n\n\n\n\n\n\n\n\nWe can use the train_test_split() function with the stratify parameter to perform stratified sampling based on the income category:\n\nstrat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, stratify=housing['income_cat'])\n\nstrat_test_set['income_cat'].value_counts() / len(strat_test_set)\n\nincome_cat\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114341\n1    0.039971\nName: count, dtype: float64\n\n\n\n# extra code – computes the data for proportions in the full dataset\n\ndef income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall %\": income_cat_proportions(housing),\n    \"Stratified %\": income_cat_proportions(strat_test_set),\n    \"Random %\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props.index.name = \"Income Category\"\ncompare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] / compare_props[\"Overall %\"] - 1)\ncompare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] / compare_props[\"Overall %\"] - 1)\n(compare_props * 100).round(2)\n\n\n\n\n\n\n\n\nOverall %\nStratified %\nRandom %\nStrat. Error %\nRand. Error %\n\n\nIncome Category\n\n\n\n\n\n\n\n\n\n1\n3.98\n4.00\n4.24\n0.36\n6.45\n\n\n2\n31.88\n31.88\n30.74\n-0.02\n-3.59\n\n\n3\n35.06\n35.05\n34.52\n-0.01\n-1.53\n\n\n4\n17.63\n17.64\n18.41\n0.03\n4.42\n\n\n5\n11.44\n11.43\n12.09\n-0.08\n5.63\n\n\n\n\n\n\n\n\n# We can now remove the `income_cat` attribute so the data is back to its original state.\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)\n\n\n\n\n1.3.4 Explore and Visualize the Data to Gain Insights\nFirst, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast during the exploration phase. In this case, the training set is quite small, so you can just work directly on the full set. Since we are going to modify the training set a lot during the exploration phase, it is a good idea to make a copy of it so that you can always go back to the original data if necessary:\n\nhousing = strat_train_set.copy()\n\n\n1.3.4.1 Visualizing Geographical Data\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n# plt.savefig('im/1_2/bad_visualization.png')\nplt.show()\n\n\n\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2, grid=True)\n# plt.savefig('im/1_2/better_visualization.png')\nplt.show()\n\n\n\n\n\n\n\n\nYou can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley (in particular, around Sacramento and Fresno).\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n             s=housing[\"population\"] / 100, label=\"population\",\n             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\n# plt.savefig(\"im/1_2/housing_prices_scatterplot.png\")\nplt.show()\n\n\n\n\n\n\n\n\nThis picture tells you that the housing prices are much related to the location (e.g. close to the ocean) and to the population density (the size of each circle is proportional to the district’s population). The ocean proximity attribute may be useful as well, although in Northern California the prices in coastal districts are not too high, so it’s not a simple rule.\n\n\n1.3.4.2 Look for Correlations\n\n\n\nCorrelation examples\n\n\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method:\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688380\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\nlongitude            -0.050859\nlatitude             -0.139584\nName: median_house_value, dtype: float64\n\n\nAnother way to check for correlation between attributes is to use the Pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. We decide to focus on a few promising attributes (including the target attribute, median_house_value):\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\n# plt.savefig(\"im/1_2/scatter_matrix_plot.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1, grid=True)\n# plt.savefig(\"im/1_2/income_vs_house_value_scatterplot.png\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals a few things. First, correlation is indeed quite strong; you can clearly see the upward trend, and the points are not too dispersed. Second, the price cap you noticed earlier is clearly visible as a horizontal line around $500,000. But the plot also reveals other less obvious straight lines: a horizontal line around $450,000,another around $350,000, perhaps aone around $280,000, and a few vertical lines as well. You may want to try removing the corresponding districts to prevent your algorithms from learning these quirks.\n\n\n1.3.4.3 Experiment with Attribute Combinations\nOne last thing you may want to do before preparing the data for machine learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. You create these new attributes as follows:\n\nhousing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\nhousing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n\nThen we can look at the correlation matrix again to see how these new attributes compare to the others:\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688380\nrooms_per_house       0.143663\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\npeople_per_house     -0.038224\nlongitude            -0.050859\nlatitude             -0.139584\nbedrooms_ratio       -0.256397\nName: median_house_value, dtype: float64\n\n\nThe new bedrooms_ratio attribute is much more correlated with the median house value than the total number of rooms or bedrooms. It’s a strong negative correlation, so it looks like houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district -obviously the larger the houses, the more expensive they are.\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.\n\n\n\n1.3.5 Prepare the Data for Machine Learning Algorithms\nLet’s revert to the original training set and separate the target (note that strat_train_set.drop() creates a copy of strat_train_set without the column, it doesn’t actually modify strat_train_set itself, unless you pass inplace=True):\n\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n\n\n1.3.5.1 Data Cleaning\nMost machine learning algorithms cannot work with missing features, so you’ll need to take care of these. For example, you noticed earlier that the total_bedrooms attribute has some missing values. You have three options to fix this:\n\nGet rid of the corresponding districts.\nGet rid of the whole attribute.\nSet the missing values to some value (zero, the mean, the median, etc.). This is called imputation.\n\nYou can accomplish these easily using the Pandas DataFrame’s dropna(), drop(), and fillna() methods:\nhousing.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\n\nhousing.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2\n\nmedian = housing[\"total_bedrooms\"].median()  # option 3\nhousing[\"total_bedrooms\"] = housing[\"total_bedrooms\"].fillna(median)\nIt seems best to go for option 3 since it is the least destructive, but instead of the preceding code, we can use a handy Scikit-Learn class: SimpleImputer. The benefit is that it will store the median value of each feature: this will make it possible to impute missing values not only on the training set, but also on the validation set, the test set, and any new data fed to the model. To use it, first you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\n\nSeparating out the numerical attributes to use the “median” strategy (as it cannot be calculated on text attributes like ocean_proximity):\n\nimport numpy as np\n\nhousing_num = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_num)\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputer?Documentation for SimpleImputeriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    \n\n\nThe imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but you cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n\nimputer.statistics_\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\n\nhousing_num.median()\n\nlongitude             -118.5100\nlatitude                34.2600\nhousing_median_age      29.0000\ntotal_rooms           2125.0000\ntotal_bedrooms         434.0000\npopulation            1167.0000\nhouseholds             408.0000\nmedian_income            3.5385\ndtype: float64\n\n\nTransforming the training set:\n\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\nhousing_tr.head()\n\n#from sklearn import set_config\n# set_config(transform_output=\"pandas\")  # scikit-learn &gt;= 1.2\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n13096\n-122.42\n37.80\n52.0\n3321.0\n1115.0\n1576.0\n1034.0\n2.0987\n\n\n14973\n-118.38\n34.14\n40.0\n1965.0\n354.0\n666.0\n357.0\n6.0876\n\n\n3785\n-121.98\n38.36\n33.0\n1083.0\n217.0\n562.0\n203.0\n2.4330\n\n\n14689\n-117.11\n33.75\n17.0\n4174.0\n851.0\n1845.0\n780.0\n2.2618\n\n\n20507\n-118.15\n33.77\n36.0\n4366.0\n1211.0\n1912.0\n1172.0\n3.5292\n\n\n\n\n\n\n\nMissing values can also be replaced with the mean value (strategy=\"mean\"), or with the most frequent value (strategy=\"most_frequent\"), or with a constant value (strategy=\"constant\", fill_value=…​). The last two strategies support non-numerical data.\nThere are also more powerful imputers available in the sklearn.impute package (both for numerical features only):\n\nKNNImputer replaces each missing value with the mean of the k-nearest neighbors’ values for that feature. The distance is based on all the available features.\nIterativeImputer trains a regression model per feature to predict the missing values based on all the other available features. It then trains the model again on the updated data, and repeats the process several times, improving the models and the replacement values at each iteration.\n\nNow let’s drop some outliers:\n\nfrom sklearn.ensemble import IsolationForest\n\nisolation_forest = IsolationForest(random_state=42)\noutlier_pred = isolation_forest.fit_predict(X)\noutlier_pred\n\narray([-1,  1,  1, ...,  1,  1,  1], shape=(16512,))\n\n\nIf you wanted to drop outliers, you would run the following code:\n\n# housing = housing.iloc[outlier_pred == 1]\n# housing_labels = housing_labels.iloc[outlier_pred == 1]\n\nFor you to understand how Isolation Forest works under the hood, the following articles are recommended:\n\nIsolation Forest Anomaly Detection — Identify Outliers\nIsolation Forest Guide: Explanation and Python Implementation\n\n\n\n1.3.5.2 Handling Text and Categorical Attributes\nNow let’s preprocess the categorical input feature, ocean_proximity:\n\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)\n\n\n\n\n\n\n\n\nocean_proximity\n\n\n\n\n13096\nNEAR BAY\n\n\n14973\n&lt;1H OCEAN\n\n\n3785\nINLAND\n\n\n14689\nINLAND\n\n\n20507\nNEAR OCEAN\n\n\n1286\nINLAND\n\n\n18078\n&lt;1H OCEAN\n\n\n4396\nNEAR BAY\n\n\n18031\n&lt;1H OCEAN\n\n\n6753\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n\n\nhousing_cat_encoded[:10]\n\narray([[3.],\n       [0.],\n       [1.],\n       [1.],\n       [4.],\n       [1.],\n       [0.],\n       [3.],\n       [0.],\n       [0.]])\n\n\n\nordinal_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n\n\nhousing_cat_1hot\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 16512 stored elements and shape (16512, 5)&gt;\n\n\nBy default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:\n\nhousing_cat_1hot.toarray()\n\narray([[0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.]], shape=(16512, 5))\n\n\nAlternatively, you can set sparse_output=False when creating the OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse_output=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)  # now a dense array\nhousing_cat_1hot\n\narray([[0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.]], shape=(16512, 5))\n\n\n\ncat_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\nPandas has a function called get_dummies(), which also converts each categorical feature into a one-hot representation, with one binary feature per category:\n\ndf_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\npd.get_dummies(df_test)\n\n\n\n\n\n\n\n\nocean_proximity_INLAND\nocean_proximity_NEAR BAY\n\n\n\n\n0\nTrue\nFalse\n\n\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\ncat_encoder.transform(df_test)\n\narray([[0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\n\ndf_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"&lt;2H OCEAN\", \"ISLAND\"]})\npd.get_dummies(df_test_unknown)\n\n\n\n\n\n\n\n\nocean_proximity_&lt;2H OCEAN\nocean_proximity_ISLAND\n\n\n\n\n0\nTrue\nFalse\n\n\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\ncat_encoder.handle_unknown = \"ignore\"\ncat_encoder.transform(df_test_unknown)\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.]])\n\n\n\ncat_encoder.feature_names_in_\n\narray(['ocean_proximity'], dtype=object)\n\n\n\ncat_encoder.get_feature_names_out()\n\narray(['ocean_proximity_&lt;1H OCEAN', 'ocean_proximity_INLAND',\n       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n       'ocean_proximity_NEAR OCEAN'], dtype=object)\n\n\n\ndf_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n                         columns=cat_encoder.get_feature_names_out(),\n                         index=df_test_unknown.index)\ndf_output\n\n\n\n\n\n\n\n\nocean_proximity_&lt;1H OCEAN\nocean_proximity_INLAND\nocean_proximity_ISLAND\nocean_proximity_NEAR BAY\nocean_proximity_NEAR OCEAN\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n1.3.5.3 Feature Scaling\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\nThere are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\nWarning: As with all estimators, it is important to fit the scalers to the training data only: never use fit() or fit_transform() for anything else than the training set. Once you have a trained scaler, you can then use it to transform() any other set, including the validation set, the test set, and new data.\nMin-max scaling (many people call this normalization) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value from all values, and dividing the results by the difference between the min and the max. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1 (e.g., neural networks work best with zero-mean inputs, so a range of –1 to 1 is preferable)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n\nStandardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. However, standardization is much less affected by outliers. For example, suppose a district has a median income equal to 100 (by mistake), instead of the usual 0–15. Min-max scaling to the 0–1 range would map this outlier down to 1 and it would crush all the other values down to 0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called StandardScaler for standardization:\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\n\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its logarithm may help. For example, the population feature roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. Figure 2-17 shows how much better this feature looks when you compute its log: it’s very close to a Gaussian distribution (i.e., bell-shaped).\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\nhousing[\"population\"].hist(ax=axs[0], bins=50)\nhousing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\naxs[0].set_xlabel(\"Population\")\naxs[1].set_xlabel(\"Log of population\")\naxs[0].set_ylabel(\"Number of districts\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWhat if we replace each value with its percentile?\n\npercentiles = [np.percentile(housing[\"median_income\"], p) for p in range(1, 100)]\nflattened_median_income = pd.cut(housing[\"median_income\"],\n                                 bins=[-np.inf] + percentiles + [np.inf],\n                                 labels=range(1, 100 + 1))\nflattened_median_income.hist(bins=50)\nplt.xlabel(\"Median income percentile\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n# Note: incomes below the 1st percentile are labeled 1, and incomes above the\n# 99th percentile are labeled 100. This is why the distribution below ranges\n# from 1 to 100 (not 0 to 100).\n\n\n\n\n\n\n\n\nWhen a feature has a multimodal distribution (i.e., with two or more clear peaks, called modes), such as the housing_median_age feature, it can also be helpful to bucketize it, but this time treating the bucket IDs as categories, rather than as numerical values. This means that the bucket indices must be encoded, for example using a OneHotEncoder (so you usually don’t want to use too many buckets).\nAnother approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF)—any function that depends only on the distance between the input value and a fixed point. The most commonly used RBF is the Gaussian RBF, whose output value decays exponentially as the input value moves away from the fixed point. For example, the Gaussian RBF similarity between the housing age x and 35 is given by the equation exp(–γ(x – 35)²). The hyperparameter γ (gamma) determines how quickly the similarity measure decays as x moves away from 35. Using Scikit-Learn’s rbf_kernel() function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:\n\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nage_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)\n\n# extra code – plots the age similarity feature along with the housing median age histogram\nages = np.linspace(housing[\"housing_median_age\"].min(),\n                   housing[\"housing_median_age\"].max(),\n                   500).reshape(-1, 1)\ngamma1 = 0.1\ngamma2 = 0.03\nrbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\nrbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n\nfig, ax1 = plt.subplots()\n\nax1.set_xlabel(\"Housing median age\")\nax1.set_ylabel(\"Number of districts\")\nax1.hist(housing[\"housing_median_age\"], bins=50)\n\nax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\ncolor = \"blue\"\nax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\nax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylabel(\"Age similarity\", color=color)\n\nplt.legend(loc=\"upper left\")\n\nplt.show()\n\n\n\n\n\n\n\n\nSo far we’ve only looked at the input features, but the target values may also need to be transformed. For example, if the target distribution has a heavy tail, you may choose to replace the target with its logarithm. But if you do, the regression model will now predict the log of the median house value, not the median house value itself. You will need to compute the exponential of the model’s prediction if you want the predicted median house value.\n\nhousing_labels\n\n13096    458300.0\n14973    483800.0\n3785     101700.0\n14689     96100.0\n20507    361800.0\n           ...   \n14207    500001.0\n13105     88800.0\n19301    148800.0\n19121    500001.0\n19888    233300.0\nName: median_house_value, Length: 16512, dtype: float64\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\ntarget_scaler = StandardScaler()\nscaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n\nmodel = LinearRegression()\nmodel.fit(housing[[\"median_income\"]], scaled_labels)\nsome_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n\nscaled_predictions = model.predict(some_new_data)\npredictions = target_scaler.inverse_transform(scaled_predictions)\npredictions\n\narray([[131997.15275877],\n       [299359.35844434],\n       [146023.37185694],\n       [138840.33653057],\n       [192016.61557639]])\n\n\n\nfrom sklearn.compose import TransformedTargetRegressor\n\nmodel = TransformedTargetRegressor(LinearRegression(),\n                                   transformer=StandardScaler())\nmodel.fit(housing[[\"median_income\"]], housing_labels)\npredictions = model.predict(some_new_data)\npredictions\n\narray([131997.15275877, 299359.35844434, 146023.37185694, 138840.33653057,\n       192016.61557639])\n\n\n\n\n1.3.5.4 Custom Transformers\n\nfrom sklearn.preprocessing import FunctionTransformer\n\nlog_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\nlog_pop = log_transformer.transform(housing[[\"population\"]])\nlog_pop.head()\n\n\n\n\n\n\n\n\npopulation\n\n\n\n\n13096\n7.362645\n\n\n14973\n6.501290\n\n\n3785\n6.331502\n\n\n14689\n7.520235\n\n\n20507\n7.555905\n\n\n\n\n\n\n\n\nrbf_transformer = FunctionTransformer(rbf_kernel,\n                                      kw_args=dict(Y=[[35.]], gamma=0.1))\nage_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\nage_simil_35\n\narray([[2.81118530e-13],\n       [8.20849986e-02],\n       [6.70320046e-01],\n       ...,\n       [9.55316054e-22],\n       [6.70320046e-01],\n       [3.03539138e-04]], shape=(16512, 1))\n\n\nFor example, here’s how to add a feature that will measure the geographic similarity between each district and San Francisco:\n\nsf_coords = 37.7749, -122.41\nsf_transformer = FunctionTransformer(rbf_kernel,\n                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\nsf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])\nsf_simil\n\narray([[0.999927  ],\n       [0.05258419],\n       [0.94864161],\n       ...,\n       [0.00388525],\n       [0.05038518],\n       [0.99868067]], shape=(16512, 1))\n\n\nCustom transformers are also useful to combine features. For example, here’s a FunctionTransformer that computes the ratio between the input features 0 and 1:\n\nratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\nratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n\narray([[0.5 ],\n       [0.75]])\n\n\nFunctionTransformer is very handy, but what if you would like your transformer to be trainable, learning some parameters in the fit() method and using them later in the transform() method? For this, you need to write a custom class. For example, here’s a custom transformer that acts much like the StandardScaler:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\n\nclass StandardScalerClone(BaseEstimator, TransformerMixin):\n    def __init__(self, with_mean=True):  # no *args or **kwargs!\n        self.with_mean = with_mean\n\n    def fit(self, X, y=None):  # y is required even though we don't use it\n        X = check_array(X)  # checks that X is an array with finite float values\n        self.mean_ = X.mean(axis=0)\n        self.scale_ = X.std(axis=0)\n        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n        return self  # always return self!\n\n    def transform(self, X):\n        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n        X = check_array(X)\n        assert self.n_features_in_ == X.shape[1]\n        if self.with_mean:\n            X = X - self.mean_\n        return X / self.scale_\n\nA custom transformer can (and often does) use other estimators in its implementation. For example, the following code demonstrates a custom transformer that uses a KMeans clusterer in the fit() method to identify the main clusters in the training data, and then uses rbf_kernel() in the transform() method to measure how similar each sample is to each cluster center:\n\nfrom sklearn.cluster import KMeans\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n\n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n                                           sample_weight=housing_labels)\n\nThis code creates a ClusterSimilarity transformer, setting the number of clusters to 10. Then it calls fit_transform() with the latitude and longitude of every district in the training set, weighting each district by its median house value. The transformer uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster. Let’s look at the first five rows, rounding to two decimal places:\n\nsimilarities[:5].round(2)\n\narray([[0.  , 0.98, 0.  , 0.  , 0.  , 0.  , 0.13, 0.55, 0.  , 0.56],\n       [0.64, 0.  , 0.11, 0.04, 0.  , 0.  , 0.  , 0.  , 0.99, 0.  ],\n       [0.  , 0.65, 0.  , 0.  , 0.01, 0.  , 0.49, 0.59, 0.  , 0.28],\n       [0.63, 0.  , 0.  , 0.52, 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  ],\n       [0.87, 0.  , 0.03, 0.14, 0.  , 0.  , 0.  , 0.  , 0.89, 0.  ]])\n\n\n\n# extra code\n\nhousing_renamed = housing.rename(columns={\n    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n    \"population\": \"Population\",\n    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\nhousing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n\nhousing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n                     c=\"Max cluster similarity\",\n                     cmap=\"jet\", colorbar=True,\n                     legend=True, sharex=False, figsize=(10, 7))\nplt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n         cluster_simil.kmeans_.cluster_centers_[:, 0],\n         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n         label=\"Cluster centers\")\nplt.legend(loc=\"upper right\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.3.5.5 Transformation Pipelines\nNow let’s build a pipeline to preprocess the numerical attributes:\n\nfrom sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n])\nnum_pipeline\n\nPipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('standardize', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('impute', ...), ('standardize', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\n\nfrom sklearn.pipeline import make_pipeline\n\nnum_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\nnum_pipeline\n\nPipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('simpleimputer', ...), ('standardscaler', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\nLet’s call the pipeline’s fit_transform() method and look at the output’s first five rows, rounded to two decimal places:\n\nhousing_num_prepared = num_pipeline.fit_transform(housing_num)\nhousing_num_prepared[:5].round(2)\n\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\n       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17],\n       [-1.2 ,  1.28,  0.35, -0.71, -0.76, -0.79, -0.78, -0.76],\n       [ 1.23, -0.88, -0.92,  0.7 ,  0.74,  0.38,  0.73, -0.85],\n       [ 0.71, -0.88,  0.59,  0.79,  1.6 ,  0.44,  1.76, -0.18]])\n\n\n\ndf_housing_num_prepared = pd.DataFrame(\n    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n    index=housing_num.index)\ndf_housing_num_prepared.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n13096\n-1.423037\n1.013606\n1.861119\n0.311912\n1.368167\n0.137460\n1.394812\n-0.936491\n\n\n14973\n0.596394\n-0.702103\n0.907630\n-0.308620\n-0.435925\n-0.693771\n-0.373485\n1.171942\n\n\n3785\n-1.203098\n1.276119\n0.351428\n-0.712240\n-0.760709\n-0.788768\n-0.775727\n-0.759789\n\n\n14689\n1.231216\n-0.884924\n-0.919891\n0.702262\n0.742306\n0.383175\n0.731375\n-0.850281\n\n\n20507\n0.711362\n-0.875549\n0.589800\n0.790125\n1.595753\n0.444376\n1.755263\n-0.180365\n\n\n\n\n\n\n\n\nnum_pipeline.steps\n\n[('simpleimputer', SimpleImputer(strategy='median')),\n ('standardscaler', StandardScaler())]\n\n\n\nnum_pipeline[1]\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\n\nnum_pipeline[:-1]\n\nPipeline(steps=[('simpleimputer', SimpleImputer(strategy='median'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('simpleimputer', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nnum_pipeline.named_steps[\"simpleimputer\"]\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputer?Documentation for SimpleImputeriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nnum_pipeline.set_params(simpleimputer__strategy=\"median\")\n\nPipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('simpleimputer', ...), ('standardscaler', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a ColumnTransformer. For example, the following ColumnTransformer will apply num_pipeline (the one we just defined) to the numerical attributes, and cat_pipeline to the categorical attribute:\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\ncat_attribs = [\"ocean_proximity\"]\n\ncat_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),\n    OneHotEncoder(handle_unknown=\"ignore\"))\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", cat_pipeline, cat_attribs),\n])\n\nSince listing all the column names is not very convenient, Scikit-Learn provides a make_column_selector class that you can use to automatically select all the features of a given type, such as numerical or categorical. You can pass a selector to the ColumnTransformer instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use make_column_transformer(), which chooses the names for you, just like make_pipeline() does. For example, the following code creates the same ColumnTransformer as earlier, except the transformers are automatically named \"pipeline-1\" and \"pipeline-2\" instead of \"num\" and \"cat\":\n\nfrom sklearn.compose import make_column_selector, make_column_transformer\n\npreprocessing = make_column_transformer(\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\n    (cat_pipeline, make_column_selector(dtype_include=object)),\n)\n\n\nhousing_prepared = preprocessing.fit_transform(housing)\n\n\nhousing_prepared_fr = pd.DataFrame(\n    housing_prepared,\n    columns=preprocessing.get_feature_names_out(),\n    index=housing.index)\nhousing_prepared_fr.head()\n\n\n\n\n\n\n\n\npipeline-1__longitude\npipeline-1__latitude\npipeline-1__housing_median_age\npipeline-1__total_rooms\npipeline-1__total_bedrooms\npipeline-1__population\npipeline-1__households\npipeline-1__median_income\npipeline-2__ocean_proximity_&lt;1H OCEAN\npipeline-2__ocean_proximity_INLAND\npipeline-2__ocean_proximity_ISLAND\npipeline-2__ocean_proximity_NEAR BAY\npipeline-2__ocean_proximity_NEAR OCEAN\n\n\n\n\n13096\n-1.423037\n1.013606\n1.861119\n0.311912\n1.368167\n0.137460\n1.394812\n-0.936491\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n14973\n0.596394\n-0.702103\n0.907630\n-0.308620\n-0.435925\n-0.693771\n-0.373485\n1.171942\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3785\n-1.203098\n1.276119\n0.351428\n-0.712240\n-0.760709\n-0.788768\n-0.775727\n-0.759789\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n14689\n1.231216\n-0.884924\n-0.919891\n0.702262\n0.742306\n0.383175\n0.731375\n-0.850281\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n20507\n0.711362\n-0.875549\n0.589800\n0.790125\n1.595753\n0.444376\n1.755263\n-0.180365\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nYou now want to create a single pipeline that will perform all the transformations we’ve experimented with up to now. Let’s recap what the pipeline will do and why:\n\nMissing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\nThe categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.\nA few ratio features will be computed and added: bedrooms_ratio, rooms_per_house, and people_per_house. Hopefully these will better correlate with the median house value, and thereby help the ML models.\nA few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\nFeatures with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.\nAll numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.\n\nThe code that builds the pipeline to do all of this should look familiar to you by now:\n\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n                                     StandardScaler())\n\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n\n\nhousing_prepared = preprocessing.fit_transform(housing)\nhousing_prepared.shape\n\n(16512, 24)\n\n\n\npreprocessing.get_feature_names_out()\n\narray(['bedrooms__ratio', 'rooms_per_house__ratio',\n       'people_per_house__ratio', 'log__total_bedrooms',\n       'log__total_rooms', 'log__population', 'log__households',\n       'log__median_income', 'geo__Cluster 0 similarity',\n       'geo__Cluster 1 similarity', 'geo__Cluster 2 similarity',\n       'geo__Cluster 3 similarity', 'geo__Cluster 4 similarity',\n       'geo__Cluster 5 similarity', 'geo__Cluster 6 similarity',\n       'geo__Cluster 7 similarity', 'geo__Cluster 8 similarity',\n       'geo__Cluster 9 similarity', 'cat__ocean_proximity_&lt;1H OCEAN',\n       'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',\n       'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',\n       'remainder__housing_median_age'], dtype=object)\n\n\n\nhousing_prepared_fr = pd.DataFrame(\n    housing_prepared,\n    columns=preprocessing.get_feature_names_out(),\n    index=housing.index)\nhousing_prepared_fr.head()\n\n\n\n\n\n\n\n\nbedrooms__ratio\nrooms_per_house__ratio\npeople_per_house__ratio\nlog__total_bedrooms\nlog__total_rooms\nlog__population\nlog__households\nlog__median_income\ngeo__Cluster 0 similarity\ngeo__Cluster 1 similarity\n...\ngeo__Cluster 6 similarity\ngeo__Cluster 7 similarity\ngeo__Cluster 8 similarity\ngeo__Cluster 9 similarity\ncat__ocean_proximity_&lt;1H OCEAN\ncat__ocean_proximity_INLAND\ncat__ocean_proximity_ISLAND\ncat__ocean_proximity_NEAR BAY\ncat__ocean_proximity_NEAR OCEAN\nremainder__housing_median_age\n\n\n\n\n13096\n1.846624\n-0.866027\n-0.330204\n1.324114\n0.637892\n0.456906\n1.310369\n-1.071522\n4.581829e-01\n1.241847e-14\n...\n8.489216e-04\n9.770322e-01\n2.382191e-08\n3.819126e-18\n0.0\n0.0\n0.0\n1.0\n0.0\n1.861119\n\n\n14973\n-0.508121\n0.024550\n-0.253616\n-0.252671\n-0.063576\n-0.711654\n-0.142030\n1.194712\n6.511495e-10\n9.579596e-01\n...\n5.614049e-27\n1.260964e-13\n1.103491e-01\n3.547610e-01\n1.0\n0.0\n0.0\n0.0\n0.0\n0.907630\n\n\n3785\n-0.202155\n-0.041193\n-0.051041\n-0.925266\n-0.859927\n-0.941997\n-0.913030\n-0.756981\n3.432506e-01\n4.261141e-15\n...\n5.641131e-03\n7.303265e-01\n2.508224e-08\n2.669659e-18\n0.0\n1.0\n0.0\n0.0\n0.0\n0.351428\n\n\n14689\n-0.149006\n-0.034858\n-0.141475\n0.952773\n0.943475\n0.670700\n0.925373\n-0.912253\n2.244844e-15\n2.704823e-01\n...\n5.913326e-35\n5.201263e-20\n1.712982e-03\n8.874598e-01\n0.0\n1.0\n0.0\n0.0\n0.0\n-0.919891\n\n\n20507\n0.963208\n-0.666554\n-0.306148\n1.437622\n1.003590\n0.719093\n1.481464\n0.034537\n1.090228e-11\n9.422206e-01\n...\n5.421817e-30\n1.048030e-15\n2.568824e-02\n5.279506e-01\n0.0\n0.0\n0.0\n0.0\n1.0\n0.589800\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\n\n1.3.6 Select and Train a Model\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapter 2 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, 3rd edition"
  },
  {
    "objectID": "IDS/1_1_fundamentals.html",
    "href": "IDS/1_1_fundamentals.html",
    "title": "1. Introduction to Data Science",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "IDS/1_1_fundamentals.html#basic-concepts-and-definitions",
    "href": "IDS/1_1_fundamentals.html#basic-concepts-and-definitions",
    "title": "1. Introduction to Data Science",
    "section": "1.1 Basic Concepts and Definitions",
    "text": "1.1 Basic Concepts and Definitions\n\n1.1.1 What is Data Science?\nThere’s a common joke that a data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician. (Whether or not it’s funny is up for debate.) Like all good jokes, though, it contains a kernel of truth. Some data scientists resemble statisticians who happen to code, while others look more like software engineers who dabble in analytics. Some are cutting-edge machine learning experts, while others focus primarily on business reporting or experimentation. There are PhDs with lengthy publication lists, and there are practitioners who have never opened an academic paper. In short, “data science” is notoriously hard to pin down, because almost any definition risks excluding someone who legitimately practices it.\nStill, the attempt at definition is worthwhile. At its core, we can say that data science is about extracting insight from messy, real-world data. The people who do this go by many titles—data scientist, data analyst, machine learning engineer, statistician, or business intelligence analyst. Even software engineers, researchers, and domain experts frequently find themselves doing the work of data science, whether or not it appears in their job description.\n\n\n\nVenn diagram showing overlap of skills in Data Science\n\n\nThe first diagram helps explain why data science is so difficult to define. It shows that data science sits at the intersection of computer science, mathematics & statistics, and domain knowledge. A successful data scientist combines all three: they need programming and software skills to manipulate data at scale, statistical and mathematical understanding to interpret results rigorously, and domain expertise to make those results meaningful in context. Add in machine learning, and you see how easily the field overlaps with multiple other disciplines.\n\n\n\nVenn diagram showing overlap of skills in Data Roles\n\n\nThe second diagram adds further clarity by mapping how different data-related roles emphasize different skill sets. Software engineers lean toward programming and system design, while data engineers focus on databases, pipelines, and scalable architectures. Data analysts center on reporting, visualization, and business understanding. Data scientists, meanwhile, sit in the overlap: they must balance coding ability, mathematical rigor, and communication skills to bridge technical work with actionable insight.\nThis overlap also underscores another key truth: data science is collaborative by nature. No one person masters the full breadth of skills in both diagrams. Data scientists usually work closely with engineers, analysts, domain experts, and business stakeholders. Technical proficiency alone is not enough—effective communication, critical thinking, and teamwork are equally essential to transform raw data into decisions that matter.\nIn this sense, data science is best seen not as a rigid job title, but as a mode of problem-solving that blends disciplines. The diversity of backgrounds among practitioners isn’t a weakness—it’s the very thing that makes the field so dynamic and impactful.\n\n\n1.1.1 Methodology of Data Science\nThe data science process is an iterative and interdisciplinary workflow that transforms raw data into actionable knowledge. While the exact steps may vary depending on the project, industry, or team, the process generally includes the following key stages:\n\nProblem Definition: The process begins with a clear understanding of the problem to be solved or the question to be answered. This step often requires close collaboration with domain experts and stakeholders to ensure the problem is framed in a way that is both meaningful and solvable with data. A poorly defined problem can derail the entire process, making this stage critical.\nData Collection: Relevant data must then be gathered from diverse sources, which may include relational databases, APIs, web scraping, sensors, surveys, or third-party datasets. At this stage, considerations such as data accessibility, privacy, and quality play a major role in determining the project’s feasibility.\nData Cleaning and Preprocessing: Raw data is rarely analysis-ready. This step involves addressing missing values, correcting inconsistencies, removing duplicates, and transforming variables into usable formats. Depending on the project, preprocessing may also involve scaling, normalization, text parsing, or image transformation. High-quality preprocessing is essential, since poor data hygiene undermines all subsequent stages.\nExploratory Data Analysis (EDA): Before formal modeling, data scientists perform EDA to uncover structure, detect anomalies, visualize distributions, and generate initial hypotheses. This step combines statistical methods, visualization tools, and domain knowledge to shape an intuitive understanding of the dataset and guide later decisions about modeling.\nFeature Engineering: Raw data rarely contains all the information needed for predictive modeling. Data scientists therefore create new variables or transform existing ones to capture relevant patterns. Examples include aggregating time-series signals, encoding categorical variables, or constructing interaction terms. Good feature engineering often determines whether a model succeeds or fails.\nModel Selection and Training: With features prepared, data scientists select suitable algorithms based on the problem type (classification, regression, clustering, etc.), the data structure, and performance trade-offs. The chosen models are then trained on the dataset, often using cross-validation to tune hyperparameters and prevent overfitting.\nModel Evaluation: Trained models must be rigorously evaluated using appropriate metrics, such as accuracy, precision, recall, F1-score, ROC-AUC, or mean squared error, depending on the task. Evaluation also includes robustness testing, fairness assessments, and comparisons with baseline methods to ensure the model adds genuine value.\nModel Deployment: Once validated, the model is deployed into a production environment where it can generate predictions on new, unseen data. Deployment may involve integrating the model into applications, dashboards, or automated decision systems, often requiring collaboration with software engineers and DevOps teams.\nMonitoring and Maintenance: The data science process does not end with deployment. Models degrade over time due to changing data distributions, user behavior, or external factors—a phenomenon known as data drift. Continuous monitoring, retraining, and updating ensure the model remains reliable and relevant.\n\nThis cycle is iterative rather than strictly linear. Insights from later stages (such as poor model performance) often lead to revisiting earlier steps (such as redefining features or collecting additional data). Just as the diagrams showed, successful data science requires more than technical execution: it depends on problem framing, communication, domain expertise, and collaboration across multiple roles."
  },
  {
    "objectID": "IDS/1_1_fundamentals.html#fundamentals-for-data-scientists",
    "href": "IDS/1_1_fundamentals.html#fundamentals-for-data-scientists",
    "title": "1. Introduction to Data Science",
    "section": "1.2 Fundamentals for Data Scientists",
    "text": "1.2 Fundamentals for Data Scientists\n\n1.2.1 Visualizing Data\nA fundamental part of the data scientist’s toolkit is data visualization. Although it is very easy to create visualizations, it’s much harder to produce good ones. There are two primary uses for data visualization:\n\nTo explore the data and find patterns, trends, and anomalies.\nTo communicate results to others.\n\nA wide variety of tools exist for visualizing data. We will be using matplotlib, which is a popular Python library for creating visualizations. It provides a wide range of tools and functions to create various types of plots and charts. To install Matplotlib, you can use pip, the Python package manager. Open your terminal or command prompt and run the following command:\npip install matplotlib\nWe will be using the matplotlib.pyplot module. In it’s simplest use, pyplot maintains an internal state in which you build up a visualization step by step. Once you’re done, you can save it with savefig or display it with show. For example, making simple plots is pretty simple:\n\nimport matplotlib.pyplot as plt\n\nyears = [1950,  1960,  1970,  1980,  1990,  2000,  2010]\ngdp = [300.2,  543.3,  1075.9,  2862.5,  5979.6,  10289.7,  14958.3]\n\n# create a line chart, years on x-axis, gdp on y-axis\nplt.plot(years,  gdp,  color='green',  marker='o',  linestyle='solid')\n\n# add a title\nplt.title(\"Nominal GDP\")\n\n# add a label to the y-axis\nplt.ylabel(\"Billions of $\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_gdp.png')\nplt.show()\n\n\n\n\n\n\n\n\nMaking plots that look publication-quality good is more complicated. There are many ways you can customize your charts with, for example, axis labels, line styles, and point markers. Rather than attempt a comprehensive treatment of these options, we’ll just use (and call attention to) some of them in our examples. Although we won’t be using much of this functionality, matplotlib is capable of producing complicated plots within plots, sophisticated formatting, and interactive visualizations. Check out the matplotlib documentation if you want to go deeper.\n\nBar Charts\nA bar chart is a good choice when you want to show how some quantity varies among some discrete set of items. For instance, the next figure shows how many Academy Awards were won by each of a variety of movies:\n\nmovies = [\"Annie Hall\", \"Ben-Hur\", \"Casablanca\", \"Gandhi\", \"West Side Story\"]\nnum_oscars = [5, 11, 3, 8, 10]\n\n# plot bars with left x-coordinates [0, 1, 2, 3, 4], heights [num_oscars]\nplt.bar(range(len(movies)), num_oscars)\n\nplt.title(\"My Favorite Movies\")     # add a title\nplt.ylabel(\"# of Academy Awards\")   # label the y-axis\n\n# label x-axis with movie names at bar centers\nplt.xticks(range(len(movies)), movies)\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_movies.png')\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart can also be a good choice for plotting histograms of bucketed numeric values, as in the next figure , in order to visually explore how the values are distributed:\n\nfrom collections import Counter\ngrades = [83, 95, 91, 87, 70, 0, 85, 82, 100, 67, 73, 77, 0]\n\n# Bucket grades by decile, but put 100 in with the 90s\nhistogram = Counter(min(grade // 10 * 10, 90) for grade in grades)\n\nplt.bar([x + 5 for x in histogram.keys()],  # Shift bars right by 5\n        list(histogram.values()),           # Give each bar its correct height\n        10,                                 # Give each bar a width of 8\n        edgecolor=(0, 0, 0))                # Black edges for each bar\n\nplt.axis((-5, 105, 0, 5))                  # x-axis from -5 to 105,\n                                           # y-axis from 0 to 5\n\nplt.xticks([10 * i for i in range(11)])    # x-axis labels at 0, 10, ..., 100\nplt.xlabel(\"Decile\")\nplt.ylabel(\"# of Students\")\nplt.title(\"Distribution of Exam 1 Grades\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_grades.png')\nplt.show()\n\n\n\n\n\n\n\n\nThe third argument to plt.bar specifies the bar width. Here we chose a width of 10, to fill the entire decile. We also shifted the bars right by 5, so that, for example, the “10” bar (which corresponds to the decile 10–20) would have its center at 15 and hence occupy the correct range. We also added a black edge to eacch bar to make them visually distinct.\nThe call to plt.axis indicates that we want the x-axis to range from –5 to 105 (just to leave a little space on the left and right), and that the y-axis should range from 0 to 5. And the call to plt.xticks puts x-axis labels at 0, 10, 20, …, 100. Be judicious when using plt.axis. When creating bar charts it is considered especially bad form for your y-axis not to start at 0, since this is an easy way to mislead people:\n\nmentions = [500, 505]\nyears = [2017, 2018]\n\nplt.bar(years, mentions, 0.8)\nplt.xticks(years)\nplt.ylabel(\"# of times I heard someone say 'data science'\")\n\n# if you don't do this, matplotlib will label the x-axis 0, 1\n# and then add a +2.013e3 off in the corner (bad matplotlib!)\nplt.ticklabel_format(useOffset=False)\n\n# misleading y-axis only shows the part above 500\nplt.axis((2016.5, 2018.5, 499, 506))\nplt.title(\"Look at the 'Huge' Increase!\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_misleading_y_axis.png')\nplt.show()\n\n\n\n\n\n\n\n\nHere we use more sensible axes, and it looks far less impressive:\n\nplt.bar(years, mentions, 0.8)\nplt.xticks(years)\nplt.ylabel(\"# of times I heard someone say 'data science'\")\nplt.ticklabel_format(useOffset=False)\n\nplt.axis((2016.5, 2018.5, 0, 550))\nplt.title(\"Not So Huge Anymore\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_non_misleading_y_axis.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLine Charts\nAs we saw already, we can make line charts using plt.plot. These are a good choice for showing trends:\n\nvariance = [1, 2, 4, 8, 16, 32, 64, 128, 256]\nbias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]\ntotal_error = [x + y for x, y in zip(variance, bias_squared)]\nxs = [i for i, _ in enumerate(variance)]\n\n# We can make multiple calls to plt.plot\n# to show multiple series on the same chart\nplt.plot(xs, variance,     'g-',  label='variance')    # green solid line\nplt.plot(xs, bias_squared, 'r-.', label='bias^2')      # red dot-dashed line\nplt.plot(xs, total_error,  'b:',  label='total error')  # blue dotted line\n\n# Because we've assigned labels to each series,\n# we can get a legend for free (loc=9 means \"top center\")\nplt.legend(loc=9)\nplt.xlabel(\"model complexity\")\nplt.xticks([])\nplt.title(\"The Bias-Variance Tradeoff\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_line_chart.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nScatterplots\nA scatterplot is the right choice for visualizing the relationship between two paired sets of data. For example, the next visualization illustrates the relationship between the number of friends your users have and the number of minutes they spend on the site every day:\n\nfriends = [ 70,  65,  72,  63,  71,  64,  60,  64,  67]\nminutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]\nlabels =  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n\nplt.scatter(friends, minutes)\n\n# label each point\nfor label, friend_count, minute_count in zip(labels, friends, minutes):\n    plt.annotate(label,\n        xy=(friend_count, minute_count), # Put the label with its point\n        xytext=(5, -5),                  # but slightly offset\n        textcoords='offset points')\n\nplt.title(\"Daily Minutes vs. Number of Friends\")\nplt.xlabel(\"# of friends\")\nplt.ylabel(\"daily minutes spent on the site\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_scatterplot.png')\nplt.show()\n\n\n\n\n\n\n\n\nIf you’re scattering comparable variables, you might get a misleading picture if you let matplotlib choose the scale.\n\ntest_1_grades = [ 99, 90, 85, 97, 80]\ntest_2_grades = [100, 85, 60, 90, 70]\n\nplt.scatter(test_1_grades, test_2_grades)\nplt.title(\"Axes Aren't Comparable\")\nplt.xlabel(\"test 1 grade\")\nplt.ylabel(\"test 2 grade\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_scatterplot_axes_not_comparable.png')\nplt.show()\n\n\n\n\n\n\n\n\nIf we include a call to plt.axis(\"equal\"), the plot more accurately shows that most of the variation occurs on test 2.\n\ntest_1_grades = [99, 90, 85, 97, 80]\ntest_2_grades = [100, 85, 60, 90, 70]\n\nplt.scatter(test_1_grades, test_2_grades)\nplt.title(\"Axes Are Comparable\")\nplt.axis(\"equal\")\nplt.xlabel(\"test 1 grade\")\nplt.ylabel(\"test 2 grade\")\n\n# save the plot as a PNG file\n# plt.savefig('im/viz_scatterplot_axes_comparable.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFor Further Exploration\n\nMatplotlib Gallery will give you a good idea of the sorts of things you can do with matplotlib (and how to do them).\nSeaborn is a higher-level visualization library built on top of matplotlib that provides a more user-friendly interface and additional features for creating attractive and informative statistical graphics.\nAltair is a declarative statistical visualization library for Python. It allows you to create complex visualizations with concise and expressive code.\nPlotly is another popular visualization library that allows for interactive and web-based visualizations. It provides a wide range of chart types and customization options.\nD3.js is a JavaScript library for creating interactive and dynamic visualizations on the web. While it is not a Python library, it is widely used for data visualization and can be integrated with Python using libraries like Bokeh or Dash.\n\n\n\n\n1.2.2 Linear Algebra\n\nVectors\nA vector is a mathematical object that represents both magnitude (size) and direction. In data science, we think of vectors as ordered lists of numbers that can represent data points, features, or measurements. Geometrically, a vector can be visualized as an arrow pointing from the origin to a specific point in space.\nVectors are fundamental building blocks in linear algebra and essential for:\n\nStoring and manipulating data\nRepresenting features in machine learning\nPerforming mathematical operations on datasets\nComputing distances and similarities between data points\n\nA vector has countless practical applications. In physics, a vector is often thought of as a direction and magnitude. In math, it is a direction and scale on an XY plane, kind of like a movement. In computer science, it is an array of numbers storing data. The computer science context is the one we will become the most familiar with as data science professionals.\nMathematical Notation:\nWe represent vectors using lowercase letters with arrows: \\(\\vec{v}\\), \\(\\vec{w}\\), or in column form:\n\\[\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\]\nTypes of Vectors\nRow Vector: Numbers arranged horizontally \\[\\vec{v} = [v_1, v_2, v_3, ..., v_n]\\]\nColumn Vector: Numbers arranged vertically \\[\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\]\nUnit Vector: A vector with magnitude (length) equal to 1\nZero Vector: A vector where all components are zero \\[\\vec{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\]\nTo emphasize again, the purpose of the vector is to visually represent a piece of data. If you have a data record for the square footage of a house 18,000 square feet and its valuation $260,000, we could express that as a vector [18000, 2600000], stepping 18,000 steps in the horizontal direction and 260,000 steps in the vertical direction.\nWe can declare a vector mathematically like this:\n\\(\\vec{v} = \\begin{bmatrix} 3 , 2 \\end{bmatrix}\\)\nWe can declare a vector using a simple Python collection, like a Python list:\n\nv = [3, 2]\nprint(v)\n\n[3, 2]\n\n\nHowever, when we start doing mathematical computations with vectors, especially when doing tasks like machine learning, we should probably use the numpy library as it is more efficient than plain Python. You can also use sympy to perform linear algebra operations; however, numpy is what you will likely use in practice so that is what we will mainly stick to. To declare a vector, you can use NumPy’s array() function and then can pass a collection of numbers to it\n\nimport numpy as np\n\n# Creating a basic 2D vector\nv = np.array([3, 3])\nprint(\"2D Vector:\")\nprint(f\"v = {v}\")\nprint(f\"Type: {type(v)}\")\nprint(f\"Shape: {v.shape}\")\nprint(f\"Dimension: {v.ndim}\")\n\n2D Vector:\nv = [3 3]\nType: &lt;class 'numpy.ndarray'&gt;\nShape: (2,)\nDimension: 1\n\n\nNote also vectors can exist on more than two dimensions. Next we declare a three- dimensional vector along axes \\(x\\), \\(y\\), and \\(z\\):\n\\(\\vec{v}  = \\begin{bmatrix} 4 , 1 , 2 \\end{bmatrix}\\)\nNaturally, we can express this three-dimensional vector in Python using three numeric values:\n\n# 3D vector\nv3d = np.array([4, 1, 2])\nprint(\"3D Vector:\")\nprint(f\"v3d = {v3d}\")\n\n3D Vector:\nv3d = [4 1 2]\n\n\nOr something like:\n\\(\\vec{v} = \\begin{bmatrix} 6 , 1 , 5 , 8 , 3 \\end{bmatrix}\\)\n\n# Higher dimensional vector (5D)\nv5d = np.array([6, 1, 5, 8, 3])\nprint(\"\\n5D Vector:\")\nprint(f\"v5d = {v5d}\")\n\n\n5D Vector:\nv5d = [6 1 5 8 3]\n\n\n\nVector properties and operations\n\n# Calculate vector magnitude (length)\nv = np.array([3, 4])\nmagnitude = np.linalg.norm(v)\nprint(f\"Vector v = {v}\")\nprint(f\"Magnitude of v = {magnitude}\")\n\n# Create unit vector (normalize)\nunit_v = v / magnitude\nprint(f\"Unit vector = {unit_v}\")\nprint(f\"Unit vector magnitude = {np.linalg.norm(unit_v)}\")\n\n# Zero vector\nzero_vector = np.zeros(3)\nprint(f\"Zero vector: {zero_vector}\")\n\n# Vector of ones\nones_vector = np.ones(4)\nprint(f\"Ones vector: {ones_vector}\")\n\nVector v = [3 4]\nMagnitude of v = 5.0\nUnit vector = [0.6 0.8]\nUnit vector magnitude = 1.0\nZero vector: [0. 0. 0.]\nOnes vector: [1. 1. 1. 1.]\n\n\n\n# Vector addition and subtraction\n\nv1 = np.array([2, 3, 1])\nv2 = np.array([1, -1, 4])\n\n# Addition\nv_sum = v1 + v2\nprint(f\"v1 = {v1}\")\nprint(f\"v2 = {v2}\")\nprint(f\"v1 + v2 = {v_sum}\")\n\n# Subtraction\nv_diff = v1 - v2\nprint(f\"v1 - v2 = {v_diff}\")\n\n# Element-wise operations\nv_mult = v1 * v2  # Element-wise multiplication\nprint(f\"v1 * v2 (element-wise) = {v_mult}\")\n\nv1 = [2 3 1]\nv2 = [ 1 -1  4]\nv1 + v2 = [3 2 5]\nv1 - v2 = [ 1  4 -3]\nv1 * v2 (element-wise) = [ 2 -3  4]\n\n\n\n# Scalar multiplication (scaling)\n\nv = np.array([2, -3, 1])\nscalar = 2.5\n\n# Scale the vector\nscaled_v = scalar * v\nprint(f\"Original vector: {v}\")\nprint(f\"Scalar: {scalar}\")\nprint(f\"Scaled vector: {scaled_v}\")\n\n# Scaling changes magnitude but not direction (unless scalar is negative)\nprint(f\"Original magnitude: {np.linalg.norm(v):.2f}\")\nprint(f\"Scaled magnitude: {np.linalg.norm(scaled_v):.2f}\")\nprint(f\"Magnitude ratio: {np.linalg.norm(scaled_v) / np.linalg.norm(v):.2f}\")\n\nOriginal vector: [ 2 -3  1]\nScalar: 2.5\nScaled vector: [ 5.  -7.5  2.5]\nOriginal magnitude: 3.74\nScaled magnitude: 9.35\nMagnitude ratio: 2.50\n\n\n\n# Dot product (scalar product)\n\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\n\n# Calculate dot product\ndot_product = np.dot(v1, v2)\nprint(f\"v1 = {v1}\")\nprint(f\"v2 = {v2}\")\nprint(f\"Dot product v1 · v2 = {dot_product}\")\n\n# Alternative syntax\ndot_product_alt = v1 @ v2  # Matrix multiplication operator\nprint(f\"Alternative syntax: v1 @ v2 = {dot_product_alt}\")\n\n# Geometric interpretation: dot product relates to angle between vectors\nangle_cos = dot_product / (np.linalg.norm(v1) * np.linalg.norm(v2))\nangle_rad = np.arccos(angle_cos)\nangle_deg = np.degrees(angle_rad)\nprint(f\"Angle between vectors: {angle_deg:.2f} degrees\")\n\nv1 = [1 2 3]\nv2 = [4 5 6]\nDot product v1 · v2 = 32\nAlternative syntax: v1 @ v2 = 32\nAngle between vectors: 12.93 degrees\n\n\n\n# Vector indexing and slicing\n\ndata = np.array([10, 20, 30, 40, 50, 60])\nprint(f\"Original vector: {data}\")\n\n# Access single elements\nprint(f\"First element: {data[0]}\")\nprint(f\"Last element: {data[-1]}\")\nprint(f\"Third element: {data[2]}\")\n\n# Slicing\nprint(f\"First three elements: {data[:3]}\")\nprint(f\"Last two elements: {data[-2:]}\")\nprint(f\"Middle elements: {data[2:4]}\")\nprint(f\"Every second element: {data[::2]}\")\n\n# Boolean indexing\nmask = data &gt; 30\nprint(f\"Elements greater than 30: {data[mask]}\")\n\n# Find indices where condition is true\nindices = np.where(data &gt; 30)\nprint(f\"Indices where data &gt; 30: {indices[0]}\")\n\nOriginal vector: [10 20 30 40 50 60]\nFirst element: 10\nLast element: 60\nThird element: 30\nFirst three elements: [10 20 30]\nLast two elements: [50 60]\nMiddle elements: [30 40]\nEvery second element: [10 30 50]\nElements greater than 30: [40 50 60]\nIndices where data &gt; 30: [3 4 5]\n\n\n\n# Creating special vectors\n\n# Range-based vectors\nrange_vector = np.arange(0, 10, 2)  # Start, stop, step\nprint(f\"Range vector: {range_vector}\")\n\n# Linearly spaced vectors\nlinear_space = np.linspace(0, 1, 5)  # Start, stop, number of points\nprint(f\"Linear space: {linear_space}\")\n\n# Random vectors\nnp.random.seed(42)  # For reproducible results\nrandom_vector = np.random.randint(1, 10, size=5)\nprint(f\"Random integers: {random_vector}\")\n\nrandom_normal = np.random.normal(0, 1, 5)  # Mean=0, std=1, size=5\nprint(f\"Random normal: {random_normal}\")\n\n# Vector from list\nlist_data = [1.5, 2.7, 3.1, 4.8, 5.2]\nvector_from_list = np.array(list_data)\nprint(f\"From list: {vector_from_list}\")\n\nRange vector: [0 2 4 6 8]\nLinear space: [0.   0.25 0.5  0.75 1.  ]\nRandom integers: [7 4 8 5 7]\nRandom normal: [-0.91682684 -0.12414718 -2.01096289 -0.49280342  0.39257975]\nFrom list: [1.5 2.7 3.1 4.8 5.2]\n\n\nManipulating Data Is Manipulating Vectors: Every data operation can be thought of in terms of vectors, even simple averages. Take scaling, for example. Let’s say we were trying to get the average house value and average square footage for an entire neighborhood. We would add the vectors together to combine their value and square footage respectively, giving us one giant vector containing both total value and total square footage. We then scale down the vector by dividing by the number of houses N , which really is multiplying by 1/ N . We now have a vector containing the average house value and average square footage.\n\n# Vector operations for data analysis\n\n# Sales data over 6 months\nsales_q1 = np.array([10000, 12000, 15000])  # Jan, Feb, Mar\nsales_q2 = np.array([18000, 20000, 22000])  # Apr, May, Jun\n\nprint(\"Sales Data:\")\nprint(f\"Q1 Sales: {sales_q1}\")\nprint(f\"Q2 Sales: {sales_q2}\")\n\n# Calculate quarterly growth\ngrowth = sales_q2 - sales_q1\nprint(f\"Monthly Growth: {growth}\")\n\n# Calculate growth percentage\ngrowth_percentage = (growth / sales_q1) * 100\nprint(f\"Growth Percentage: {growth_percentage}%\")\n\n# Total sales for each quarter\nq1_total = np.sum(sales_q1)\nq2_total = np.sum(sales_q2)\nprint(f\"\\nQ1 Total: ${q1_total:,}\")\nprint(f\"Q2 Total: ${q2_total:,}\")\n\n# Average monthly sales\nq1_avg = np.mean(sales_q1)\nq2_avg = np.mean(sales_q2)\nprint(f\"Q1 Average: ${q1_avg:,.0f}\")\nprint(f\"Q2 Average: ${q2_avg:,.0f}\")\n\nSales Data:\nQ1 Sales: [10000 12000 15000]\nQ2 Sales: [18000 20000 22000]\nMonthly Growth: [8000 8000 7000]\nGrowth Percentage: [80.         66.66666667 46.66666667]%\n\nQ1 Total: $37,000\nQ2 Total: $60,000\nQ1 Average: $12,333\nQ2 Average: $20,000\n\n\n\n\n\nMatrices\nA matrix is a rectangular array of numbers arranged in rows and columns. Matrices are fundamental structures in linear algebra and serve as powerful tools for representing and manipulating data in data science. Think of a matrix as a collection of vectors arranged side by side, or as a way to organize data in a tabular format.\nMathematically, we represent a matrix using capital letters like \\(A\\), \\(B\\), or \\(M\\). A matrix with \\(m\\) rows and \\(n\\) columns is called an \\(m \\times n\\) matrix:\n\\[A = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\\]\nFor example, a \\(2 \\times 3\\) matrix looks like:\n\\[A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\\]\nIn data science, matrices are everywhere. A dataset with multiple features for multiple observations is essentially a matrix where each row represents an observation and each column represents a feature.\nTypes of Matrices\nSquare Matrix: A matrix where the number of rows equals the number of columns (\\(n \\times n\\)).\n\\[A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\\]\nIdentity Matrix: A special square matrix with 1s on the diagonal and 0s elsewhere. It’s the matrix equivalent of the number 1 in multiplication.\n\\[I = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\]\nZero Matrix: A matrix where all elements are zero.\n\\[O = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\\]\nDiagonal Matrix: A square matrix where all non-diagonal elements are zero.\n\\[D = \\begin{bmatrix}\n3 & 0 & 0 \\\\\n0 & 5 & 0 \\\\\n0 & 0 & 2\n\\end{bmatrix}\\]\nLet’s see how to work with these matrices using NumPy:\n\n# Creating a basic matrix (2x3)\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"Shape: {A.shape}\")\nprint(f\"Dimension: {A.ndim}\")\nprint(f\"Type: {type(A)}\")\n\nMatrix A:\n[[1 2 3]\n [4 5 6]]\nShape: (2, 3)\nDimension: 2\nType: &lt;class 'numpy.ndarray'&gt;\n\n\n\n# Creating a square matrix (3x3)\nB = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\nprint(\"Square Matrix B:\")\nprint(B)\nprint(f\"Shape: {B.shape}\")\n\nSquare Matrix B:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nShape: (3, 3)\n\n\n\n# Creating an identity matrix\nI = np.eye(3)  # 3x3 identity matrix\nprint(\"Identity Matrix:\")\nprint(I)\n\nIdentity Matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n# Creating a zero matrix\nzeros = np.zeros((2, 4))  # 2x4 zero matrix\nprint(\"Zero Matrix:\")\nprint(zeros)\n\nZero Matrix:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n\n\n# Creating a diagonal matrix\ndiagonal_values = [3, 5, 2]\nD = np.diag(diagonal_values)\nprint(\"Diagonal Matrix:\")\nprint(D)\n\nDiagonal Matrix:\n[[3 0 0]\n [0 5 0]\n [0 0 2]]\n\n\nMatrix operations\n\n# Matrix addition (matrices must have the same dimensions)\nA1 = np.array([[1, 2], \n               [3, 4]])\nA2 = np.array([[5, 6], \n               [7, 8]])\n\nmatrix_sum = A1 + A2\nprint(\"Matrix Addition:\")\nprint(f\"A1 + A2 = \\n{matrix_sum}\")\n\nMatrix Addition:\nA1 + A2 = \n[[ 6  8]\n [10 12]]\n\n\n\n# Scalar multiplication\nscalar = 3\nscaled_matrix = scalar * A1\nprint(\"Scalar Multiplication:\")\nprint(f\"3 * A1 = \\n{scaled_matrix}\")\n\nScalar Multiplication:\n3 * A1 = \n[[ 3  6]\n [ 9 12]]\n\n\n\n# Matrix transpose (swap rows and columns)\noriginal = np.array([[1, 2, 3],\n                     [4, 5, 6]])\ntransposed = original.T  # or np.transpose(original)\nprint(\"Original Matrix:\")\nprint(original)\nprint(\"Transposed Matrix:\")\nprint(transposed)\n\nOriginal Matrix:\n[[1 2 3]\n [4 5 6]]\nTransposed Matrix:\n[[1 4]\n [2 5]\n [3 6]]\n\n\n\n# Accessing matrix elements\nmatrix = np.array([[10, 20, 30],\n                   [40, 50, 60],\n                   [70, 80, 90]])\n\nprint(\"Full matrix:\")\nprint(matrix)\nprint(f\"Element at row 1, column 2: {matrix[1, 2]}\")  # Remember: 0-indexed\nprint(f\"First row: {matrix[0, :]}\")\nprint(f\"Second column: {matrix[:, 1]}\")\nprint(f\"Submatrix (first 2x2): \\n{matrix[:2, :2]}\")\n\nFull matrix:\n[[10 20 30]\n [40 50 60]\n [70 80 90]]\nElement at row 1, column 2: 60\nFirst row: [10 20 30]\nSecond column: [20 50 80]\nSubmatrix (first 2x2): \n[[10 20]\n [40 50]]\n\n\n\n# Practical example: Student grades matrix\n# Rows represent students, columns represent subjects\ngrades = np.array([[85, 92, 78, 90],  # Student 1: Math, Science, English, History\n                   [79, 85, 88, 92],  # Student 2\n                   [92, 88, 85, 87],  # Student 3\n                   [88, 90, 92, 85]]) # Student 4\n\nsubjects = ['Math', 'Science', 'English', 'History']\nstudents = ['Alice', 'Bob', 'Charlie', 'Diana']\n\nprint(\"Student Grades Matrix:\")\nprint(grades)\nprint(f\"Shape: {grades.shape} (4 students, 4 subjects)\")\n\n# Calculate average grade per student\nstudent_averages = np.mean(grades, axis=1)  # axis=1 means across columns\nprint(\"\\nAverage grades per student:\")\nfor i, student in enumerate(students):\n    print(f\"{student}: {student_averages[i]:.2f}\")\n\n# Calculate average grade per subject\nsubject_averages = np.mean(grades, axis=0)  # axis=0 means across rows\nprint(\"\\nAverage grades per subject:\")\nfor i, subject in enumerate(subjects):\n    print(f\"{subject}: {subject_averages[i]:.2f}\")\n\nStudent Grades Matrix:\n[[85 92 78 90]\n [79 85 88 92]\n [92 88 85 87]\n [88 90 92 85]]\nShape: (4, 4) (4 students, 4 subjects)\n\nAverage grades per student:\nAlice: 86.25\nBob: 86.00\nCharlie: 88.00\nDiana: 88.75\n\nAverage grades per subject:\nMath: 86.00\nScience: 88.75\nEnglish: 85.75\nHistory: 88.50\n\n\n\n\nDeterminants\nA determinant is a scalar value that can be calculated from a square matrix. It provides important information about the matrix’s properties and has numerous applications in linear algebra, geometry, and data science. The determinant tells us whether a matrix is invertible, how it transforms areas and volumes, and is crucial for solving systems of linear equations.\nMathematical Notation:\nFor a square matrix \\(A\\), we denote its determinant as \\(\\det(A)\\) or \\(|A|\\).\nKey Properties of Determinants:\n\nOnly square matrices have determinants\nIf \\(\\det(A) = 0\\), the matrix is singular (non-invertible)\nIf \\(\\det(A) \\neq 0\\), the matrix is invertible\nThe determinant represents the scaling factor of the linear transformation\nFor a 2D matrix, it represents the area scaling factor; for 3D, the volume scaling factor\n\nGeometric Interpretation:\nThe absolute value of the determinant tells us how much the matrix scales areas (in 2D) or volumes (in 3D). A negative determinant indicates that the transformation reverses orientation.\nApplications in Data Science:\n\nChecking if a system of equations has a unique solution\nComputing matrix inverses\nPrincipal Component Analysis (PCA)\nMeasuring multicollinearity in regression analysis\nVolume calculations in high-dimensional spaces\n\n\n# Calculate determinant of a 2x2 matrix\nA_2x2 = np.array([[1, 2],\n                  [2, 5]])\n\ndet_A = np.linalg.det(A_2x2)\nprint(\"2x2 Matrix:\")\nprint(A_2x2)\nprint(f\"Determinant: {det_A}\")\n\n# Manual calculation for 2x2: ad - bc\nmanual_det = A_2x2[0,0] * A_2x2[1,1] - A_2x2[0,1] * A_2x2[1,0]\nprint(f\"Manual calculation: {manual_det}\")\n\n2x2 Matrix:\n[[1 2]\n [2 5]]\nDeterminant: 1.0\nManual calculation: 1\n\n\n\n# Calculate determinant of a 3x3 matrix\nA_3x3 = np.array([[1, 2, 3],\n                  [0, 1, 4],\n                  [5, 6, 0]])\n\ndet_A_3x3 = np.linalg.det(A_3x3)\nprint(\"3x3 Matrix:\")\nprint(A_3x3)\nprint(f\"Determinant: {det_A_3x3:.2f}\")\n\n3x3 Matrix:\n[[1 2 3]\n [0 1 4]\n [5 6 0]]\nDeterminant: 1.00\n\n\n\n# Special matrices and their determinants\n\n# Identity matrix - determinant is always 1\nI = np.eye(3)\nprint(\"Identity Matrix:\")\nprint(I)\nprint(f\"Determinant: {np.linalg.det(I)}\")\n\n# Diagonal matrix - determinant is product of diagonal elements\nD = np.diag([4, 4, 2])\nprint(\"\\nDiagonal Matrix:\")\nprint(D)\nprint(f\"Determinant: {np.linalg.det(D)}\")\nprint(f\"Product of diagonal elements: {4 * 4 * 2}\")\n\nIdentity Matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nDeterminant: 1.0\n\nDiagonal Matrix:\n[[4 0 0]\n [0 4 0]\n [0 0 2]]\nDeterminant: 32.0\nProduct of diagonal elements: 32\n\n\n\n# Singular matrix (determinant = 0)\nsingular_matrix = np.array([[1, 2, 3],\n                           [2, 4, 6],\n                           [1, 1, 1]])\n\ndet_singular = np.linalg.det(singular_matrix)\nprint(\"Singular Matrix (rows are linearly dependent):\")\nprint(singular_matrix)\nprint(f\"Determinant: {det_singular:.10f}\")  # Should be very close to 0\n\n# This matrix is not invertible\ntry:\n    inverse = np.linalg.inv(singular_matrix)\nexcept np.linalg.LinAlgError as e:\n    print(f\"Cannot invert: {e}\")\n\nSingular Matrix (rows are linearly dependent):\n[[1 2 3]\n [2 4 6]\n [1 1 1]]\nDeterminant: 0.0000000000\nCannot invert: Singular matrix\n\n\n\n# Properties of determinants\n\nA = np.array([[2, 1],\n              [3, 4]])\n\nB = np.array([[1, 2],\n              [0, 3]])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"det(A) = {np.linalg.det(A)}\")\n\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(f\"det(B) = {np.linalg.det(B)}\")\n\n# Property: det(AB) = det(A) * det(B)\nAB = A @ B\nprint(f\"\\nA @ B:\")\nprint(AB)\nprint(f\"det(AB) = {np.linalg.det(AB)}\")\nprint(f\"det(A) * det(B) = {np.linalg.det(A) * np.linalg.det(B)}\")\n\n# Property: det(A^T) = det(A)\nA_transpose = A.T\nprint(f\"\\ndet(A^T) = {np.linalg.det(A_transpose)}\")\nprint(f\"det(A) = {np.linalg.det(A)}\")\n\nMatrix A:\n[[2 1]\n [3 4]]\ndet(A) = 5.000000000000001\n\nMatrix B:\n[[1 2]\n [0 3]]\ndet(B) = 3.0000000000000004\n\nA @ B:\n[[ 2  7]\n [ 3 18]]\ndet(AB) = 15.0\ndet(A) * det(B) = 15.000000000000005\n\ndet(A^T) = 5.000000000000001\ndet(A) = 5.000000000000001\n\n\n\n# Practical example: Checking system solvability\n\n# System of equations: \n# 2x + 3y = 7\n# 4x + 6y = 14\n\n# Coefficient matrix\ncoeff_matrix = np.array([[2, 3],\n                        [4, 6]])\n\n# Constants vector\nconstants = np.array([7, 14])\n\ndet_coeff = np.linalg.det(coeff_matrix)\nprint(\"Coefficient Matrix:\")\nprint(coeff_matrix)\nprint(f\"Determinant: {det_coeff}\")\n\nif abs(det_coeff) &lt; 1e-10:  # Close to zero\n    print(\"System has no unique solution (infinite solutions or no solution)\")\n    # Check if system is consistent\n    augmented = np.column_stack([coeff_matrix, constants])\n    rank_coeff = np.linalg.matrix_rank(coeff_matrix)\n    rank_augmented = np.linalg.matrix_rank(augmented)\n    \n    if rank_coeff == rank_augmented:\n        print(\"System has infinite solutions\")\n    else:\n        print(\"System has no solution\")\nelse:\n    print(\"System has a unique solution\")\n    solution = np.linalg.solve(coeff_matrix, constants)\n    print(f\"Solution: x = {solution[0]}, y = {solution[1]}\")\n\nCoefficient Matrix:\n[[2 3]\n [4 6]]\nDeterminant: 0.0\nSystem has no unique solution (infinite solutions or no solution)\nSystem has infinite solutions\n\n\n\n\nEigenvectors and Eigenvalues\nEigenvectors and eigenvalues are fundamental concepts in linear algebra that reveal the intrinsic properties of linear transformations. They are crucial for understanding how matrices act on vectors and have profound applications in data science, particularly in dimensionality reduction, principal component analysis (PCA), and machine learning.\nMathematical Definition:\nFor a square matrix \\(A\\), an eigenvector \\(\\vec{v}\\) is a non-zero vector that, when multiplied by \\(A\\), results in a scalar multiple of itself:\n\\[A\\vec{v} = \\lambda\\vec{v}\\]\nWhere: - \\(\\vec{v}\\) is the eigenvector (direction that doesn’t change) - \\(\\lambda\\) (lambda) is the eigenvalue (scaling factor) - \\(A\\) is the square matrix\nGeometric Interpretation:\nWhen a matrix transforms a vector, most vectors change both direction and magnitude. However, eigenvectors are special - they only change in magnitude (scaled by the eigenvalue) but maintain their direction. Think of it as finding the “natural” directions of a transformation.\nKey Properties:\n\nA matrix can have multiple eigenvector-eigenvalue pairs\nEigenvectors corresponding to different eigenvalues are orthogonal\nThe eigenvalues tell us how much the matrix stretches or shrinks in each eigenvector direction\nIf eigenvalue λ &gt; 1: stretching, if 0 &lt; λ &lt; 1: shrinking, if λ &lt; 0: reflection\n\nApplications in Data Science:\n\nPrincipal Component Analysis (PCA): Finding directions of maximum variance\nDimensionality Reduction: Identifying most important features\nGoogle PageRank: Ranking web pages using eigenvectors\nImage Compression: Using eigenfaces for face recognition\nStability Analysis: Determining system behavior in dynamic models\nSpectral Clustering: Graph-based clustering algorithms\n\n\n# Basic example: Finding eigenvalues and eigenvectors\nA = np.array([[3, 1],\n              [0, 2]])\n\n# Calculate eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"\\nEigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors:\\n{eigenvectors}\")\n\n# Verify the eigenvalue equation: Av = λv\nfor i in range(len(eigenvalues)):\n    v = eigenvectors[:, i]  # i-th eigenvector\n    lambda_val = eigenvalues[i]  # i-th eigenvalue\n    \n    Av = A @ v\n    lambda_v = lambda_val * v\n    \n    print(f\"\\nEigenvector {i+1}: {v}\")\n    print(f\"A * v = {Av}\")\n    print(f\"λ * v = {lambda_v}\")\n    print(f\"Are they equal? {np.allclose(Av, lambda_v)}\")\n\nMatrix A:\n[[3 1]\n [0 2]]\n\nEigenvalues: [3. 2.]\nEigenvectors:\n[[ 1.         -0.70710678]\n [ 0.          0.70710678]]\n\nEigenvector 1: [1. 0.]\nA * v = [3. 0.]\nλ * v = [3. 0.]\nAre they equal? True\n\nEigenvector 2: [-0.70710678  0.70710678]\nA * v = [-1.41421356  1.41421356]\nλ * v = [-1.41421356  1.41421356]\nAre they equal? True\n\n\n\n# Symmetric matrix example (real eigenvalues, orthogonal eigenvectors)\nS = np.array([[4, 2],\n              [2, 3]])\n\neigenvals_s, eigenvecs_s = np.linalg.eig(S)\n\nprint(\"Symmetric Matrix S:\")\nprint(S)\nprint(f\"Eigenvalues: {eigenvals_s}\")\nprint(f\"Eigenvectors:\\n{eigenvecs_s}\")\n\n# Check orthogonality of eigenvectors\ndot_product = np.dot(eigenvecs_s[:, 0], eigenvecs_s[:, 1])\nprint(f\"\\nDot product of eigenvectors: {dot_product:.10f}\")\nprint(f\"Eigenvectors are orthogonal: {abs(dot_product) &lt; 1e-10}\")\n\nSymmetric Matrix S:\n[[4 2]\n [2 3]]\nEigenvalues: [5.56155281 1.43844719]\nEigenvectors:\n[[ 0.78820544 -0.61541221]\n [ 0.61541221  0.78820544]]\n\nDot product of eigenvectors: 0.0000000000\nEigenvectors are orthogonal: True\n\n\n\n# 3D example\nA_3d = np.array([[6, -2, 2],\n                 [-2, 3, -1],\n                 [2, -1, 3]])\n\neigenvals_3d, eigenvecs_3d = np.linalg.eig(A_3d)\n\nprint(\"3D Matrix:\")\nprint(A_3d)\nprint(f\"\\nEigenvalues: {eigenvals_3d}\")\nprint(f\"\\nEigenvectors:\")\nfor i in range(len(eigenvals_3d)):\n    print(f\"λ{i+1} = {eigenvals_3d[i]:.3f}, v{i+1} = {eigenvecs_3d[:, i]}\")\n\n3D Matrix:\n[[ 6 -2  2]\n [-2  3 -1]\n [ 2 -1  3]]\n\nEigenvalues: [8. 2. 2.]\n\nEigenvectors:\nλ1 = 8.000, v1 = [ 0.81649658 -0.40824829  0.40824829]\nλ2 = 2.000, v2 = [-0.57735027 -0.57735027  0.57735027]\nλ3 = 2.000, v3 = [-0.11547005  0.57735027  0.80829038]\n\n\n\n# Practical example: Principal Component Analysis (PCA) basics\nnp.random.seed(42)\n\n# Generate correlated 2D data\nn_samples = 100\nx1 = np.random.normal(0, 2, n_samples)\nx2 = 1.5 * x1 + np.random.normal(0, 1, n_samples)\ndata = np.column_stack([x1, x2])\n\nprint(\"Original data shape:\", data.shape)\n\n# Center the data\ndata_centered = data - np.mean(data, axis=0)\n\n# Calculate covariance matrix\ncov_matrix = np.cov(data_centered.T)\nprint(\"Covariance Matrix:\")\nprint(cov_matrix)\n\n# Find principal components (eigenvectors of covariance matrix)\neigenvals_pca, eigenvecs_pca = np.linalg.eig(cov_matrix)\n\n# Sort by eigenvalue (descending order)\nidx = np.argsort(eigenvals_pca)[::-1]\neigenvals_pca = eigenvals_pca[idx]\neigenvecs_pca = eigenvecs_pca[:, idx]\n\nprint(f\"\\nPrincipal Component eigenvalues: {eigenvals_pca}\")\nprint(f\"Principal Component eigenvectors:\\n{eigenvecs_pca}\")\n\n# The first eigenvector is the direction of maximum variance\nprint(f\"\\nFirst PC explains {eigenvals_pca[0]/(eigenvals_pca[0]+eigenvals_pca[1])*100:.1f}% of variance\")\nprint(f\"Second PC explains {eigenvals_pca[1]/(eigenvals_pca[0]+eigenvals_pca[1])*100:.1f}% of variance\")\n\nOriginal data shape: (100, 2)\nCovariance Matrix:\n[[3.29907957 4.71231098]\n [4.71231098 7.62348838]]\n\nPrincipal Component eigenvalues: [10.64597323  0.27659473]\nPrincipal Component eigenvectors:\n[[-0.53989052 -0.84173525]\n [-0.84173525  0.53989052]]\n\nFirst PC explains 97.5% of variance\nSecond PC explains 2.5% of variance\n\n\n\n\nFor Further Exploration\n\n3Blue1Brown’s Linear Algebra Series provides intuitive visual explanations of linear algebra concepts, including eigenvectors and eigenvalues.\n\n\n\n\n1.2.3 Probability and Statistics\nProbability and statistics form the mathematical foundation of data science. While probability helps us model uncertainty and make predictions about future events, statistics provides tools to analyze data, test hypotheses, and draw meaningful conclusions. These concepts are essential for exploratory data analysis, hypothesis testing, and building robust machine learning models.\nKey Areas We’ll Cover:\n\nBasic probability concepts and distributions\nDescriptive statistics and data summarization\nStatistical inference and hypothesis testing\nBayes’ theorem and its applications\nCorrelation and causation\nStatistical distributions commonly used in data science\n\nUnderstanding these concepts is crucial for:\n\nExploratory Data Analysis (EDA): Summarizing and visualizing data patterns\nStatistical Inference: Making conclusions about populations from samples\nMachine Learning: Understanding model assumptions and uncertainty\nRisk Assessment: Quantifying uncertainty in business decisions\n\n\nBasic Probability Concepts\nProbability is a measure of the likelihood that an event will occur, expressed as a number between 0 and 1, where:\n\n0 means the event is impossible\n1 means the event is certain\n0.5 means the event has equal chances of occurring or not\n\nKey Definitions:\nSample Space (S): The set of all possible outcomes of an experiment\n\nRolling a dice: S = \\(\\{1, 2, 3, 4, 5, 6\\}\\)\nFlipping a coin: S = \\(\\{Heads, Tails\\}\\)\n\nEvent (E): A subset of the sample space\n\nRolling an even number: E = \\(\\{2, 4, 6\\}\\)\nGetting heads: E = \\(\\{Heads\\}\\)\n\nProbability of an Event: P(E) = (Number of favorable outcomes) / (Total number of possible outcomes)\nFundamental Rules:\n\nAddition Rule: P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\nMultiplication Rule: P(A ∩ B) = P(A) × P(B|A)\nComplement Rule: P(A’) = 1 - P(A)\n\nConditional Probability: P(A|B) = P(A ∩ B) / P(B) The probability of event A given that event B has occurred.\nIndependence: Two events A and B are independent if P(A|B) = P(A) Or equivalently: P(A ∩ B) = P(A) × P(B)\n\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n# Set style for better visualizations\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Basic probability examples\nnp.random.seed(42)\n\n# Simulate coin flips\nn_flips = 1000\ncoin_flips = np.random.choice(['Heads', 'Tails'], n_flips, p=[0.5, 0.5])\n\n# Calculate probabilities\nheads_count = np.sum(coin_flips == 'Heads')\nprob_heads = heads_count / n_flips\n\nprint(\"Coin Flip Simulation:\")\nprint(f\"Number of flips: {n_flips}\")\nprint(f\"Heads: {heads_count}\")\nprint(f\"Tails: {n_flips - heads_count}\")\nprint(f\"Probability of Heads: {prob_heads:.3f}\")\nprint(f\"Expected Probability: 0.500\")\n\n# Visualize convergence to true probability\ncumulative_prob = np.cumsum(coin_flips == 'Heads') / np.arange(1, n_flips + 1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_prob, linewidth=2, label='Observed Probability')\nplt.axhline(y=0.5, color='red', linestyle='--', label='True Probability (0.5)')\nplt.xlabel('Number of Flips')\nplt.ylabel('Probability of Heads')\nplt.title('Law of Large Numbers: Coin Flip Convergence')\nplt.legend()\nplt.grid(True, alpha=0.3)\n# plt.savefig('im/probability_convergence.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nCoin Flip Simulation:\nNumber of flips: 1000\nHeads: 503\nTails: 497\nProbability of Heads: 0.503\nExpected Probability: 0.500\n\n\n\n\n\n\n\n\n\n\n# Dice rolling example - multiple events\nn_rolls = 10000\ndice_rolls = np.random.randint(1, 7, n_rolls)\n\n# Calculate various probabilities\nprob_even = np.sum(dice_rolls % 2 == 0) / n_rolls\nprob_greater_4 = np.sum(dice_rolls &gt; 4) / n_rolls\nprob_1_or_6 = np.sum((dice_rolls == 1) | (dice_rolls == 6)) / n_rolls\n\nprint(\"Dice Rolling Simulation:\")\nprint(f\"Number of rolls: {n_rolls}\")\nprint(f\"P(Even number): {prob_even:.3f} (Expected: 0.500)\")\nprint(f\"P(&gt; 4): {prob_greater_4:.3f} (Expected: 0.333)\")\nprint(f\"P(1 or 6): {prob_1_or_6:.3f} (Expected: 0.333)\")\n\n# Visualize dice roll distribution\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nvalues, counts = np.unique(dice_rolls, return_counts=True)\nplt.bar(values, counts/n_rolls, alpha=0.7, color='skyblue')\nplt.axhline(y=1/6, color='red', linestyle='--', label='Expected Probability (1/6)')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.title('Dice Roll Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\ntwo_dice = []\nfor i in range(1, 7):\n    for j in range(1, 7):\n        two_dice.append(i + j)\n\nplt.hist(two_dice, bins=range(2, 14), density=True, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Sum of Two Dice')\nplt.ylabel('Probability')\nplt.title('Sum of Two Dice Distribution')\nplt.xticks(range(2, 13))\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('im/dice_probability.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Calculate P(sum = 7)\nsum_7_count = len([x for x in two_dice if x == 7])\nprob_sum_7 = sum_7_count / len(two_dice)\nprint(f\"\\nP(Sum = 7 with two dice): {prob_sum_7:.3f}\")\n\nDice Rolling Simulation:\nNumber of rolls: 10000\nP(Even number): 0.501 (Expected: 0.500)\nP(&gt; 4): 0.333 (Expected: 0.333)\nP(1 or 6): 0.334 (Expected: 0.333)\n\n\n\n\n\n\n\n\n\n\nP(Sum = 7 with two dice): 0.167\n\n\n\n\nDescriptive Statistics\nDescriptive statistics help us summarize and understand the main characteristics of a dataset. They provide insight into the center, spread, and shape of data distributions.\nMeasures of Central Tendency:\nMean (μ or x̄): The arithmetic average \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nMedian: The middle value when data is ordered\n\nRobust to outliers\nBetter than mean for skewed distributions\n\nMode: The most frequently occurring value(s)\nMeasures of Dispersion:\nVariance (σ² or s²): Average of squared differences from the mean \\[\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nStandard Deviation (σ or s): Square root of variance \\[\\sigma = \\sqrt{\\sigma^2}\\]\nRange: Difference between maximum and minimum values\nInterquartile Range (IQR): Difference between 75th and 25th percentiles\n\nQ3 - Q1\nMeasures spread of middle 50% of data\n\nMeasures of Shape:\nSkewness: Measure of asymmetry\n\nPositive skew: tail extends to the right\nNegative skew: tail extends to the left\n\nKurtosis: Measure of tail heaviness\n\nHigh kurtosis: heavy tails, more outliers\nLow kurtosis: light tails, fewer outliers\n\n\n# Generate sample datasets for statistical analysis\nnp.random.seed(42)\n\n# Create different types of distributions\nnormal_data = np.random.normal(100, 15, 1000)  # Mean=100, std=15\nskewed_data = np.random.exponential(2, 1000)   # Exponential distribution\nuniform_data = np.random.uniform(0, 100, 1000) # Uniform distribution\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'Normal': normal_data,\n    'Skewed': skewed_data,\n    'Uniform': uniform_data\n})\n\nprint(\"Descriptive Statistics Summary:\")\nprint(\"=\" * 50)\nprint(df.describe())\n\n# Calculate additional statistics\nprint(\"\\nAdditional Statistics:\")\nprint(\"=\" * 30)\nfor col in df.columns:\n    data = df[col]\n    print(f\"\\n{col} Distribution:\")\n    print(f\"  Median: {np.median(data):.2f}\")\n    print(f\"  Mode: {stats.mode(data)[0]:.2f}\")\n    print(f\"  Variance: {np.var(data, ddof=1):.2f}\")\n    print(f\"  Skewness: {stats.skew(data):.2f}\")\n    print(f\"  Kurtosis: {stats.kurtosis(data):.2f}\")\n    print(f\"  IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}\")\n\nDescriptive Statistics Summary:\n==================================================\n            Normal       Skewed      Uniform\ncount  1000.000000  1000.000000  1000.000000\nmean    100.289981     2.015972    49.449499\nstd      14.688239     2.005977    28.891967\nmin      51.380990     0.006447     0.001163\n25%      90.286145     0.567909    25.652351\n50%     100.379509     1.451862    49.171060\n75%     109.719158     2.743369    73.862888\nmax     157.790972    14.883446    99.782086\n\nAdditional Statistics:\n==============================\n\nNormal Distribution:\n  Median: 100.38\n  Mode: 51.38\n  Variance: 215.74\n  Skewness: 0.12\n  Kurtosis: 0.07\n  IQR: 19.43\n\nSkewed Distribution:\n  Median: 1.45\n  Mode: 0.01\n  Variance: 4.02\n  Skewness: 1.98\n  Kurtosis: 5.38\n  IQR: 2.18\n\nUniform Distribution:\n  Median: 49.17\n  Mode: 0.00\n  Variance: 834.75\n  Skewness: 0.01\n  Kurtosis: -1.18\n  IQR: 48.21\n\n\n\n# Visualize distributions and their statistics\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Row 1: Histograms\nfor i, col in enumerate(df.columns):\n    axes[0, i].hist(df[col], bins=50, density=True, alpha=0.7, color=f'C{i}')\n    axes[0, i].axvline(df[col].mean(), color='red', linestyle='--', \n                      linewidth=2, label=f'Mean: {df[col].mean():.1f}')\n    axes[0, i].axvline(df[col].median(), color='green', linestyle='--', \n                      linewidth=2, label=f'Median: {df[col].median():.1f}')\n    axes[0, i].set_title(f'{col} Distribution')\n    axes[0, i].set_xlabel('Value')\n    axes[0, i].set_ylabel('Density')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n\n# Row 2: Box plots\nfor i, col in enumerate(df.columns):\n    axes[1, i].boxplot(df[col])\n    axes[1, i].set_title(f'{col} Box Plot')\n    axes[1, i].set_ylabel('Value')\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('im/descriptive_stats_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStatistical Distributions\nStatistical distributions are mathematical functions that describe the probability of different outcomes in a dataset. Understanding common distributions is crucial for data analysis and modeling.\nNormal Distribution (Gaussian)\n\nBell-shaped, symmetric distribution\nDefined by mean (μ) and standard deviation (σ)\n68-95-99.7 rule (empirical rule)\nMany natural phenomena follow normal distribution\n\nProperties:\n\nMean = Median = Mode\n68% of data within 1σ of mean\n95% of data within 2σ of mean\n\n99.7% of data within 3σ of mean\n\nOther Important Distributions:\n\nUniform Distribution: All outcomes equally likely\nExponential Distribution: Models time between events\nPoisson Distribution: Models count of events in fixed intervals\nBinomial Distribution: Models number of successes in n trials\nChi-square Distribution: Used in hypothesis testing\nt-Distribution: Used when sample size is small\n\n\n# Demonstrate various statistical distributions\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfig.suptitle('Common Statistical Distributions', fontsize=16)\n\n# 1. Normal Distribution\nx = np.linspace(-4, 4, 100)\nnormal_pdf = stats.norm.pdf(x, 0, 1)\naxes[0, 0].plot(x, normal_pdf, 'b-', linewidth=2, label='μ=0, σ=1')\naxes[0, 0].fill_between(x, normal_pdf, alpha=0.3)\naxes[0, 0].set_title('Normal Distribution')\naxes[0, 0].set_xlabel('Value')\naxes[0, 0].set_ylabel('Probability Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Uniform Distribution\nx_uniform = np.linspace(-0.5, 3.5, 100)\nuniform_pdf = stats.uniform.pdf(x_uniform, 0, 3)\naxes[0, 1].plot(x_uniform, uniform_pdf, 'r-', linewidth=2, label='a=0, b=3')\naxes[0, 1].fill_between(x_uniform, uniform_pdf, alpha=0.3)\naxes[0, 1].set_title('Uniform Distribution')\naxes[0, 1].set_xlabel('Value')\naxes[0, 1].set_ylabel('Probability Density')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Exponential Distribution\nx_exp = np.linspace(0, 5, 100)\nexp_pdf = stats.expon.pdf(x_exp, scale=1)\naxes[0, 2].plot(x_exp, exp_pdf, 'g-', linewidth=2, label='λ=1')\naxes[0, 2].fill_between(x_exp, exp_pdf, alpha=0.3)\naxes[0, 2].set_title('Exponential Distribution')\naxes[0, 2].set_xlabel('Value')\naxes[0, 2].set_ylabel('Probability Density')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# 4. Poisson Distribution\nx_poisson = np.arange(0, 15)\npoisson_pmf = stats.poisson.pmf(x_poisson, mu=3)\naxes[1, 0].bar(x_poisson, poisson_pmf, alpha=0.7, color='purple', label='λ=3')\naxes[1, 0].set_title('Poisson Distribution')\naxes[1, 0].set_xlabel('Number of Events')\naxes[1, 0].set_ylabel('Probability Mass')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 5. Binomial Distribution\nx_binomial = np.arange(0, 21)\nbinomial_pmf = stats.binom.pmf(x_binomial, n=20, p=0.3)\naxes[1, 1].bar(x_binomial, binomial_pmf, alpha=0.7, color='orange', label='n=20, p=0.3')\naxes[1, 1].set_title('Binomial Distribution')\naxes[1, 1].set_xlabel('Number of Successes')\naxes[1, 1].set_ylabel('Probability Mass')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# 6. Chi-square Distribution\nx_chi2 = np.linspace(0, 15, 100)\nchi2_pdf = stats.chi2.pdf(x_chi2, df=4)\naxes[1, 2].plot(x_chi2, chi2_pdf, 'brown', linewidth=2, label='df=4')\naxes[1, 2].fill_between(x_chi2, chi2_pdf, alpha=0.3)\naxes[1, 2].set_title('Chi-square Distribution')\naxes[1, 2].set_xlabel('Value')\naxes[1, 2].set_ylabel('Probability Density')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\n# 7. t-Distribution\nx_t = np.linspace(-4, 4, 100)\nt_pdf = stats.t.pdf(x_t, df=5)\nnormal_pdf_comp = stats.norm.pdf(x_t, 0, 1)\naxes[2, 0].plot(x_t, t_pdf, 'red', linewidth=2, label='t-dist (df=5)')\naxes[2, 0].plot(x_t, normal_pdf_comp, 'blue', linewidth=2, linestyle='--', label='Normal')\naxes[2, 0].set_title('t-Distribution vs Normal')\naxes[2, 0].set_xlabel('Value')\naxes[2, 0].set_ylabel('Probability Density')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# 8. Beta Distribution\nx_beta = np.linspace(0, 1, 100)\nbeta_pdf = stats.beta.pdf(x_beta, a=2, b=5)\naxes[2, 1].plot(x_beta, beta_pdf, 'pink', linewidth=2, label='α=2, β=5')\naxes[2, 1].fill_between(x_beta, beta_pdf, alpha=0.3)\naxes[2, 1].set_title('Beta Distribution')\naxes[2, 1].set_xlabel('Value')\naxes[2, 1].set_ylabel('Probability Density')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\n# 9. Gamma Distribution\nx_gamma = np.linspace(0, 10, 100)\ngamma_pdf = stats.gamma.pdf(x_gamma, a=2, scale=1)\naxes[2, 2].plot(x_gamma, gamma_pdf, 'cyan', linewidth=2, label='α=2, β=1')\naxes[2, 2].fill_between(x_gamma, gamma_pdf, alpha=0.3)\naxes[2, 2].set_title('Gamma Distribution')\naxes[2, 2].set_xlabel('Value')\naxes[2, 2].set_ylabel('Probability Density')\naxes[2, 2].legend()\naxes[2, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('im/statistical_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Real-world applications of distributions with simulated data\nprint(\"Distribution Applications in Data Science:\")\nprint(\"=\" * 50)\n\n# 1. Normal Distribution - Height data\nheights = np.random.normal(170, 10, 1000)  # Heights in cm\nprint(f\"Height Data (Normal Distribution):\")\nprint(f\"  Mean height: {np.mean(heights):.1f} cm\")\nprint(f\"  Std deviation: {np.std(heights):.1f} cm\")\nprint(f\"  % between 160-180 cm: {np.sum((heights &gt;= 160) & (heights &lt;= 180))/len(heights)*100:.1f}%\")\n\n# 2. Exponential Distribution - Time between customer arrivals\narrival_times = np.random.exponential(5, 1000)  # Average 5 minutes between arrivals\nprint(f\"\\nCustomer Arrival Times (Exponential Distribution):\")\nprint(f\"  Average time between arrivals: {np.mean(arrival_times):.1f} minutes\")\nprint(f\"  % of arrivals within 2 minutes: {np.sum(arrival_times &lt;= 2)/len(arrival_times)*100:.1f}%\")\n\n# 3. Poisson Distribution - Number of website visits per hour\nvisits_per_hour = np.random.poisson(50, 24*7)  # 50 average visits per hour for a week\nprint(f\"\\nWebsite Visits (Poisson Distribution):\")\nprint(f\"  Average visits per hour: {np.mean(visits_per_hour):.1f}\")\nprint(f\"  Max visits in an hour: {np.max(visits_per_hour)}\")\nprint(f\"  Hours with &gt;60 visits: {np.sum(visits_per_hour &gt; 60)}\")\n\n# 4. Binomial Distribution - A/B Testing\n# 100 users, 15% conversion rate\nconversions = np.random.binomial(100, 0.15, 1000)  # 1000 experiments\nprint(f\"\\nA/B Test Conversions (Binomial Distribution):\")\nprint(f\"  Average conversions per 100 users: {np.mean(conversions):.1f}\")\nprint(f\"  95% confidence interval: [{np.percentile(conversions, 2.5):.0f}, {np.percentile(conversions, 97.5):.0f}]\")\n\n# Visualize these applications\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Heights\naxes[0, 0].hist(heights, bins=50, density=True, alpha=0.7, color='skyblue')\nx_heights = np.linspace(heights.min(), heights.max(), 100)\naxes[0, 0].plot(x_heights, stats.norm.pdf(x_heights, np.mean(heights), np.std(heights)), \n                'r-', linewidth=2, label='Normal Fit')\naxes[0, 0].set_title('Heights Distribution')\naxes[0, 0].set_xlabel('Height (cm)')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Arrival times\naxes[0, 1].hist(arrival_times, bins=50, density=True, alpha=0.7, color='lightgreen')\nx_arrivals = np.linspace(0, arrival_times.max(), 100)\naxes[0, 1].plot(x_arrivals, stats.expon.pdf(x_arrivals, scale=np.mean(arrival_times)), \n                'r-', linewidth=2, label='Exponential Fit')\naxes[0, 1].set_title('Time Between Customer Arrivals')\naxes[0, 1].set_xlabel('Time (minutes)')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Website visits\nx_visits = np.arange(0, visits_per_hour.max()+1)\naxes[1, 0].hist(visits_per_hour, bins=30, density=True, alpha=0.7, color='lightcoral')\naxes[1, 0].plot(x_visits, stats.poisson.pmf(x_visits, np.mean(visits_per_hour)), \n                'ro-', linewidth=2, markersize=4, label='Poisson Fit')\naxes[1, 0].set_title('Website Visits per Hour')\naxes[1, 0].set_xlabel('Number of Visits')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Conversions\nx_conv = np.arange(0, conversions.max()+1)\naxes[1, 1].hist(conversions, bins=20, density=True, alpha=0.7, color='gold')\naxes[1, 1].plot(x_conv, stats.binom.pmf(x_conv, 100, np.mean(conversions)/100), \n                'ro-', linewidth=2, markersize=3, label='Binomial Fit')\naxes[1, 1].set_title('A/B Test Conversions')\naxes[1, 1].set_xlabel('Number of Conversions')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('im/distribution_applications.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nDistribution Applications in Data Science:\n==================================================\nHeight Data (Normal Distribution):\n  Mean height: 170.1 cm\n  Std deviation: 10.3 cm\n  % between 160-180 cm: 65.7%\n\nCustomer Arrival Times (Exponential Distribution):\n  Average time between arrivals: 5.0 minutes\n  % of arrivals within 2 minutes: 33.4%\n\nWebsite Visits (Poisson Distribution):\n  Average visits per hour: 50.7\n  Max visits in an hour: 75\n  Hours with &gt;60 visits: 14\n\nA/B Test Conversions (Binomial Distribution):\n  Average conversions per 100 users: 14.9\n  95% confidence interval: [9, 22]\n\n\n\n\n\n\n\n\n\n\n\nBayes’ Theorem\nBayes’ theorem is a fundamental principle in probability theory that describes how to update the probability of a hypothesis based on new evidence. It’s the foundation of Bayesian statistics and has numerous applications in machine learning, medical diagnosis, spam filtering, and decision-making.\nMathematical Formula:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nWhere:\n\nP(A|B): Posterior probability - probability of A given B\nP(B|A): Likelihood - probability of B given A\n\nP(A): Prior probability - initial probability of A\nP(B): Marginal probability - total probability of B\n\nIn words: Posterior = (Likelihood × Prior) / Evidence\nKey Components:\n\nPrior P(A): Our initial belief about the probability of A before seeing evidence\nLikelihood P(B|A): How likely we are to observe evidence B if A is true\nEvidence P(B): Total probability of observing evidence B\nPosterior P(A|B): Updated probability of A after observing evidence B\n\nApplications in Data Science:\n\nMedical Diagnosis: Update disease probability based on test results\nSpam Detection: Classify emails based on word patterns\nMachine Learning: Naive Bayes classifier\n\n\n# Bayes' Theorem Examples\n\nprint(\"Bayes' Theorem Applications\")\nprint(\"=\" * 40)\n\n# Example 1: Medical Diagnosis\nprint(\"Example 1: Medical Diagnosis\")\nprint(\"-\" * 30)\n\n# Disease affects 1% of population\nprior_disease = 0.01\nprior_no_disease = 1 - prior_disease\n\n# Test accuracy: 95% sensitivity, 90% specificity\nsensitivity = 0.95  # P(Test+ | Disease)\nspecificity = 0.90  # P(Test- | No Disease)\nfalse_positive_rate = 1 - specificity  # P(Test+ | No Disease)\n\n# Evidence: P(Test+)\nevidence_test_positive = (sensitivity * prior_disease) + (false_positive_rate * prior_no_disease)\n\n# Posterior: P(Disease | Test+)\nposterior_disease_given_positive = (sensitivity * prior_disease) / evidence_test_positive\n\nprint(f\"Prior probability of disease: {prior_disease:.1%}\")\nprint(f\"Test sensitivity: {sensitivity:.1%}\")\nprint(f\"Test specificity: {specificity:.1%}\")\nprint(f\"Probability of positive test: {evidence_test_positive:.1%}\")\nprint(f\"Probability of disease given positive test: {posterior_disease_given_positive:.1%}\")\n\n# Example 2: Spam Detection\nprint(f\"\\nExample 2: Spam Detection\")\nprint(\"-\" * 30)\n\n# Prior probabilities\nprior_spam = 0.3  # 30% of emails are spam\nprior_ham = 0.7   # 70% of emails are legitimate\n\n# Likelihood of word \"free\" appearing\nlikelihood_free_given_spam = 0.8   # 80% of spam contains \"free\"\nlikelihood_free_given_ham = 0.1    # 10% of legitimate emails contain \"free\"\n\n# Evidence: P(\"free\")\nevidence_free = (likelihood_free_given_spam * prior_spam) + (likelihood_free_given_ham * prior_ham)\n\n# Posterior: P(Spam | \"free\")\nposterior_spam_given_free = (likelihood_free_given_spam * prior_spam) / evidence_free\n\nprint(f\"Prior probability of spam: {prior_spam:.1%}\")\nprint(f\"P('free' | spam): {likelihood_free_given_spam:.1%}\")\nprint(f\"P('free' | legitimate): {likelihood_free_given_ham:.1%}\")\nprint(f\"P(spam | 'free'): {posterior_spam_given_free:.1%}\")\n\nBayes' Theorem Applications\n========================================\nExample 1: Medical Diagnosis\n------------------------------\nPrior probability of disease: 1.0%\nTest sensitivity: 95.0%\nTest specificity: 90.0%\nProbability of positive test: 10.8%\nProbability of disease given positive test: 8.8%\n\nExample 2: Spam Detection\n------------------------------\nPrior probability of spam: 30.0%\nP('free' | spam): 80.0%\nP('free' | legitimate): 10.0%\nP(spam | 'free'): 77.4%\n\n\n\n\nCorrelation and Causation\nUnderstanding the relationship between variables is crucial in data analysis. Correlation measures the strength and direction of a linear relationship between two variables, while causation implies that one variable directly influences another.\nKey Principle: “Correlation does not imply causation”\nCorrelation Coefficient (Pearson’s r): \\[r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}\\]\nInterpretation:\n\nr = +1: Perfect positive correlation\nr = 0: No linear correlation\n\nr = -1: Perfect negative correlation\n|r| &gt; 0.7: Strong correlation\n0.3 &lt; |r| &lt; 0.7: Moderate correlation\n|r| &lt; 0.3: Weak correlation\n\nTypes of Relationships:\n\nNo relationship: Variables are independent\nLinear relationship: Variables change at a constant rate\nNon-linear relationship: Variables are related but not linearly\nSpurious correlation: Variables appear related due to confounding factors\n\nEstablishing Causation:\n\nTemporal precedence: Cause must precede effect\nCovariation: Cause and effect must be correlated\nNon-spuriousness: Relationship isn’t due to third variable\nMechanism: Logical explanation for how cause leads to effect\n\n\n# Generate data to demonstrate correlation vs causation\nnp.random.seed(42)\nn = 500\n\n# Example 1: Strong positive correlation\nx1 = np.random.normal(0, 1, n)\ny1 = 2 * x1 + np.random.normal(0, 0.5, n)  # y = 2x + noise\n\n# Example 2: No correlation\nx2 = np.random.normal(0, 1, n)\ny2 = np.random.normal(0, 1, n)  # Independent variables\n\n# Example 3: Non-linear relationship\nx3 = np.random.uniform(-3, 3, n)\ny3 = x3**2 + np.random.normal(0, 1, n)  # Quadratic relationship\n\n# Example 4: Spurious correlation (both depend on third variable)\nz = np.random.normal(0, 1, n)  # Hidden variable\nx4 = z + np.random.normal(0, 0.5, n)\ny4 = z + np.random.normal(0, 0.5, n)\n\n# Calculate correlation coefficients\ncorr1 = np.corrcoef(x1, y1)[0, 1]\ncorr2 = np.corrcoef(x2, y2)[0, 1]\ncorr3 = np.corrcoef(x3, y3)[0, 1]\ncorr4 = np.corrcoef(x4, y4)[0, 1]\n\nprint(\"Correlation Analysis Examples:\")\nprint(\"=\" * 40)\nprint(f\"1. Linear relationship: r = {corr1:.3f}\")\nprint(f\"2. No relationship: r = {corr2:.3f}\")\nprint(f\"3. Non-linear relationship: r = {corr3:.3f}\")\nprint(f\"4. Spurious correlation: r = {corr4:.3f}\")\n\n# Create comprehensive correlation visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Correlation vs Causation Examples', fontsize=16)\n\n# Example 1: Strong positive correlation\naxes[0, 0].scatter(x1, y1, alpha=0.6, color='blue')\nz1 = np.polyfit(x1, y1, 1)\np1 = np.poly1d(z1)\naxes[0, 0].plot(x1, p1(x1), \"r--\", alpha=0.8, linewidth=2)\naxes[0, 0].set_title(f'Strong Positive Correlation\\nr = {corr1:.3f}')\naxes[0, 0].set_xlabel('X Variable')\naxes[0, 0].set_ylabel('Y Variable')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Example 2: No correlation\naxes[0, 1].scatter(x2, y2, alpha=0.6, color='green')\naxes[0, 1].set_title(f'No Correlation\\nr = {corr2:.3f}')\naxes[0, 1].set_xlabel('X Variable')\naxes[0, 1].set_ylabel('Y Variable')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Example 3: Non-linear relationship\naxes[0, 2].scatter(x3, y3, alpha=0.6, color='red')\n# Fit quadratic curve\nz3 = np.polyfit(x3, y3, 2)\np3 = np.poly1d(z3)\nx3_sorted = np.linspace(x3.min(), x3.max(), 100)\naxes[0, 2].plot(x3_sorted, p3(x3_sorted), \"orange\", linewidth=2, label='Quadratic fit')\naxes[0, 2].set_title(f'Non-linear Relationship\\nPearson r = {corr3:.3f}')\naxes[0, 2].set_xlabel('X Variable')\naxes[0, 2].set_ylabel('Y Variable')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# Example 4: Spurious correlation\naxes[1, 0].scatter(x4, y4, alpha=0.6, color='purple')\nz4 = np.polyfit(x4, y4, 1)\np4 = np.poly1d(z4)\naxes[1, 0].plot(x4, p4(x4), \"orange\", alpha=0.8, linewidth=2)\naxes[1, 0].set_title(f'Spurious Correlation\\nr = {corr4:.3f}')\naxes[1, 0].set_xlabel('X Variable')\naxes[1, 0].set_ylabel('Y Variable')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Create correlation heatmap for multiple variables\n# Real-world example: student performance data\nnp.random.seed(42)\nstudy_hours = np.random.gamma(2, 3, n)  # Hours studied per week\nsleep_hours = 8 - 0.1 * study_hours + np.random.normal(0, 1, n)  # Sleep affected by study\nsleep_hours = np.clip(sleep_hours, 4, 12)  # Reasonable sleep range\ntest_score = 50 + 2 * study_hours + 3 * sleep_hours + np.random.normal(0, 10, n)\ntest_score = np.clip(test_score, 0, 100)  # Score between 0-100\nstress_level = 100 - test_score + np.random.normal(0, 15, n)  # Stress inversely related to score\nstress_level = np.clip(stress_level, 0, 100)\n\n# Create DataFrame\nstudent_data = pd.DataFrame({\n    'Study_Hours': study_hours,\n    'Sleep_Hours': sleep_hours,\n    'Test_Score': test_score,\n    'Stress_Level': stress_level\n})\n\n# Correlation matrix\ncorr_matrix = student_data.corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nsns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n            square=True, ax=axes[1, 1], cbar_kws={\"shrink\": .8})\naxes[1, 1].set_title('Student Performance\\nCorrelation Matrix')\n\n# Pairplot equivalent using scatter plots\naxes[1, 2].scatter(student_data['Study_Hours'], student_data['Test_Score'], \n                  alpha=0.6, color='darkblue')\naxes[1, 2].set_title('Study Hours vs Test Score\\nCausal Relationship?')\naxes[1, 2].set_xlabel('Study Hours per Week')\naxes[1, 2].set_ylabel('Test Score')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Add trend line\nz_study = np.polyfit(student_data['Study_Hours'], student_data['Test_Score'], 1)\np_study = np.poly1d(z_study)\naxes[1, 2].plot(student_data['Study_Hours'], p_study(student_data['Study_Hours']), \n               \"red\", alpha=0.8, linewidth=2)\n\nplt.tight_layout()\n# plt.savefig('im/correlation_causation_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Print correlation analysis\nprint(f\"\\nStudent Performance Correlations:\")\nprint(\"-\" * 35)\nfor i, var1 in enumerate(student_data.columns):\n    for var2 in student_data.columns[i+1:]:\n        corr = student_data[var1].corr(student_data[var2])\n        print(f\"{var1} vs {var2}: r = {corr:.3f}\")\n\nCorrelation Analysis Examples:\n========================================\n1. Linear relationship: r = 0.969\n2. No relationship: r = -0.022\n3. Non-linear relationship: r = -0.021\n4. Spurious correlation: r = 0.820\n\n\n\n\n\n\n\n\n\n\nStudent Performance Correlations:\n-----------------------------------\nStudy_Hours vs Sleep_Hours: r = -0.323\nStudy_Hours vs Test_Score: r = 0.556\nStudy_Hours vs Stress_Level: r = -0.288\nSleep_Hours vs Test_Score: r = 0.005\nSleep_Hours vs Stress_Level: r = -0.008\nTest_Score vs Stress_Level: r = -0.600\n\n\n\n\nFor Further Exploration\nCheck out these resources to deepen your understanding of probability and statistics:\n\nNormal Distribution\nCorrelation\nBayes’ Theorem\nCentral Limit Theorem\n\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapters 3-6 Data Science from Scratch by Joel Grus, 2nd edition\nChapters 2-4 Essential Math for Data Science by Thomas Nield, 1st edition"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome Gen. 2026-1!",
    "section": "",
    "text": "SIAFI Data Science and Machine Learning Handbook\nThis handbook will guide you through the main concepts and tools used in this course. It is organized as follows:\n\nIntroduction to Data Science\nMachine Learning\nDeep Learning\n\nPlease feel free to explore the contents and never stop learning.\nYou can find the source code for this handbook in the GitHub repository."
  }
]