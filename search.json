[
  {
    "objectID": "ML/2_3_classification.html",
    "href": "ML/2_3_classification.html",
    "title": "2. Machine Learning",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)"
  },
  {
    "objectID": "ML/2_3_classification.html#classification",
    "href": "ML/2_3_classification.html#classification",
    "title": "2. Machine Learning",
    "section": "2.3 Classification",
    "text": "2.3 Classification\n\n2.3.1 MNIST Dataset\nThe MNIST dataset is a set of 70,000 small images of digits handwritten by high school students and employees of the US Census Bureau. Each image is labeled with the digit it represents. This set has been studied so much that it is often called the “hello world” of machine learning: whenever people come up with a new classification algorithm they are curious to see how it will perform on MNIST, and anyone who learns machine learning tackles this dataset sooner or later.\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\n\n\nprint(mnist.DESCR)\n\n**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n**Please cite**:  \n\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n\nDownloaded from openml.org.\n\n\n\nprint(mnist.keys())\n\ndict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])\n\n\n\nX, y = mnist.data, mnist.target\nX\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], shape=(70000, 784))\n\n\n\nX.shape\n\n(70000, 784)\n\n\n\ny\n\narray(['5', '0', '4', ..., '4', '5', '6'], shape=(70000,), dtype=object)\n\n\n\ny.shape\n\n(70000,)\n\n\nThere are 70,000 images, and each image has 784 features. This is because each image is 28 × 28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black).\n\ndef plot_digit(image_data):\n    image = image_data.reshape(28, 28)\n    plt.imshow(image, cmap=\"binary\")\n    plt.axis(\"off\")\n\nsome_digit = X[0]\nplot_digit(some_digit)\n\nplt.show()\n\n\n\n\n\n\n\n\n\ny[0]\n\n'5'\n\n\n\n# extra code\nplt.figure(figsize=(9, 9))\nfor idx, image_data in enumerate(X[:100]):\n    plt.subplot(10, 10, idx + 1)\n    plot_digit(image_data)\nplt.subplots_adjust(wspace=0, hspace=0)\n\nplt.show()\n\n\n\n\n\n\n\n\nThe MNIST dataset returned by fetch_openml() is actually already split into a training set (the first 60,000 images) and a test set (the last 10,000 images).\nThe training set is already shuffled for us, which is good because this guarantees that all cross-validation folds will be similar (we don’t want one fold to be missing some digits). Moreover, some learning algorithms are sensitive to the order of the training instances, and they perform poorly if they get many similar instances in a row. Shuffling the dataset ensures that this won’t happen.\n\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n\n\n\n2.3.2 Training a Binary Classifier\nLet’s simplify the problem for now and only try to identify one digit—for example, the number 5. This “5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and non-5.\n\ny_train_5 = (y_train == '5')  # True for all 5s, False for all other digits\ny_test_5 = (y_test == '5')\n\nNow let’s pick a classifier and train it. A good place to start is with a stochastic gradient descent (SGD, or stochastic GD) classifier, using Scikit-Learn’s SGDClassifier class. This classifier is capable of handling very large datasets efficiently. This is in part because SGD deals with training instances independently, one at a time, which also makes SGD well suited for online learning. Let’s create an SGDClassifier and train it on the whole training set:\n\nfrom sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\n\nSGDClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SGDClassifier?Documentation for SGDClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'hinge'\n\n\n\npenalty \n'l2'\n\n\n\nalpha \n0.0001\n\n\n\nl1_ratio \n0.15\n\n\n\nfit_intercept \nTrue\n\n\n\nmax_iter \n1000\n\n\n\ntol \n0.001\n\n\n\nshuffle \nTrue\n\n\n\nverbose \n0\n\n\n\nepsilon \n0.1\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nlearning_rate \n'optimal'\n\n\n\neta0 \n0.0\n\n\n\npower_t \n0.5\n\n\n\nearly_stopping \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \n5\n\n\n\nclass_weight \nNone\n\n\n\nwarm_start \nFalse\n\n\n\naverage \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nsgd_clf.predict([some_digit])\n\narray([ True])\n\n\nThe classifier guesses that this image represents a 5 (True). Looks like it guessed right in this particular case! Now, let’s evaluate this model’s performance.\n\n\n2.3.3 Performance Measures\n\n2.3.3.1 Measuring Accuracy Using Cross-Validation\nAccuracy is the proportion of all classifications that were correct, whether positive or negative. It is mathematically defined as:\n\\[\\text{Accuracy} =  \\frac{\\text{correct classifications}}{\\text{total classifications}} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\\]\nLet’s use the cross_val_score() function to evaluate our SGDClassifier model, using k-fold cross-validation with three folds. Remember that k-fold cross-validation means splitting the training set into k folds (in this case, three), then training the model k times, holding out a different fold each time for evaluation.\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\n\nskfolds = StratifiedKFold(n_splits=3)  # add shuffle=True if the dataset is\n                                       # not already shuffled\nfor train_index, test_index in skfolds.split(X_train, y_train_5):\n    clone_clf = clone(sgd_clf)\n    X_train_folds = X_train[train_index]\n    y_train_folds = y_train_5[train_index]\n    X_test_fold = X_train[test_index]\n    y_test_fold = y_train_5[test_index]\n\n    clone_clf.fit(X_train_folds, y_train_folds)\n    y_pred = clone_clf.predict(X_test_fold)\n    n_correct = sum(y_pred == y_test_fold)\n    print(n_correct / len(y_pred))  # prints 0.95035, 0.96035, and 0.9604\n\n0.95035\n0.96035\n0.9604\n\n\nAlternatively can use the cross_val_score() function to perform the same cross-validation more easily:\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\nLet’s look at a dummy classifier that just classifies every single image in the most frequent class, which in this case is the negative class (i.e., non-5):\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier()\ndummy_clf.fit(X_train, y_train_5)\nprint(any(dummy_clf.predict(X_train)))\n\nFalse\n\n\n\nfrom sklearn.model_selection import cross_val_score\n\ncross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n\narray([0.90965, 0.90965, 0.90965])\n\n\nIt has over 90% accuracy! This is simply because only about 10% of the images are 5s, so if you always guess that an image is not a 5, you will be right about 90% of the time.\nThis demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with imbalanced datasets (i.e., when some classes are much more frequent than others). A much better way to evaluate the performance of a classifier is to look at the confusion matrix (CM).\n\n\n2.3.3.2 Confusion Matrix\n\n\n\nConfusion Matrix\n\n\nTo compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets. You could make predictions on the test set, but it’s best to keep that untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function:\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n\nJust like the cross_val_score() function, cross_val_predict() performs k-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set (by “clean” we mean “out-of-sample”: the model makes predictions on data that it never saw during training).\nNow you are ready to get the confusion matrix using the confusion_matrix() function. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred):\n\ny_train_pred\n\narray([ True, False, False, ...,  True, False, False], shape=(60000,))\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_train_5, y_train_pred)\ncm\n\narray([[53892,   687],\n       [ 1891,  3530]])\n\n\nEach row in a confusion matrix represents an actual class, while each column represents a predicted class. The first row of this matrix considers non-5 images (the negative class): 53,892 of them were correctly classified as non-5s (they are called true negatives), while the remaining 687 were wrongly classified as 5s (false positives, also called type I errors). The second row considers the images of 5s (the positive class): 1,891 were wrongly classified as non-5s (false negatives, also called type II errors), while the remaining 3,530 were correctly classified as 5s (true positives). A perfect classifier would only have true positives and true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left to bottom right):\n\ny_train_perfect_predictions = y_train_5  # pretend we reached perfection\nconfusion_matrix(y_train_5, y_train_perfect_predictions)\n\narray([[54579,     0],\n       [    0,  5421]])\n\n\n\n\n2.3.3.3 Recall, Precision, FPR and F1 Score\nRecall (also called sensitivity or true positive rate) is the proportion of all actual positive instances that were correctly classified. It is mathematically defined as:\n\\[\\text{Recall} =  \\frac{\\text{correctly classified actual positives}}{\\text{all actual positives}} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\]\nA hypothetical perfect model would have zero false negatives and therefore a recall (TPR) of 1.0, which is to say, a 100% detection rate.\nIn an imbalanced dataset where the number of actual positives is very low, recall is a more meaningful metric than accuracy because it measures the ability of the model to correctly identify all positive instances. For applications like disease prediction, correctly identifying the positive cases is crucial. A false negative typically has more serious consequences than a false positive.\nFalse Positive Rate (FPR) is the proportion of all actual negatives that were classified incorrectly as positives, also known as the probability of false alarm. It is mathematically defined as:\n\\[\\text{FPR} =  \\frac{\\text{incorrectly classified actual negatives}}{\\text{all actual negatives}} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\\]\nA perfect model would have zero false positives and therefore a FPR of 0.0, which is to say, a 0% false alarm rate.\nIn an imbalanced dataset where the number of actual negatives is very, very low, say 1-2 examples in total, FPR is less meaningful and less useful as a metric.\nPrecision is the proportion of all the model’s positive classifications that are actually positive. It is mathematically defined as:\n\\[\\text{Precision} =  \\frac{\\text{correctly classified actual positives}}{\\text{everything classified as positive}} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\]\nA hypothetical perfect model would have zero false positives and therefore a precision of 1.0.\nIn an imbalanced dataset where the number of actual positives is very, very low, say 1-2 examples in total, precision is less meaningful and less useful as a metric.\nPrecision improves as false positives decrease, while recall improves when false negatives decrease. But as seen in the previous section, increasing the classification threshold tends to decrease the number of false positives and increase the number of false negatives, while decreasing the threshold has the opposite effects. As a result, precision and recall often show an inverse relationship, where improving one of them worsens the other.\nF1 Score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is mathematically defined as:\n\\[\\text{F1} =  2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2\\text{TP}}{2\\text{TP}+\\text{FP}+\\text{FN}}\\]\nThis metric balances the importance of precision and recall, and is preferable to accuracy for class-imbalanced datasets. When precision and recall both have perfect scores of 1.0, F1 will also have a perfect score of 1.0. More broadly, when precision and recall are close in value, F1 will be close to their value. When precision and recall are far apart, F1 will be similar to whichever metric is worse.\n\nfrom sklearn.metrics import precision_score, recall_score\n\nprecision_score(y_train_5, y_train_pred)  # == 3530 / (687 + 3530)\n\n0.8370879772350012\n\n\n\nrecall_score(y_train_5, y_train_pred)  # == 3530 / (1891 + 3530)\n\n0.6511713705958311\n\n\nNow our 5-detector does not look as shiny as it did when we looked at its accuracy. When it claims an image represents a 5, it is correct only 83.7% of the time. Moreover, it only detects 65.1% of the 5s.\n\nfrom sklearn.metrics import f1_score\n\nf1_score(y_train_5, y_train_pred)\n\n0.7325171197343847\n\n\nThe F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters in surveillance images: it is probably fine if your classifier only has 30% precision as long as it has 99% recall. Sure, the security guards will get a few false alerts, but almost all shoplifters will get caught. Similarly, medical diagnosis usually requires a high recall to avoid missing anything important. False positives can be ruled out by follow-up medical tests.\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall trade-off.\n\n\n2.3.3.4 Choice of metrics and tradeoffs\n\n\n\nChoice of metrics\n\n\nTo understand the trade-off, let’s look at how the SGDClassifier makes its classification decisions. For each instance, it computes a score based on a decision function. If that score is greater than a threshold, it assigns the instance to the positive class; otherwise it assigns it to the negative class. Suppose the decision threshold is positioned at the central arrow (between the two 5s): you will find 4 true positives (actual 5s) on the right of that threshold, and 1 false positive (actually a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing the precision (up to 100% in this case), but one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threshold increases recall and reduces precision.\n\n\n\nTrade-off\n\n\nInstead of calling the classifier’s predict() method, you can call its decision_function() method, which returns a score for each instance. You can then use any threshold you want to make predictions based on those scores:\n\ny_scores = sgd_clf.decision_function([some_digit])\ny_scores\n\narray([2164.22030239])\n\n\nThe SGDClassifier uses a threshold equal to 0, so the following code returns the same result as the predict() method (i.e., True). Let’s raise the threshold:\n\nthreshold = 0\ny_some_digit_pred = (y_scores &gt; threshold)\ny_some_digit_pred\n\narray([ True])\n\n\n\nthreshold = 3000\ny_some_digit_pred = (y_scores &gt; threshold)\ny_some_digit_pred\n\narray([False])\n\n\nThis confirms that raising the threshold decreases recall. The image actually represents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 3,000.\nHow to decide which threshold to use? One option is to use the cross_val_predict() function to get the scores of all instances in the training set, but this time specifying we want to return decision scores instead of predictions:\n\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method=\"decision_function\")\n\nWith these scores, use the precision_recall_curve() function to compute precision and recall for all possible thresholds (the function adds a last precision of 1 and a last recall of 0, corresponding to an infinite threshold):\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\n\nplt.figure(figsize=(8, 4))  # extra code – it's not needed, just formatting\nplt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\nplt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\nplt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n\n# extra code – this section just beautifies Figure 3–5\nidx = (thresholds &gt;= threshold).argmax()  # first index ≥ threshold\nplt.plot(thresholds[idx], precisions[idx], \"bo\")\nplt.plot(thresholds[idx], recalls[idx], \"go\")\nplt.axis([-50000, 50000, 0, 1])\nplt.grid(True)\nplt.xlabel(\"Threshold\")\nplt.legend(loc=\"center right\")\n\nplt.show()\n\n\n\n\n\n\n\n\nAt this threshold value, precision is near 90% and recall is around 50%. Another way to select a good precision/recall trade-off is to plot precision directly against recall.\n\nimport matplotlib.patches as patches  # extra code – for the curved arrow\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n\nplt.plot(recalls, precisions, linewidth=2, label=\"Precision/Recall curve\")\n\n# extra code – just beautifies Figure 3–6\nplt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\nplt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\nplt.plot([recalls[idx]], [precisions[idx]], \"ko\",\n         label=\"Point at threshold 3,000\")\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.79, 0.60), (0.61, 0.78),\n    connectionstyle=\"arc3,rad=.2\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.56, 0.62, \"Higher\\nthreshold\", color=\"#333333\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid(True)\nplt.legend(loc=\"lower left\")\n\nplt.show()\n\n\n\n\n\n\n\n\nYou can see that precision really starts to fall sharply at around 80% recall. You will probably want to select a precision/recall trade-off just before that drop—for example, at around 60% recall. But of course, the choice depends on your project.\nSuppose you decide to aim for 90% precision. You could use the first plot to find the threshold you need to use, but that’s not very precise. Alternatively, you can search for the lowest threshold that gives you at least 90% precision. For this, you can use the NumPy array’s argmax() method. This returns the first index of the maximum value, which in this case means the first True value:\n\nidx_for_90_precision = (precisions &gt;= 0.90).argmax()\nthreshold_for_90_precision = thresholds[idx_for_90_precision]\nthreshold_for_90_precision\n\nnp.float64(3370.0194991439557)\n\n\n\ny_train_pred_90 = (y_scores &gt;= threshold_for_90_precision)\n\n\nprecision_score(y_train_5, y_train_pred_90)\n\n0.9000345901072293\n\n\n\nrecall_at_90_precision = recall_score(y_train_5, y_train_pred_90)\nrecall_at_90_precision\n\n0.4799852425751706\n\n\nGreat, you have a 90% precision classifier! As you can see, it is fairly easy to create a classifier with virtually any precision you want: just set a high enough threshold, and you’re done. But wait, not so fast: a high-precision classifier is not very useful if its recall is too low! For many applications, 48% recall wouldn’t be great at all.\nThere are two classes you can use to more easily adjust the decision threshold:\n\nThe FixedThresholdClassifier class lets you wrap a binary classifier and set the desired threshold manually. If the underlying classifier has a predict_proba() method, then the threshold should be a value between 0 and 1 (the default is 0.5). Otherwise, it should be a decision score, comparable to the output of the model’s decision_function() (the default is 0).\nThe TunedThresholdClassifierCV class uses k-fold cross-validation to automatically find the optimal threshold for a given metric. By default, it tries to find the threshold that maximizes the model’s balanced accuracy: that’s the average of each class’s recall. However, you can select another metric to optimize for (see the documentation for the full list of options).\n\n\n\n2.3.3.5 ROC and AUC\nThe Receiver Operating Characteristic (ROC) curve is a visual representation of model performance across all thresholds. The long version of the name, receiver operating characteristic, is a holdover from WWII radar detection.\nThe ROC curve is drawn by calculating the true positive rate (TPR) and false positive rate (FPR) at every possible threshold (in practice, at selected intervals), then graphing TPR over FPR. A perfect model, which at some threshold has a TPR of 1.0 and a FPR of 0.0, can be represented by either a point at (0, 1) if all other thresholds are ignored, or by the following:\n\n\n\nROC and AUC of a hypothetical perfect model\n\n\nThe area under the curve (AUC) represents the probability that the model, if given a randomly chosen positive and negative example, will rank the positive higher than the negative.\nThe perfect model above, containing a square with sides of length 1, has an area under the curve (AUC) of 1.0. This means there is a 100% probability that the model will correctly rank a randomly chosen positive example higher than a randomly chosen negative example. In other words, looking at the spread of data points below, AUC gives the probability that the model will place a randomly chosen square to the right of a randomly chosen circle, independent of where the threshold is set.\n\n\n\nA spread of predictions for a binary classification model. AUC is the chance a randomly chosen square is positioned to the right of a randomly chosen circle.\n\n\nFor a binary classifier, a model that does exactly as well as random guesses or coin flips has a ROC that is a diagonal line from (0,0) to (1,1). The AUC is 0.5, representing a 50% probability of correctly ranking a random positive and negative example.\n\n\n\nROC and AUC of completely random guesses\n\n\nAUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. The model with greater area under the curve is generally the better one.\n\n\n\nROC and AUC of two hypothetical models. The curve on the right, with a greater AUC, represents the better of the two models.\n\n\nThe points on a ROC curve closest to (0,1) represent a range of the best-performing thresholds for the given model. The threshold you choose depends on which metric is most important to the specific use case. Consider the points A, B, and C in the following diagram, each representing a threshold:\n\n\n\nThree labeled points representing thresholds\n\n\nIf false positives (false alarms) are highly costly, it may make sense to choose a threshold that gives a lower FPR, like the one at point A, even if TPR is reduced. Conversely, if false positives are cheap and false negatives (missed true positives) highly costly, the threshold for point C, which maximizes TPR, may be preferable. If the costs are roughly equivalent, point B may offer the best balance between TPR and FPR.\nAUC and ROC work well for comparing models when the dataset is roughly balanced between classes. When the dataset is imbalanced, precision-recall curves (PRCs) and the area under those curves may offer a better comparative visualization of model performance. Precision-recall curves are created by plotting precision on the y-axis and recall on the x-axis across all thresholds.\n\n\n\nPR AUC\n\n\nTo plot the ROC curve, you first use the roc_curve() function to compute the TPR and FPR for various threshold values:\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n\n\nidx_for_threshold_at_90 = (thresholds &lt;= threshold_for_90_precision).argmax()\ntpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\nplt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\nplt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\nplt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n\n# extra code – just beautifies Figure 3–7\nplt.gca().add_patch(patches.FancyArrowPatch(\n    (0.20, 0.89), (0.07, 0.70),\n    connectionstyle=\"arc3,rad=.4\",\n    arrowstyle=\"Simple, tail_width=1.5, head_width=8, head_length=10\",\n    color=\"#444444\"))\nplt.text(0.12, 0.71, \"Higher\\nthreshold\", color=\"#333333\")\nplt.xlabel('False Positive Rate (Fall-Out)')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.axis([0, 1, 0, 1])\nplt.legend(loc=\"lower right\", fontsize=13)\n\nplt.show()\n\n\n\n\n\n\n\n\nOnce again there is a trade-off: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\n\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_train_5, y_scores)\n\n0.9604938554008616\n\n\nLet’s now create a RandomForestClassifier, whose PR curve and F1 score we can compare to those of the SGDClassifier:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state=42)\n\nWe can call the cross_val_predict() function to train the RandomForestClassifier using cross-validation and make it predict class probabilities for every image as follows:\n\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n                                    method=\"predict_proba\")\n\nLet’s look at the estimated class probabilities for the first two images in the training set:\n\ny_probas_forest[:2]\n\narray([[0.11, 0.89],\n       [0.99, 0.01]])\n\n\nThe model predicts that the first image is positive with 89% probability, and it predicts that the second image is negative with 99% probability. Since each image is either positive or negative, the estimated probabilities in each row add up to 100%.\nThe second column contains the estimated probabilities for the positive class, so let’s pass them to the precision_recall_curve() function:\n\ny_scores_forest = y_probas_forest[:, 1]\nprecisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n    y_train_5, y_scores_forest)\n\n\nplt.figure(figsize=(6, 5))  # extra code – not needed, just formatting\n\nplt.plot(recalls_forest, precisions_forest, \"b-\", linewidth=2,\n         label=\"Random Forest\")\nplt.plot(recalls, precisions, \"--\", linewidth=2, label=\"SGD\")\n\n# extra code – just beautifies Figure 3–8\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid(True)\nplt.legend(loc=\"lower left\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe RandomForestClassifier’s PR curve looks much better than the SGDClassifier’s: it comes much closer to the top-right corner. Its F1 score and ROC AUC score are also significantly better:\n\ny_train_pred_forest = y_probas_forest[:, 1] &gt;= 0.5  # positive proba ≥ 50%\nf1_score(y_train_5, y_train_pred_forest)\n\n0.9274509803921569\n\n\n\nroc_auc_score(y_train_5, y_scores_forest)\n\n0.9983436731328145\n\n\n\nprecision_score(y_train_5, y_train_pred_forest)\n\n0.9897468089558485\n\n\n\nrecall_score(y_train_5, y_train_pred_forest)\n\n0.8725327430363402\n\n\nYou now know how to train binary classifiers, choose the appropriate metric for your task, evaluate your classifiers using cross-validation, select the precision/recall trade-off that fits your needs, and use several metrics and curves to compare various models. You’re ready to try to detect more than just the 5s.\n\n\n\n2.3.4 Multiclass Classification\nWhereas binary classifiers distinguish between two classes, multiclass classifiers (also called multinomial classifiers) can distinguish between more than two classes.\nSome Scikit-Learn classifiers (e.g., LogisticRegression, RandomForestClassifier, and GaussianNB) are capable of handling multiple classes natively. Others are strictly binary classifiers (e.g., SGDClassifier and SVC). However, there are various strategies that you can use to perform multiclass classification with multiple binary classifiers.\nOne way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on). Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score. This is called the one-versus-the-rest (OvR) strategy, or sometimes one-versus-all (OvA).\nAnother strategy is to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45 binary classifiers! When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels. The main advantage of OvO is that each classifier only needs to be trained on the part of the training set containing the two classes that it must distinguish.\nScikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm. Let’s try this with a support vector machine classifier using the sklearn.svm.SVC class:\n\nfrom sklearn.svm import SVC\n\nsvm_clf = SVC(random_state=42)\nsvm_clf.fit(X_train[:2000], y_train[:2000])  # y_train, not y_train_5\n\nSVC(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC \n1.0\n\n\n\nkernel \n'rbf'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    \n\n\nWe trained the SVC using the original target classes from 0 to 9 (y_train), instead of the 5-versus-the-rest target classes (y_train_5). Since there are 10 classes (i.e., more than 2), Scikit-Learn used the OvO strategy and trained 45 binary classifiers. Now let’s make a prediction on an image:\n\nsvm_clf.predict([some_digit])\n\narray(['5'], dtype=object)\n\n\nThis code actually made 45 predictions—one per pair of classes—and it selected the class that won the most duels.⁠ If you call the decision_function() method, you will see that it returns 10 scores per instance: one per class. Each class gets a score equal to the number of won duels plus or minus a small tweak (max ±0.33) to break ties, based on the classifier scores:\n\nsome_digit_scores = svm_clf.decision_function([some_digit])\nsome_digit_scores.round(2)\n\narray([[ 3.79,  0.73,  6.06,  8.3 , -0.29,  9.3 ,  1.75,  2.77,  7.21,\n         4.82]])\n\n\n\nclass_id = some_digit_scores.argmax()\nclass_id\n\nnp.int64(5)\n\n\nWhen a classifier is trained, it stores the list of target classes in its classes_ attribute, ordered by value. In the case of MNIST, the index of each class in the classes_ array conveniently matches the class itself (e.g., the class at index 5 happens to be class ‘5’), but in general you won’t be so lucky; you will need to look up the class label like this:\n\nsvm_clf.classes_\n\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype=object)\n\n\n\nsvm_clf.classes_[class_id]\n\n'5'\n\n\n\n# extra code – shows how to get all 45 OvO scores if needed\nsvm_clf.decision_function_shape = \"ovo\"\nsome_digit_scores_ovo = svm_clf.decision_function([some_digit])\nsome_digit_scores_ovo.round(2)\n\narray([[ 0.11, -0.21, -0.97,  0.51, -1.01,  0.19,  0.09, -0.31, -0.04,\n        -0.45, -1.28,  0.25, -1.01, -0.13, -0.32, -0.9 , -0.36, -0.93,\n         0.79, -1.  ,  0.45,  0.24, -0.24,  0.25,  1.54, -0.77,  1.11,\n         1.13,  1.04,  1.2 , -1.42, -0.53, -0.45, -0.99, -0.95,  1.21,\n         1.  ,  1.  ,  1.08, -0.02, -0.67, -0.14, -0.3 , -0.13,  0.25]])\n\n\nIf you want to force Scikit-Learn to use one-versus-one or one-versus-the-rest, you can use the OneVsOneClassifier or OneVsRestClassifier classes. Simply create an instance and pass a classifier to its constructor (it doesn’t even have to be a binary classifier). For example, this code creates a multiclass classifier using the OvR strategy, based on an SVC:\n\nfrom sklearn.multiclass import OneVsRestClassifier\n\novr_clf = OneVsRestClassifier(SVC(random_state=42))\novr_clf.fit(X_train[:2000], y_train[:2000])\n\nOneVsRestClassifier(estimator=SVC(random_state=42))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.OneVsRestClassifier?Documentation for OneVsRestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nSVC(random_state=42)\n\n\n\nn_jobs \nNone\n\n\n\nverbose \n0\n\n\n\n\n            \n        \n    estimator: SVCSVC(random_state=42)SVC?Documentation for SVC\n        \n            \n                Parameters\n                \n\n\n\n\nC \n1.0\n\n\n\nkernel \n'rbf'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    \n\n\nLet’s make a prediction, and check the number of trained classifiers:\n\novr_clf.predict([some_digit])\n\narray(['5'], dtype='&lt;U1')\n\n\n\nlen(ovr_clf.estimators_)\n\n10\n\n\nTraining an SGDClassifier on a multiclass dataset and using it to make predictions is just as easy:\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit])\n\narray(['3'], dtype='&lt;U1')\n\n\nOops, that’s incorrect. Prediction errors do happen! This time Scikit-Learn used the OvR strategy under the hood: since there are 10 classes, it trained 10 binary classifiers. The decision_function() method now returns one value per class. Let’s look at the scores that the SGD classifier assigned to each class:\n\nsgd_clf.decision_function([some_digit]).round()\n\narray([[-31893., -34420.,  -9531.,   1824., -22320.,  -1386., -26189.,\n        -16148.,  -4604., -12051.]])\n\n\nYou can see that the classifier is not very confident about its prediction: almost all scores are very negative, while class 3 has a score of +1,824, and class 5 is not too far behind at –1,386. Of course, you’ll want to evaluate this classifier on more than one image. Since there are roughly the same number of images in each class, the accuracy metric is fine. As usual, you can use the cross_val_score() function to evaluate the model:\n\ncross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n\narray([0.87365, 0.85835, 0.8689 ])\n\n\nIt gets over 85.8% on all test folds. If you used a random classifier, you would get 10% accuracy, so this is not such a bad score, but you can still do much better. Simply scaling the inputs increases accuracy above 89.1%:\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(\"float64\"))\ncross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\n\narray([0.8983, 0.891 , 0.9018])\n\n\n\n\n2.3.5 Error Analysis\nIf this were a real project, you would now follow the steps in your machine learning project checklist (see Intro to DS Course). You’d explore data preparation options, try out multiple models, shortlist the best ones, fine-tune their hyperparameters using GridSearchCV, and automate as much as possible. Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.\nFirst, look at the confusion matrix. For this, you first need to make predictions using the cross_val_predict() function; then you can pass the labels and predictions to the confusion_matrix() function, just like you did earlier. However, since there are now 10 classes instead of 2, the confusion matrix will contain quite a lot of numbers, and it may be hard to read.\nA colored diagram of the confusion matrix is much easier to analyze. To plot such a diagram, use the ConfusionMatrixDisplay.from_predictions() function like this:\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nplt.rc('font', size=9)  # make the text smaller\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\nplt.show()\n\n\n\n\n\n\n\n\nThis confusion matrix looks pretty good: most images are on the main diagonal, which means that they were classified correctly. Notice that the cell on the diagonal in row #5 and column #5 looks slightly darker than the other digits. This could be because the model made more errors on 5s, or because there are fewer 5s in the dataset than the other digits. That’s why it’s important to normalize the confusion matrix by dividing each value by the total number of images in the corresponding (true) class (i.e., divide by the row’s sum). This can be done simply by setting normalize=\"true\". We can also specify the values_format=\".0%\" argument to show percentages with no decimals.\n\nplt.rc('font', size=10)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n                                        normalize=\"true\", values_format=\".0%\")\nplt.show()\n\n\n\n\n\n\n\n\nNow we can easily see that only 82% of the images of 5s were classified correctly. The most common error the model made with images of 5s was to misclassify them as 8s: this happened for 10% of all 5s. But only 2% of 8s got misclassified as 5s; confusion matrices are generally not symmetrical! If you look carefully, you will notice that many digits have been misclassified as 8s, but this is not immediately obvious from this diagram. If you want to make the errors stand out more, you can try putting zero weight on the correct predictions.\n\nsample_weight = (y_train_pred != y_train)\nplt.rc('font', size=10)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n                                        sample_weight=sample_weight,\n                                        normalize=\"true\", values_format=\".0%\")\nplt.show()\n\n\n\n\n\n\n\n\nNow you can see much more clearly the kinds of errors the classifier makes. The column for class 8 is now really bright, which confirms that many images got misclassified as 8s. In fact this is the most common misclassification for almost all classes. But be careful how you interpret the percentages in this diagram: remember that we’ve excluded the correct predictions. For example, the 36% in row #7, column #9 in the left grid does not mean that 36% of all images of 7s were misclassified as 9s. It means that 36% of the errors the model made on images of 7s were misclassifications as 9s. In reality, only 3% of images of 7s were misclassified as 9s.\nIt is also possible to normalize the confusion matrix by column rather than by row: if you set normalize=\"pred\", you get the diagram below:\n\nsample_weight = (y_train_pred != y_train)\nplt.rc('font', size=10)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n                                        sample_weight=sample_weight,\n                                        normalize=\"pred\", values_format=\".0%\")\nplt.show()\n\n\n\n\n\n\n\n\nLet’s put all plots in a couple of figures:\n\n# extra code\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\nplt.rc('font', size=8)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=axs[0])\naxs[0].set_title(\"Confusion matrix\")\nplt.rc('font', size=8)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=axs[1],\n                                        normalize=\"true\", values_format=\".0%\")\naxs[1].set_title(\"CM normalized by row\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# extra code\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\nplt.rc('font', size=8)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=axs[0],\n                                        sample_weight=sample_weight,\n                                        normalize=\"true\", values_format=\".0%\")\naxs[0].set_title(\"Errors normalized by row\")\nplt.rc('font', size=8)\nConfusionMatrixDisplay.from_predictions(y_train, y_train_pred, ax=axs[1],\n                                        sample_weight=sample_weight,\n                                        normalize=\"pred\", values_format=\".0%\")\naxs[1].set_title(\"Errors normalized by column\")\n\nplt.show()\nplt.rc('font', size=14)  # make fonts great again\n\n\n\n\n\n\n\n\nAnalyzing the confusion matrix often gives you insights into ways to improve your classifier. Looking at these plots, it seems that your efforts should be spent on reducing the false 8s. For example, you could try to gather more training data for digits that look like 8s (but are not) so that the classifier can learn to distinguish them from real 8s. Or you could engineer new features that would help the classifier—for example, writing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make some patterns, such as closed loops, stand out more.\nAnalyzing individual errors can also be a good way to gain insights into what your classifier is doing and why it is failing. For example, let’s plot examples of 3s and 5s in a confusion matrix style\n\ncl_a, cl_b = '3', '5'\nX_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\nX_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\nX_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\nX_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n\n\n# extra code\nsize = 5\npad = 0.2\nplt.figure(figsize=(size, size))\nfor images, (label_col, label_row) in [(X_ba, (0, 0)), (X_bb, (1, 0)),\n                                       (X_aa, (0, 1)), (X_ab, (1, 1))]:\n    for idx, image_data in enumerate(images[:size*size]):\n        x = idx % size + label_col * (size + pad)\n        y = idx // size + label_row * (size + pad)\n        plt.imshow(image_data.reshape(28, 28), cmap=\"binary\",\n                   extent=(x, x + 1, y, y + 1))\nplt.xticks([size / 2, size + pad + size / 2], [str(cl_a), str(cl_b)])\nplt.yticks([size / 2, size + pad + size / 2], [str(cl_b), str(cl_a)])\nplt.plot([size + pad / 2, size + pad / 2], [0, 2 * size + pad], \"k:\")\nplt.plot([0, 2 * size + pad], [size + pad / 2, size + pad / 2], \"k:\")\nplt.axis((0, 2 * size + pad, 0, 2 * size + pad))\nplt.xlabel(\"Predicted label\")\nplt.ylabel(\"True label\")\n\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, some of the digits that the classifier gets wrong (i.e., in the bottom-left and top-right blocks) are so badly written that even a human would have trouble classifying them. However, most misclassified images seem like obvious errors to us. It may be hard to understand why the classifier made the mistakes it did, but remember that the human brain is a fantastic pattern recognition system, and our visual system does a lot of complex preprocessing before any information even reaches our consciousness. So, the fact that this task feels simple does not mean that it is. Recall that we used a simple SGDClassifier, which is just a linear model: all it does is assign a weight per class to each pixel, and when it sees a new image it just sums up the weighted pixel intensities to get a score for each class. Since 3s and 5s differ by only a few pixels, this model will easily confuse them.\nThe main difference between 3s and 5s is the position of the small line that joins the top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left, the classifier might classify it as a 5, and vice versa. In other words, this classifier is quite sensitive to image shifting and rotation. One way to reduce the 3/5 confusion is to preprocess the images to ensure that they are well centered and not too rotated. However, this may not be easy since it requires predicting the correct rotation of each image. A much simpler approach consists of augmenting the training set with slightly shifted and rotated variants of the training images. This will force the model to learn to be more tolerant to such variations. This is called data augmentation\n\n\n2.3.6 Multilabel Classification\nUntil now, each instance has always been assigned to just one class. But in some cases you may want your classifier to output multiple classes for each instance. Consider a face-recognition classifier: what should it do if it recognizes several people in the same picture? It should attach one tag per person it recognizes. Say the classifier has been trained to recognize three faces: Alice, Bob, and Charlie. Then when the classifier is shown a picture of Alice and Charlie, it should output [True, False, True] (meaning “Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple binary tags is called a multilabel classification system.\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for illustration purposes:\n\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny_train_large = (y_train &gt;= '7')\ny_train_odd = (y_train.astype('int8') % 2 == 1)\ny_multilabel = np.c_[y_train_large, y_train_odd]\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_multilabel)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n5\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\nThis code creates a y_multilabel array containing two target labels for each digit image: the first indicates whether the digit is large (7, 8, or 9), and the second indicates whether it is odd. Then the code creates a KNeighborsClassifier instance, which supports multilabel classification (not all classifiers do), and trains this model using the multiple targets array. Now you can make a prediction, and notice that it outputs two labels:\n\nknn_clf.predict([some_digit])\n\narray([[False,  True]])\n\n\nAnd it gets it right! The digit 5 is indeed not large (False) and odd (True).\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric really depends on your project. One approach is to measure the F1 score for each individual label (or any other binary classifier metric discussed earlier), then simply compute the average score. The following code computes the average F1 score across all labels:\n\ny_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\nf1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n\n0.9764102655606048\n\n\n\n# extra code – shows that we get a negligible performance improvement when we\n#              set average=\"weighted\" because the classes are already pretty\n#              well balanced.\nf1_score(y_multilabel, y_train_knn_pred, average=\"weighted\")\n\n0.9778357403921755\n\n\nThis approach assumes that all labels are equally important, which may not be the case. In particular, if you have many more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier’s score on pictures of Alice. One simple option is to give each label a weight equal to its support (i.e., the number of instances with that target label). To do this, simply set average=\"weighted\" when calling the f1_score() function.\nIf you wish to use a classifier that does not natively support multilabel classification, such as SVC, one possible strategy is to train one model per label. However, this strategy may have a hard time capturing the dependencies between the labels. For example, a large digit (7, 8, or 9) is twice more likely to be odd than even, but the classifier for the “odd” label does not know what the classifier for the “large” label predicted. To solve this issue, the models can be organized in a chain: when a model makes a prediction, it uses the input features plus all the predictions of the models that come before it in the chain.\nThe good news is that Scikit-Learn has a class called ClassifierChain that does just that! By default it will use the true labels for training, feeding each model the appropriate labels depending on their position in the chain. But if you set the cv hyperparameter, it will use cross-validation to get “clean” (out-of-sample) predictions from each trained model for every instance in the training set, and these predictions will then be used to train all the models later in the chain. Note that the order of the classifiers in the chain may affect the final performance. Here’s an example showing how to create and train a ClassifierChain using the cross-validation strategy. As earlier, we’ll just use the first 2,000 images in the training set to speed things up:\n\nfrom sklearn.multioutput import ClassifierChain\n\nchain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\nchain_clf.fit(X_train[:2000], y_multilabel[:2000])\n\nClassifierChain(cv=3, estimator=SVC(), random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ClassifierChain?Documentation for ClassifierChainiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nSVC()\n\n\n\norder \nNone\n\n\n\ncv \n3\n\n\n\nchain_method \n'predict'\n\n\n\nrandom_state \n42\n\n\n\nverbose \nFalse\n\n\n\nbase_estimator \n'deprecated'\n\n\n\n\n            \n        \n    estimator: SVCSVC()SVC?Documentation for SVC\n        \n            \n                Parameters\n                \n\n\n\n\nC \n1.0\n\n\n\nkernel \n'rbf'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\n\nchain_clf.predict([some_digit])\n\narray([[0., 1.]])\n\n\n\n\n2.3.7 Multioutput Classification\nThe last type of classification task we’ll discuss here is called multioutput–multiclass classification (or just multioutput classification). It is a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).\nTo illustrate this, let’s build a system that removes noise from images. It will take as input a noisy digit image, and it will (hopefully) output a clean digit image, represented as an array of pixel intensities, just like the MNIST images. Notice that the classifier’s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). This is thus an example of a multioutput classification system.\nNote:\nThe line between classification and regression is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have a system that outputs multiple labels per instance, including both class labels and value labels.\nLet’s start by creating the training and test sets by taking the MNIST images and adding noise to their pixel intensities, using a random number generator’s integers() method. The target images will be the original images:\n\nrng = np.random.default_rng(seed=42)\nnoise_train = rng.integers(0, 100, (len(X_train), 784))\nX_train_mod = X_train + noise_train\nnoise_test = rng.integers(0, 100, (len(X_test), 784))\nX_test_mod = X_test + noise_test\ny_train_mod = X_train\ny_test_mod = X_test\n\nLet’s take a peek at the first image from the test set. Yes, we’re snooping on the test data, so you should be frowning right now.\n\n# extra code\nplt.subplot(121); plot_digit(X_test_mod[0])\nplt.subplot(122); plot_digit(y_test_mod[0])\n\nplt.show()\n\n\n\n\n\n\n\n\nOn the left is the noisy input image, and on the right is the clean target image. Now let’s train the classifier and make it clean up this image:\n\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train_mod, y_train_mod)\nclean_digit = knn_clf.predict([X_test_mod[0]])\nplot_digit(clean_digit)\nplt.show()\n\n\n\n\n\n\n\n\nLooks close enough to the target! This concludes our tour of classification. You now know how to select good metrics for classification tasks, pick the appropriate precision/recall trade-off, compare classifiers, and more generally build good classification systems for a variety of tasks. In the next lesson, you’ll learn how all these machine learning models you’ve been using actually work.\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapter 3 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, 3rd edition"
  },
  {
    "objectID": "ML/2_2_slf.html#foundational-supervised-learning-concepts",
    "href": "ML/2_2_slf.html#foundational-supervised-learning-concepts",
    "title": "Supervised Learning",
    "section": "Foundational supervised learning concepts",
    "text": "Foundational supervised learning concepts\nSupervised machine learning is based on the following core concepts:\n\nData\nModel\nTraining\nEvaluating\nInference"
  },
  {
    "objectID": "ML/2_2_slf.html#data",
    "href": "ML/2_2_slf.html#data",
    "title": "Supervised Learning",
    "section": "Data",
    "text": "Data\nData is the driving force of ML. Data comes in the form of words and numbers stored in tables, or as the values of pixels and waveforms captured in images and audio files. We store related data in datasets. For example, we might have a dataset of the following:\n\nImages of cats\nHousing prices\nWeather information"
  },
  {
    "objectID": "ML/2_2_slf.html#data-1",
    "href": "ML/2_2_slf.html#data-1",
    "title": "Supervised Learning",
    "section": "Data",
    "text": "Data\nDatasets are made up of individual examples that contain features and a label. You could think of an example as analogous to a single row in a spreadsheet. Features are the values that a supervised model uses to predict the label. The label is the “answer,” or the value we want the model to predict. In a weather model that predicts rainfall, the features could be latitude, longitude, temperature, humidity, cloud coverage, wind direction, and atmospheric pressure. The label would be rainfall amount."
  },
  {
    "objectID": "ML/2_2_slf.html#labeled-examples",
    "href": "ML/2_2_slf.html#labeled-examples",
    "title": "Supervised Learning",
    "section": "Labeled examples",
    "text": "Labeled examples\nExamples that contain both features and a label are called labeled examples."
  },
  {
    "objectID": "ML/2_2_slf.html#unlabeled-examples",
    "href": "ML/2_2_slf.html#unlabeled-examples",
    "title": "Supervised Learning",
    "section": "Unlabeled examples",
    "text": "Unlabeled examples\nIn contrast, unlabeled examples contain features, but no label. After you create a model, the model predicts the label from the features."
  },
  {
    "objectID": "ML/2_2_slf.html#dataset-characteristics",
    "href": "ML/2_2_slf.html#dataset-characteristics",
    "title": "Supervised Learning",
    "section": "Dataset characteristics",
    "text": "Dataset characteristics\nA dataset is characterized by its size and diversity. Size indicates the number of examples. Diversity indicates the range those examples cover. Good datasets are both large and highly diverse.\nDatasets can be large and diverse, or large but not diverse, or small but highly diverse. In other words, a large dataset doesn’t guarantee sufficient diversity, and a dataset that is highly diverse doesn’t guarantee sufficient examples."
  },
  {
    "objectID": "ML/2_2_slf.html#dataset-characteristics-1",
    "href": "ML/2_2_slf.html#dataset-characteristics-1",
    "title": "Supervised Learning",
    "section": "Dataset characteristics",
    "text": "Dataset characteristics\nFor instance, a dataset might contain 100 years worth of data, but only for the month of July. Using this dataset to predict rainfall in January would produce poor predictions. Conversely, a dataset might cover only a few years but contain every month. This dataset might produce poor predictions because it doesn’t contain enough years to account for variability."
  },
  {
    "objectID": "ML/2_2_slf.html#dataset-characteristics-2",
    "href": "ML/2_2_slf.html#dataset-characteristics-2",
    "title": "Supervised Learning",
    "section": "Dataset characteristics",
    "text": "Dataset characteristics\nA dataset can also be characterized by the number of its features. For example, some weather datasets might contain hundreds of features, ranging from satellite imagery to cloud coverage values. Other datasets might contain only three or four features, like humidity, atmospheric pressure, and temperature. Datasets with more features can help a model discover additional patterns and make better predictions. However, datasets with more features don’t always produce models that make better predictions because some features might have no causal relationship to the label."
  },
  {
    "objectID": "ML/2_2_slf.html#model",
    "href": "ML/2_2_slf.html#model",
    "title": "Supervised Learning",
    "section": "Model",
    "text": "Model\nIn supervised learning, a model is the complex collection of numbers that define the mathematical relationship from specific input feature patterns to specific output label values. The model discovers these patterns through training."
  },
  {
    "objectID": "ML/2_2_slf.html#training",
    "href": "ML/2_2_slf.html#training",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nBefore a supervised model can make predictions, it must be trained. To train a model, we give the model a dataset with labeled examples. The model’s goal is to work out the best solution for predicting the labels from the features. The model finds the best solution by comparing its predicted value to the label’s actual value. Based on the difference between the predicted and actual values—defined as the loss—the model gradually updates its solution. In other words, the model learns the mathematical relationship between the features and the label so that it can make the best predictions on unseen data."
  },
  {
    "objectID": "ML/2_2_slf.html#training-1",
    "href": "ML/2_2_slf.html#training-1",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nFor example, if the model predicted 1.15 inches of rain, but the actual value was 0.75 inches, the model modifies its solution so its prediction is closer to 0.75 inches. After the model has looked at each example in the dataset—in some cases, multiple times—it arrives at a solution that makes the best predictions, on average, for each of the examples."
  },
  {
    "objectID": "ML/2_2_slf.html#training-2",
    "href": "ML/2_2_slf.html#training-2",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nThe model takes in a single labeled example and provides a prediction.\n\nAn ML model making a prediction from a labeled example"
  },
  {
    "objectID": "ML/2_2_slf.html#training-3",
    "href": "ML/2_2_slf.html#training-3",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nThe model compares its predicted value with the actual value and updates its solution.\n\nAn ML model updating its predicted value"
  },
  {
    "objectID": "ML/2_2_slf.html#training-4",
    "href": "ML/2_2_slf.html#training-4",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nThe model repeats this process for each labeled example in the dataset.\n\nAn ML model updating its predictions for each labeled example in the training dataset."
  },
  {
    "objectID": "ML/2_2_slf.html#training-5",
    "href": "ML/2_2_slf.html#training-5",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nIn this way, the model gradually learns the correct relationship between the features and the label. This gradual understanding is also why large and diverse datasets produce a better model. The model has seen more data with a wider range of values and has refined its understanding of the relationship between the features and the label."
  },
  {
    "objectID": "ML/2_2_slf.html#training-6",
    "href": "ML/2_2_slf.html#training-6",
    "title": "Supervised Learning",
    "section": "Training",
    "text": "Training\nDuring training, ML practitioners can make subtle adjustments to the configurations and features the model uses to make predictions. For example, certain features have more predictive power than others. Therefore, ML practitioners can select which features the model uses during training. For example, suppose a weather dataset contains time_of_day as a feature. In this case, an ML practitioner can add or remove time_of_day during training to see whether the model makes better predictions with or without it."
  },
  {
    "objectID": "ML/2_2_slf.html#evaluating",
    "href": "ML/2_2_slf.html#evaluating",
    "title": "Supervised Learning",
    "section": "Evaluating",
    "text": "Evaluating\nWe evaluate a trained model to determine how well it learned. When we evaluate a model, we use a labeled dataset, but we only give the model the dataset’s features. We then compare the model’s predictions to the label’s true values.\nDepending on the model’s predictions, we might do more training and evaluating before deploying the model in a real-world application."
  },
  {
    "objectID": "ML/2_2_slf.html#evaluating-1",
    "href": "ML/2_2_slf.html#evaluating-1",
    "title": "Supervised Learning",
    "section": "Evaluating",
    "text": "Evaluating\n\nEvaluating an ML model by comparing its predictions to the actual values."
  },
  {
    "objectID": "ML/2_2_slf.html#inference",
    "href": "ML/2_2_slf.html#inference",
    "title": "Supervised Learning",
    "section": "Inference",
    "text": "Inference\nOnce we’re satisfied with the results from evaluating the model, we can use the model to make predictions, called inferences, on unlabeled examples. In the weather app example, we would give the model the current weather conditions—like temperature, atmospheric pressure, and relative humidity—and it would predict the amount of rainfall."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome Gen. 2026-1!",
    "section": "",
    "text": "SIAFI Data Science and Machine Learning Handbook\nThis handbook will guide you through the main concepts and tools used in this course. It is organized as follows:\n\nIntroduction to Data Science\nMachine Learning\nDeep Learning\n\nPlease feel free to explore the contents and never stop learning.\nYou can find the source code for this handbook in the GitHub repository."
  },
  {
    "objectID": "IDS/1_3_end_to_end_ml_project.html",
    "href": "IDS/1_3_end_to_end_ml_project.html",
    "title": "1. Introduction to Data Science",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "IDS/1_3_end_to_end_ml_project.html#end-to-end-machine-learning-project",
    "href": "IDS/1_3_end_to_end_ml_project.html#end-to-end-machine-learning-project",
    "title": "1. Introduction to Data Science",
    "section": "1.3 End-to-End Machine Learning Project",
    "text": "1.3 End-to-End Machine Learning Project\nIn this section, you will work through an example project end to end, pretending to be a recently hired data scientist at a real estate company. The goal of this example is to illustrate the main steps of a machine learning project, not to learn everything about the real estate business. Here are the main steps we will go through:\n\nLook at the big picture: What is the business problem we are trying to solve? How can we measure success? What is the current solution?\nGet the data: Where does the data come from? How can we get it?\nExplore and visualize the data to gain insights: What is in the data? What are the main characteristics of the data? What are the relationships between different attributes?\nPrepare the data for machine learning algorithms: How can we clean the data? How can we transform the data into a format that is suitable for machine learning algorithms?\nSelect and train a model: What are the different types of machine learning algorithms? How can we select the best algorithm for our problem? How can we train the model?\nFine-tune the model: How can we improve the model’s performance? How can we select the best hyperparameters?\nPresent the solution: How can we communicate the results to stakeholders? How can we present the solution in a way that is easy to understand?\nLaunch, monitor, and maintain the system: How can we deploy the model in a production environment? How can we monitor the model’s performance over time? How can we update the model as new data becomes available?\n\n\n1.3.1 Working with Real Data\nWhen learning about machine learning, it is best to experiment with real-world data, not artificial datasets. Fortunately, there are thousands of open datasets to choose from, ranging across all sorts of domains. Here are a few places you can look to get data:\n\nPopular open data repositories:\n\nOpenML.org\nKaggle.com\nUCI Machine Learning Repository\nAmazon AWS Open Data Registry\nGoogle Dataset Search\nTensorFlow Datasets\nPyTorch Datasets\nScikit-learn Datasets\n\nMeta portals that list many datasets:\n\nDataPortals.org\nAwesome Public Datasets\n\nOther pages listing many popular data repositories:\n\nWikipedia: List of datasets for machine-learning research\nQuora.com\nThe datasets subreddit\n\n\nIn this example, we will use the California housing dataset, which contains information about various districts in California, including features such as median income, average house age, and average number of rooms. This dataset is available in the StatLib repository and is also included in the Scikit-learn library.\n\n\n\nCalifornia housing prices\n\n\n\n\n1.3.2 Look at the Big Picture\nOur first task is to use California census data to build a model of housing prices in the state. This model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.\nThe first question to ask is what exactly the business objective is. Building a model is probably not the end goal. How does the company expect to use and benefit from this model? Knowing the objective is important because it will determine how you frame the problem, which algorithms you will select, which performance measure you will use to evaluate your model, and how much effort you will spend tweaking it.\n\n1.3.2.1 Pipelines\nA sequence of data processing components is called a data pipeline. Pipelines are very common in machine learning systems, since there is a lot of data to manipulate and many data transformations to apply.\nComponents typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls in this data and spits out its own output. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust.\nOn the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system’s performance drops.\n\n\n\nMachine learning real estate pipeline\n\n\n\n\n1.3.2.2 Frame the Problem\nFirst, determine what kind of training supervision the model will need: is it a supervised, unsupervised, semi-supervised, self-supervised, or reinforcement learning task? And is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\nThis is clearly a typical supervised learning task, since the model can be trained with labeled examples (each instance comes with the expected output, i.e., the district’s median housing price). It is a typical regression task, since the model will be asked to predict a value. More specifically, this is a multiple regression problem, since the system will use multiple features to make a prediction (the district’s population, the median income, etc.). It is also a univariate regression problem, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust changing data rapidly, and data is small enough to fit in memory, so plain batch learning should do just fine.\n\n\n1.3.2.3 Select a Performance Measure\nA typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight given to large errors.\n\\[ RMSE(\\textbf{X}, h) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} \\left( h(\\textbf{x}^{(i)}) - y^{(i)} \\right) ^2} \\]\nwhere:\n\n\\(\\textbf{x}^{(i)}\\) is a vector of all the feature values of the \\(i^{th}\\) instance in the dataset \\(\\textbf{X}\\), and \\(y^{(i)}\\) is the corresponding target value or label (the median housing price in this case).\n\\(\\textbf{X}\\) is a matrix containing all the feature values (excluding labels) of all instances in the dataset. There is one row per instance, and the \\(i^{th}\\) row contains is equal to the transpose of the feature vector \\(\\textbf{x}^{(i)}\\).\n\\(m\\) is the number of instances in the dataset \\(\\textbf{X}\\)\n\\(h\\) is your system’s prediction function, also called hypothesis. When a system is given instance’s feature vector \\(\\textbf{x}^{(i)}\\), it outputs a predicted value \\(\\hat{y}^{(i)} = h(\\textbf{x}^{(i)})\\) for that instance.\n\nAlthough the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. For example, if there are many outliers in the data, you may want to use the Mean Absolute Error (MAE) instead, since it is less sensitive to outliers.\n\\[ MAE(\\textbf{X}, h) = \\frac{1}{m} \\sum_{i=1}^{m} | h(\\textbf{x}^{(i)}) - y^{(i)} | \\]\n\n\n\n1.3.3 Get the Data\nIn typical environments your data would be available in a relational database or some other common data store, and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations⁠ and familiarize yourself with the data schema. In this project, however, things are much simpler: We are going to download a single compressed file, housing.tgz, which contains a comma-separated values (CSV) file called housing.csv with all the data\n\nfrom pathlib import Path\nimport pandas as pd\nimport tarfile\nimport urllib.request\n\ndef load_housing_data():\n    tarball_path = Path(\"datasets/housing.tgz\")\n    if not tarball_path.is_file():\n        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n        urllib.request.urlretrieve(url, tarball_path)\n    with tarfile.open(tarball_path) as housing_tarball:\n            housing_tarball.extractall(path=\"datasets\", filter='data')\n    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n\n\n1.3.3.1 Take a Quick Look at the Data Structure\n\nhousing = load_housing_data()\nhousing\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n-121.09\n39.48\n25.0\n1665.0\n374.0\n845.0\n330.0\n1.5603\n78100.0\nINLAND\n\n\n20636\n-121.21\n39.49\n18.0\n697.0\n150.0\n356.0\n114.0\n2.5568\n77100.0\nINLAND\n\n\n20637\n-121.22\n39.43\n17.0\n2254.0\n485.0\n1007.0\n433.0\n1.7000\n92300.0\nINLAND\n\n\n20638\n-121.32\n39.43\n18.0\n1860.0\n409.0\n741.0\n349.0\n1.8672\n84700.0\nINLAND\n\n\n20639\n-121.24\n39.37\n16.0\n2785.0\n616.0\n1387.0\n530.0\n2.3886\n89400.0\nINLAND\n\n\n\n\n20640 rows × 10 columns\n\n\n\nThe info() method is useful to get a quick description of the data, in particular the total number of rows, each attribute’s type, and the number of non-null val\n\nhousing.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\n\nhousing['ocean_proximity'].value_counts()\n\nocean_proximity\n&lt;1H OCEAN     9136\nINLAND        6551\nNEAR OCEAN    2658\nNEAR BAY      2290\nISLAND           5\nName: count, dtype: int64\n\n\n\nhousing.describe()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20433.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n-119.569704\n35.631861\n28.639486\n2635.763081\n537.870553\n1425.476744\n499.539680\n3.870671\n206855.816909\n\n\nstd\n2.003532\n2.135952\n12.585558\n2181.615252\n421.385070\n1132.462122\n382.329753\n1.899822\n115395.615874\n\n\nmin\n-124.350000\n32.540000\n1.000000\n2.000000\n1.000000\n3.000000\n1.000000\n0.499900\n14999.000000\n\n\n25%\n-121.800000\n33.930000\n18.000000\n1447.750000\n296.000000\n787.000000\n280.000000\n2.563400\n119600.000000\n\n\n50%\n-118.490000\n34.260000\n29.000000\n2127.000000\n435.000000\n1166.000000\n409.000000\n3.534800\n179700.000000\n\n\n75%\n-118.010000\n37.710000\n37.000000\n3148.000000\n647.000000\n1725.000000\n605.000000\n4.743250\n264725.000000\n\n\nmax\n-114.310000\n41.950000\n52.000000\n39320.000000\n6445.000000\n35682.000000\n6082.000000\n15.000100\n500001.000000\n\n\n\n\n\n\n\nThe count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (so, for example, the count of total_bedrooms is 20,433, not 20,640). The std row shows the standard deviation, which measures how dispersed the values are.⁠ The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations fall. For example, 25% of the districts have a housing_median_age lower than 18, while 50% are lower than 29 and 75% are lower than 37. These are often called the 25th percentile (or first quartile), the median, and the 75th percentile (or third quartile).\nAnother quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have given a value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the entire DataFrame to plot a histogram for each numerical attribute. Here is the result:\n\nimport matplotlib.pyplot as plt\n\nhousing.hist(bins=50, figsize=(16, 8))\nplt.show()\n\n\n\n\n\n\n\n\nLooking at these histograms, you notice a few things:\n\nFirst, the median income attribute does not look like it is expressed in US dollars (USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually, 15.0001) for higher median incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in machine learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\nThe housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your machine learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:\n\nCollect proper labels for the districts whose labels were capped.\nRemove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\n\nThese attributes have very different scales. We will discuss this later in this chapter, when we explore feature scaling.\nFinally, many histograms are skewed right: they extend much farther to the right of the median than to the left. This may make it a bit harder for some machine learning algorithms to detect patterns in the data. Later, we’ll try to tranform these attributes to have a more symmetrical and bell-shaped distribution.\n\n\n\n1.3.3.2 Create a Test Set\nIt may seem strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which also means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of machine learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called data snooping bias.\nCreating a test set is theoretically simple; pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside.\nScikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is train_test_split(). There is a random_state parameter that allows you to set the random generator seed. Second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is useful when you have a separate set of labels). Here is how to use it:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\nHowever, we are considering a purely random sampling method. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When employees at a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population, with regard to the questions they want to ask. For example, the US population is 51.1% females and 48.9% males, so a well-conducted survey in the US would try to maintain this ratio in the sample: 511 females and 489 males (at least if it seems possible that the answers may vary across genders). This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population. If the people running the survey used purely random sampling, there would be about a 10.7% chance of sampling a skewed test set with less than 48.5% female or more than 53.5% female participants. Either way, the survey results would likely be quite biased.\nSuppose you’ve chatted with some experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely (back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of a stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the pd.cut() function to create an income category attribute with five categories (labeled from 1 to 5); category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on:\n\nhousing['income_cat'] = pd.cut(housing['median_income'],\n                               bins=[0., 1.5, 3.0, 4.5, 6., float('inf')],\n                               labels=[1, 2, 3, 4, 5])\n\nhousing['income_cat'].value_counts().sort_index().plot.bar(rot=0, grid=True)\nplt.xlabel('Income Category')\nplt.ylabel('Number of Districts')\nplt.show()\n\n\n\n\n\n\n\n\nWe can use the train_test_split() function with the stratify parameter to perform stratified sampling based on the income category:\n\nstrat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, stratify=housing['income_cat'])\n\nstrat_test_set['income_cat'].value_counts() / len(strat_test_set)\n\nincome_cat\n3    0.350533\n2    0.318798\n4    0.176357\n5    0.114341\n1    0.039971\nName: count, dtype: float64\n\n\n\n# extra code – computes the data for proportions in the full dataset\n\ndef income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() / len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall %\": income_cat_proportions(housing),\n    \"Stratified %\": income_cat_proportions(strat_test_set),\n    \"Random %\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props.index.name = \"Income Category\"\ncompare_props[\"Strat. Error %\"] = (compare_props[\"Stratified %\"] / compare_props[\"Overall %\"] - 1)\ncompare_props[\"Rand. Error %\"] = (compare_props[\"Random %\"] / compare_props[\"Overall %\"] - 1)\n(compare_props * 100).round(2)\n\n\n\n\n\n\n\n\nOverall %\nStratified %\nRandom %\nStrat. Error %\nRand. Error %\n\n\nIncome Category\n\n\n\n\n\n\n\n\n\n1\n3.98\n4.00\n4.24\n0.36\n6.45\n\n\n2\n31.88\n31.88\n30.74\n-0.02\n-3.59\n\n\n3\n35.06\n35.05\n34.52\n-0.01\n-1.53\n\n\n4\n17.63\n17.64\n18.41\n0.03\n4.42\n\n\n5\n11.44\n11.43\n12.09\n-0.08\n5.63\n\n\n\n\n\n\n\n\n# We can now remove the `income_cat` attribute so the data is back to its original state.\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)\n\n\n\n\n1.3.4 Explore and Visualize the Data to Gain Insights\nFirst, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast during the exploration phase. In this case, the training set is quite small, so you can just work directly on the full set. Since we are going to modify the training set a lot during the exploration phase, it is a good idea to make a copy of it so that you can always go back to the original data if necessary:\n\nhousing = strat_train_set.copy()\n\n\n1.3.4.1 Visualizing Geographical Data\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n# plt.savefig('img/1_2/bad_visualization.png')\nplt.show()\n\n\n\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2, grid=True)\n# plt.savefig('img/1_2/better_visualization.png')\nplt.show()\n\n\n\n\n\n\n\n\nYou can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high-density areas in the Central Valley (in particular, around Sacramento and Fresno).\n\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n             s=housing[\"population\"] / 100, label=\"population\",\n             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n             legend=True, sharex=False, figsize=(10, 7))\n# plt.savefig(\"img/1_2/housing_prices_scatterplot.png\")\nplt.show()\n\n\n\n\n\n\n\n\nThis picture tells you that the housing prices are much related to the location (e.g. close to the ocean) and to the population density (the size of each circle is proportional to the district’s population). The ocean proximity attribute may be useful as well, although in Northern California the prices in coastal districts are not too high, so it’s not a simple rule.\n\n\n1.3.4.2 Look for Correlations\n\n\n\nCorrelation examples\n\n\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the corr() method:\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688380\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\nlongitude            -0.050859\nlatitude             -0.139584\nName: median_house_value, dtype: float64\n\n\nAnother way to check for correlation between attributes is to use the Pandas scatter_matrix() function, which plots every numerical attribute against every other numerical attribute. We decide to focus on a few promising attributes (including the target attribute, median_house_value):\n\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\n# plt.savefig(\"img/1_2/scatter_matrix_plot.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1, grid=True)\n# plt.savefig(\"img/1_2/income_vs_house_value_scatterplot.png\")\nplt.show()\n\n\n\n\n\n\n\n\nThis plot reveals a few things. First, correlation is indeed quite strong; you can clearly see the upward trend, and the points are not too dispersed. Second, the price cap you noticed earlier is clearly visible as a horizontal line around $500,000. But the plot also reveals other less obvious straight lines: a horizontal line around $450,000,another around $350,000, perhaps aone around $280,000, and a few vertical lines as well. You may want to try removing the corresponding districts to prevent your algorithms from learning these quirks.\n\n\n1.3.4.3 Experiment with Attribute Combinations\nOne last thing you may want to do before preparing the data for machine learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. You create these new attributes as follows:\n\nhousing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\nhousing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\nhousing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]\n\nThen we can look at the correlation matrix again to see how these new attributes compare to the others:\n\ncorr_matrix = housing.corr(numeric_only=True)\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n\nmedian_house_value    1.000000\nmedian_income         0.688380\nrooms_per_house       0.143663\ntotal_rooms           0.137455\nhousing_median_age    0.102175\nhouseholds            0.071426\ntotal_bedrooms        0.054635\npopulation           -0.020153\npeople_per_house     -0.038224\nlongitude            -0.050859\nlatitude             -0.139584\nbedrooms_ratio       -0.256397\nName: median_house_value, dtype: float64\n\n\nThe new bedrooms_ratio attribute is much more correlated with the median house value than the total number of rooms or bedrooms. It’s a strong negative correlation, so it looks like houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district -obviously the larger the houses, the more expensive they are.\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.\n\n\n\n1.3.5 Prepare the Data for Machine Learning Algorithms\nLet’s revert to the original training set and separate the target (note that strat_train_set.drop() creates a copy of strat_train_set without the column, it doesn’t actually modify strat_train_set itself, unless you pass inplace=True):\n\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\n\n\n1.3.5.1 Data Cleaning\nMost machine learning algorithms cannot work with missing features, so you’ll need to take care of these. For example, you noticed earlier that the total_bedrooms attribute has some missing values. You have three options to fix this:\n\nGet rid of the corresponding districts.\nGet rid of the whole attribute.\nSet the missing values to some value (zero, the mean, the median, etc.). This is called imputation.\n\nYou can accomplish these easily using the Pandas DataFrame’s dropna(), drop(), and fillna() methods:\nhousing.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1\n\nhousing.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2\n\nmedian = housing[\"total_bedrooms\"].median()  # option 3\nhousing[\"total_bedrooms\"] = housing[\"total_bedrooms\"].fillna(median)\nIt seems best to go for option 3 since it is the least destructive, but instead of the preceding code, we can use a handy Scikit-Learn class: SimpleImputer. The benefit is that it will store the median value of each feature: this will make it possible to impute missing values not only on the training set, but also on the validation set, the test set, and any new data fed to the model. To use it, first you need to create a SimpleImputer instance, specifying that you want to replace each attribute’s missing values with the median of that attribute:\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")\n\nSeparating out the numerical attributes to use the “median” strategy (as it cannot be calculated on text attributes like ocean_proximity):\n\nimport numpy as np\n\nhousing_num = housing.select_dtypes(include=[np.number])\nimputer.fit(housing_num)\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputer?Documentation for SimpleImputeriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    \n\n\nThe imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable. Only the total_bedrooms attribute had missing values, but you cannot be sure that there won’t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:\n\nimputer.statistics_\n\narray([-118.51  ,   34.26  ,   29.    , 2125.    ,  434.    , 1167.    ,\n        408.    ,    3.5385])\n\n\n\nhousing_num.median()\n\nlongitude             -118.5100\nlatitude                34.2600\nhousing_median_age      29.0000\ntotal_rooms           2125.0000\ntotal_bedrooms         434.0000\npopulation            1167.0000\nhouseholds             408.0000\nmedian_income            3.5385\ndtype: float64\n\n\nTransforming the training set:\n\nX = imputer.transform(housing_num)\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\nhousing_tr\n\n#from sklearn import set_config\n# set_config(transform_output=\"pandas\")  # scikit-learn &gt;= 1.2\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n13096\n-122.42\n37.80\n52.0\n3321.0\n1115.0\n1576.0\n1034.0\n2.0987\n\n\n14973\n-118.38\n34.14\n40.0\n1965.0\n354.0\n666.0\n357.0\n6.0876\n\n\n3785\n-121.98\n38.36\n33.0\n1083.0\n217.0\n562.0\n203.0\n2.4330\n\n\n14689\n-117.11\n33.75\n17.0\n4174.0\n851.0\n1845.0\n780.0\n2.2618\n\n\n20507\n-118.15\n33.77\n36.0\n4366.0\n1211.0\n1912.0\n1172.0\n3.5292\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14207\n-118.40\n33.86\n41.0\n2237.0\n597.0\n938.0\n523.0\n4.7105\n\n\n13105\n-119.31\n36.32\n23.0\n2945.0\n592.0\n1419.0\n532.0\n2.5733\n\n\n19301\n-117.06\n32.59\n13.0\n3920.0\n775.0\n2814.0\n760.0\n4.0616\n\n\n19121\n-118.40\n34.06\n37.0\n3781.0\n873.0\n1725.0\n838.0\n4.1455\n\n\n19888\n-122.41\n37.66\n44.0\n431.0\n195.0\n682.0\n212.0\n3.2833\n\n\n\n\n16512 rows × 8 columns\n\n\n\nMissing values can also be replaced with the mean value (strategy=\"mean\"), or with the most frequent value (strategy=\"most_frequent\"), or with a constant value (strategy=\"constant\", fill_value=…​). The last two strategies support non-numerical data.\nThere are also more powerful imputers available in the sklearn.impute package (both for numerical features only):\n\nKNNImputer replaces each missing value with the mean of the k-nearest neighbors’ values for that feature. The distance is based on all the available features.\nIterativeImputer trains a regression model per feature to predict the missing values based on all the other available features. It then trains the model again on the updated data, and repeats the process several times, improving the models and the replacement values at each iteration.\n\nNow let’s drop some outliers:\n\nfrom sklearn.ensemble import IsolationForest\n\nisolation_forest = IsolationForest(random_state=42)\noutlier_pred = isolation_forest.fit_predict(X)\noutlier_pred\n\narray([-1,  1,  1, ...,  1,  1,  1], shape=(16512,))\n\n\nIf you wanted to drop outliers, you would run the following code:\n\n# housing = housing.iloc[outlier_pred == 1]\n# housing_labels = housing_labels.iloc[outlier_pred == 1]\n\nFor you to understand how Isolation Forest works under the hood, the following articles are recommended:\n\nIsolation Forest Anomaly Detection — Identify Outliers\nIsolation Forest Guide: Explanation and Python Implementation\n\n\n\n1.3.5.2 Handling Text and Categorical Attributes\nNow let’s preprocess the categorical input feature, ocean_proximity:\n\nhousing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)\n\n\n\n\n\n\n\n\nocean_proximity\n\n\n\n\n13096\nNEAR BAY\n\n\n14973\n&lt;1H OCEAN\n\n\n3785\nINLAND\n\n\n14689\nINLAND\n\n\n20507\nNEAR OCEAN\n\n\n1286\nINLAND\n\n\n18078\n&lt;1H OCEAN\n\n\n4396\nNEAR BAY\n\n\n18031\n&lt;1H OCEAN\n\n\n6753\n&lt;1H OCEAN\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n\n\nhousing_cat_encoded[:10]\n\narray([[3.],\n       [0.],\n       [1.],\n       [1.],\n       [4.],\n       [1.],\n       [0.],\n       [3.],\n       [0.],\n       [0.]])\n\n\n\nordinal_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n\n\nhousing_cat_1hot\n\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 16512 stored elements and shape (16512, 5)&gt;\n\n\nBy default, the OneHotEncoder class returns a sparse array, but we can convert it to a dense array if needed by calling the toarray() method:\n\nhousing_cat_1hot.toarray()\n\narray([[0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.]], shape=(16512, 5))\n\n\nAlternatively, you can set sparse_output=False when creating the OneHotEncoder\n\ncat_encoder = OneHotEncoder(sparse_output=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)  # now a dense array\nhousing_cat_1hot\n\narray([[0., 0., 0., 1., 0.],\n       [1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0.],\n       ...,\n       [0., 0., 0., 0., 1.],\n       [1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1.]], shape=(16512, 5))\n\n\n\ncat_encoder.categories_\n\n[array(['&lt;1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n       dtype=object)]\n\n\nPandas has a function called get_dummies(), which also converts each categorical feature into a one-hot representation, with one binary feature per category:\n\ndf_test = pd.DataFrame({\"ocean_proximity\": [\"INLAND\", \"NEAR BAY\"]})\npd.get_dummies(df_test)\n\n\n\n\n\n\n\n\nocean_proximity_INLAND\nocean_proximity_NEAR BAY\n\n\n\n\n0\nTrue\nFalse\n\n\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\ncat_encoder.transform(df_test)\n\narray([[0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0.]])\n\n\n\ndf_test_unknown = pd.DataFrame({\"ocean_proximity\": [\"&lt;2H OCEAN\", \"ISLAND\"]})\npd.get_dummies(df_test_unknown)\n\n\n\n\n\n\n\n\nocean_proximity_&lt;2H OCEAN\nocean_proximity_ISLAND\n\n\n\n\n0\nTrue\nFalse\n\n\n1\nFalse\nTrue\n\n\n\n\n\n\n\n\ncat_encoder.handle_unknown = \"ignore\"\ncat_encoder.transform(df_test_unknown)\n\narray([[0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0.]])\n\n\n\ncat_encoder.feature_names_in_\n\narray(['ocean_proximity'], dtype=object)\n\n\n\ncat_encoder.get_feature_names_out()\n\narray(['ocean_proximity_&lt;1H OCEAN', 'ocean_proximity_INLAND',\n       'ocean_proximity_ISLAND', 'ocean_proximity_NEAR BAY',\n       'ocean_proximity_NEAR OCEAN'], dtype=object)\n\n\n\ndf_output = pd.DataFrame(cat_encoder.transform(df_test_unknown),\n                         columns=cat_encoder.get_feature_names_out(),\n                         index=df_test_unknown.index)\ndf_output\n\n\n\n\n\n\n\n\nocean_proximity_&lt;1H OCEAN\nocean_proximity_INLAND\nocean_proximity_ISLAND\nocean_proximity_NEAR BAY\nocean_proximity_NEAR OCEAN\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\n1.3.5.3 Feature Scaling\nOne of the most important transformations you need to apply to your data is feature scaling. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.\nThere are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\nWarning: As with all estimators, it is important to fit the scalers to the training data only: never use fit() or fit_transform() for anything else than the training set. Once you have a trained scaler, you can then use it to transform() any other set, including the validation set, the test set, and new data.\nMin-max scaling (many people call this normalization) is the simplest: for each attribute, the values are shifted and rescaled so that they end up ranging from 0 to 1. This is performed by subtracting the min value from all values, and dividing the results by the difference between the min and the max. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if, for some reason, you don’t want 0–1 (e.g., neural networks work best with zero-mean inputs, so a range of –1 to 1 is preferable)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler(feature_range=(-1, 1))\nhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)\n\nStandardization is different: first it subtracts the mean value (so standardized values have a zero mean), then it divides the result by the standard deviation (so standardized values have a standard deviation equal to 1). Unlike min-max scaling, standardization does not restrict values to a specific range. However, standardization is much less affected by outliers. For example, suppose a district has a median income equal to 100 (by mistake), instead of the usual 0–15. Min-max scaling to the 0–1 range would map this outlier down to 1 and it would crush all the other values down to 0–0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called StandardScaler for standardization:\n\nfrom sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\nhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\n\nWhen a feature’s distribution has a heavy tail (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range. So before you scale the feature, you should first transform it to shrink the heavy tail, and if possible to make the distribution roughly symmetrical. For example, a common way to do this for positive features with a heavy tail to the right is to replace the feature with its square root (or raise the feature to a power between 0 and 1). If the feature has a really long and heavy tail, such as a power law distribution, then replacing the feature with its logarithm may help. For example, the population feature roughly follows a power law: districts with 10,000 inhabitants are only 10 times less frequent than districts with 1,000 inhabitants, not exponentially less frequent. Figure 2-17 shows how much better this feature looks when you compute its log: it’s very close to a Gaussian distribution (i.e., bell-shaped).\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\nhousing[\"population\"].hist(ax=axs[0], bins=50)\nhousing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\naxs[0].set_xlabel(\"Population\")\naxs[1].set_xlabel(\"Log of population\")\naxs[0].set_ylabel(\"Number of districts\")\n\nplt.show()\n\n\n\n\n\n\n\n\nWhat if we replace each value with its percentile?\n\npercentiles = [np.percentile(housing[\"median_income\"], p) for p in range(1, 100)]\nflattened_median_income = pd.cut(housing[\"median_income\"],\n                                 bins=[-np.inf] + percentiles + [np.inf],\n                                 labels=range(1, 100 + 1))\nflattened_median_income.hist(bins=50)\nplt.xlabel(\"Median income percentile\")\nplt.ylabel(\"Number of districts\")\nplt.show()\n# Note: incomes below the 1st percentile are labeled 1, and incomes above the\n# 99th percentile are labeled 100. This is why the distribution below ranges\n# from 1 to 100 (not 0 to 100).\n\n\n\n\n\n\n\n\n\nhousing['median_income'].head()\n\n13096    2.0987\n14973    6.0876\n3785     2.4330\n14689    2.2618\n20507    3.5292\nName: median_income, dtype: float64\n\n\nAnother approach to transforming multimodal distributions is to add a feature for each of the modes (at least the main ones), representing the similarity between the housing median age and that particular mode. The similarity measure is typically computed using a radial basis function (RBF)—any function that depends only on the distance between the input value and a fixed point. The most commonly used RBF is the Gaussian RBF, whose output value decays exponentially as the input value moves away from the fixed point. For example, the Gaussian RBF similarity between the housing age x and 35 is given by the equation exp(–γ(x – 35)²). The hyperparameter γ (gamma) determines how quickly the similarity measure decays as x moves away from 35. Using Scikit-Learn’s rbf_kernel() function, you can create a new Gaussian RBF feature measuring the similarity between the housing median age and 35:\n\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nage_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)\n\n# extra code – plots the age similarity feature along with the housing median age histogram\nages = np.linspace(housing[\"housing_median_age\"].min(),\n                   housing[\"housing_median_age\"].max(),\n                   500).reshape(-1, 1)\ngamma1 = 0.1\ngamma2 = 0.03\nrbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\nrbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n\nfig, ax1 = plt.subplots()\n\nax1.set_xlabel(\"Housing median age\")\nax1.set_ylabel(\"Number of districts\")\nax1.hist(housing[\"housing_median_age\"], bins=50)\n\nax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\ncolor = \"blue\"\nax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\nax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\nax2.tick_params(axis='y', labelcolor=color)\nax2.set_ylabel(\"Age similarity\", color=color)\n\nplt.legend(loc=\"upper left\")\n\nplt.show()\n\n\n\n\n\n\n\n\nSo far we’ve only looked at the input features, but the target values may also need to be transformed. For example, if the target distribution has a heavy tail, you may choose to replace the target with its logarithm. But if you do, the regression model will now predict the log of the median house value, not the median house value itself. You will need to compute the exponential of the model’s prediction if you want the predicted median house value.\n\nhousing_labels\n\n13096    458300.0\n14973    483800.0\n3785     101700.0\n14689     96100.0\n20507    361800.0\n           ...   \n14207    500001.0\n13105     88800.0\n19301    148800.0\n19121    500001.0\n19888    233300.0\nName: median_house_value, Length: 16512, dtype: float64\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\ntarget_scaler = StandardScaler()\nscaled_labels = target_scaler.fit_transform(housing_labels.to_frame())\n\nmodel = LinearRegression()\nmodel.fit(housing[[\"median_income\"]], scaled_labels)\nsome_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n\nscaled_predictions = model.predict(some_new_data)\npredictions = target_scaler.inverse_transform(scaled_predictions)\npredictions\n\narray([[131997.15275877],\n       [299359.35844434],\n       [146023.37185694],\n       [138840.33653057],\n       [192016.61557639]])\n\n\n\nfrom sklearn.compose import TransformedTargetRegressor\n\nmodel = TransformedTargetRegressor(LinearRegression(),\n                                   transformer=StandardScaler())\nmodel.fit(housing[[\"median_income\"]], housing_labels)\npredictions = model.predict(some_new_data)\npredictions\n\narray([131997.15275877, 299359.35844434, 146023.37185694, 138840.33653057,\n       192016.61557639])\n\n\n\n\n1.3.5.4 Custom Transformers\n\nfrom sklearn.preprocessing import FunctionTransformer\n\nlog_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\nlog_pop = log_transformer.transform(housing[[\"population\"]])\nlog_pop\n\n\n\n\n\n\n\n\npopulation\n\n\n\n\n13096\n7.362645\n\n\n14973\n6.501290\n\n\n3785\n6.331502\n\n\n14689\n7.520235\n\n\n20507\n7.555905\n\n\n...\n...\n\n\n14207\n6.843750\n\n\n13105\n7.257708\n\n\n19301\n7.942362\n\n\n19121\n7.452982\n\n\n19888\n6.525030\n\n\n\n\n16512 rows × 1 columns\n\n\n\n\nrbf_transformer = FunctionTransformer(rbf_kernel,\n                                      kw_args=dict(Y=[[35.]], gamma=0.1))\nage_simil_35 = rbf_transformer.transform(housing[[\"housing_median_age\"]])\nage_simil_35\n\narray([[2.81118530e-13],\n       [8.20849986e-02],\n       [6.70320046e-01],\n       ...,\n       [9.55316054e-22],\n       [6.70320046e-01],\n       [3.03539138e-04]], shape=(16512, 1))\n\n\nFor example, here’s how to add a feature that will measure the geographic similarity between each district and San Francisco:\n\nsf_coords = 37.7749, -122.41\nsf_transformer = FunctionTransformer(rbf_kernel,\n                                     kw_args=dict(Y=[sf_coords], gamma=0.1))\nsf_simil = sf_transformer.transform(housing[[\"latitude\", \"longitude\"]])\nsf_simil\n\narray([[0.999927  ],\n       [0.05258419],\n       [0.94864161],\n       ...,\n       [0.00388525],\n       [0.05038518],\n       [0.99868067]], shape=(16512, 1))\n\n\nCustom transformers are also useful to combine features. For example, here’s a FunctionTransformer that computes the ratio between the input features 0 and 1:\n\nratio_transformer = FunctionTransformer(lambda X: X[:, [0]] / X[:, [1]])\nratio_transformer.transform(np.array([[1., 2.], [3., 4.]]))\n\narray([[0.5 ],\n       [0.75]])\n\n\nFunctionTransformer is very handy, but what if you would like your transformer to be trainable, learning some parameters in the fit() method and using them later in the transform() method? For this, you need to write a custom class. For example, here’s a custom transformer that acts much like the StandardScaler:\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_array, check_is_fitted\n\nclass StandardScalerClone(BaseEstimator, TransformerMixin):\n    def __init__(self, with_mean=True):  # no *args or **kwargs!\n        self.with_mean = with_mean\n\n    def fit(self, X, y=None):  # y is required even though we don't use it\n        X = check_array(X)  # checks that X is an array with finite float values\n        self.mean_ = X.mean(axis=0)\n        self.scale_ = X.std(axis=0)\n        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n        return self  # always return self!\n\n    def transform(self, X):\n        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n        X = check_array(X)\n        assert self.n_features_in_ == X.shape[1]\n        if self.with_mean:\n            X = X - self.mean_\n        return X / self.scale_\n\nA custom transformer can (and often does) use other estimators in its implementation. For example, the following code demonstrates a custom transformer that uses a KMeans clusterer in the fit() method to identify the main clusters in the training data, and then uses rbf_kernel() in the transform() method to measure how similar each sample is to each cluster center:\n\nfrom sklearn.cluster import KMeans\n\nclass ClusterSimilarity(BaseEstimator, TransformerMixin):\n    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n        self.n_clusters = n_clusters\n        self.gamma = gamma\n        self.random_state = random_state\n\n    def fit(self, X, y=None, sample_weight=None):\n        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n        self.kmeans_.fit(X, sample_weight=sample_weight)\n        return self  # always return self!\n\n    def transform(self, X):\n        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n\n    def get_feature_names_out(self, names=None):\n        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\nsimilarities = cluster_simil.fit_transform(housing[[\"latitude\", \"longitude\"]],\n                                           sample_weight=housing_labels)\n\nThis code creates a ClusterSimilarity transformer, setting the number of clusters to 10. Then it calls fit_transform() with the latitude and longitude of every district in the training set, weighting each district by its median house value. The transformer uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each district and all 10 cluster centers. The result is a matrix with one row per district, and one column per cluster. Let’s look at the first five rows, rounding to two decimal places:\n\nsimilarities[:5].round(2)\n\narray([[0.  , 0.98, 0.  , 0.  , 0.  , 0.  , 0.13, 0.55, 0.  , 0.56],\n       [0.64, 0.  , 0.11, 0.04, 0.  , 0.  , 0.  , 0.  , 0.99, 0.  ],\n       [0.  , 0.65, 0.  , 0.  , 0.01, 0.  , 0.49, 0.59, 0.  , 0.28],\n       [0.63, 0.  , 0.  , 0.52, 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  ],\n       [0.87, 0.  , 0.03, 0.14, 0.  , 0.  , 0.  , 0.  , 0.89, 0.  ]])\n\n\n\n# extra code\n\nhousing_renamed = housing.rename(columns={\n    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n    \"population\": \"Population\",\n    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\nhousing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n\nhousing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n                     c=\"Max cluster similarity\",\n                     cmap=\"jet\", colorbar=True,\n                     legend=True, sharex=False, figsize=(10, 7))\nplt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n         cluster_simil.kmeans_.cluster_centers_[:, 0],\n         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n         label=\"Cluster centers\")\nplt.legend(loc=\"upper right\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.3.5.5 Transformation Pipelines\nNow let’s build a pipeline to preprocess the numerical attributes:\n\nfrom sklearn.pipeline import Pipeline\n\nnum_pipeline = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"standardize\", StandardScaler()),\n])\nnum_pipeline\n\nPipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('standardize', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('impute', ...), ('standardize', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\n\nfrom sklearn.pipeline import make_pipeline\n\nnum_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\nnum_pipeline\n\nPipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('simpleimputer', ...), ('standardscaler', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\nLet’s call the pipeline’s fit_transform() method and look at the output’s first five rows, rounded to two decimal places:\n\nhousing_num_prepared = num_pipeline.fit_transform(housing_num)\nhousing_num_prepared[:5].round(2)\n\narray([[-1.42,  1.01,  1.86,  0.31,  1.37,  0.14,  1.39, -0.94],\n       [ 0.6 , -0.7 ,  0.91, -0.31, -0.44, -0.69, -0.37,  1.17],\n       [-1.2 ,  1.28,  0.35, -0.71, -0.76, -0.79, -0.78, -0.76],\n       [ 1.23, -0.88, -0.92,  0.7 ,  0.74,  0.38,  0.73, -0.85],\n       [ 0.71, -0.88,  0.59,  0.79,  1.6 ,  0.44,  1.76, -0.18]])\n\n\n\ndf_housing_num_prepared = pd.DataFrame(\n    housing_num_prepared, columns=num_pipeline.get_feature_names_out(),\n    index=housing_num.index)\ndf_housing_num_prepared.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\n\n\n\n\n13096\n-1.423037\n1.013606\n1.861119\n0.311912\n1.368167\n0.137460\n1.394812\n-0.936491\n\n\n14973\n0.596394\n-0.702103\n0.907630\n-0.308620\n-0.435925\n-0.693771\n-0.373485\n1.171942\n\n\n3785\n-1.203098\n1.276119\n0.351428\n-0.712240\n-0.760709\n-0.788768\n-0.775727\n-0.759789\n\n\n14689\n1.231216\n-0.884924\n-0.919891\n0.702262\n0.742306\n0.383175\n0.731375\n-0.850281\n\n\n20507\n0.711362\n-0.875549\n0.589800\n0.790125\n1.595753\n0.444376\n1.755263\n-0.180365\n\n\n\n\n\n\n\n\nnum_pipeline.steps\n\n[('simpleimputer', SimpleImputer(strategy='median')),\n ('standardscaler', StandardScaler())]\n\n\n\nnum_pipeline[1]\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScaler?Documentation for StandardScaleriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\n\nnum_pipeline[:-1]\n\nPipeline(steps=[('simpleimputer', SimpleImputer(strategy='median'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('simpleimputer', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nnum_pipeline.named_steps[\"simpleimputer\"]\n\nSimpleImputer(strategy='median')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SimpleImputer?Documentation for SimpleImputeriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nnum_pipeline.set_params(simpleimputer__strategy=\"median\")\n\nPipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n                ('standardscaler', StandardScaler())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('simpleimputer', ...), ('standardscaler', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    \n\n\nSo far, we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer capable of handling all columns, applying the appropriate transformations to each column. For this, you can use a ColumnTransformer. For example, the following ColumnTransformer will apply num_pipeline (the one we just defined) to the numerical attributes, and cat_pipeline to the categorical attribute:\n\nfrom sklearn.compose import ColumnTransformer\n\nnum_attribs = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n               \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\ncat_attribs = [\"ocean_proximity\"]\n\ncat_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"most_frequent\"),\n    OneHotEncoder(handle_unknown=\"ignore\"))\n\npreprocessing = ColumnTransformer([\n    (\"num\", num_pipeline, num_attribs),\n    (\"cat\", cat_pipeline, cat_attribs),\n])\n\nSince listing all the column names is not very convenient, Scikit-Learn provides a make_column_selector class that you can use to automatically select all the features of a given type, such as numerical or categorical. You can pass a selector to the ColumnTransformer instead of column names or indices. Moreover, if you don’t care about naming the transformers, you can use make_column_transformer(), which chooses the names for you, just like make_pipeline() does. For example, the following code creates the same ColumnTransformer as earlier, except the transformers are automatically named \"pipeline-1\" and \"pipeline-2\" instead of \"num\" and \"cat\":\n\nfrom sklearn.compose import make_column_selector, make_column_transformer\n\npreprocessing = make_column_transformer(\n    (num_pipeline, make_column_selector(dtype_include=np.number)),\n    (cat_pipeline, make_column_selector(dtype_include=object)),\n)\n\n\nhousing_prepared = preprocessing.fit_transform(housing)\n\n\nhousing_prepared_fr = pd.DataFrame(\n    housing_prepared,\n    columns=preprocessing.get_feature_names_out(),\n    index=housing.index)\nhousing_prepared_fr.head()\n\n\n\n\n\n\n\n\npipeline-1__longitude\npipeline-1__latitude\npipeline-1__housing_median_age\npipeline-1__total_rooms\npipeline-1__total_bedrooms\npipeline-1__population\npipeline-1__households\npipeline-1__median_income\npipeline-2__ocean_proximity_&lt;1H OCEAN\npipeline-2__ocean_proximity_INLAND\npipeline-2__ocean_proximity_ISLAND\npipeline-2__ocean_proximity_NEAR BAY\npipeline-2__ocean_proximity_NEAR OCEAN\n\n\n\n\n13096\n-1.423037\n1.013606\n1.861119\n0.311912\n1.368167\n0.137460\n1.394812\n-0.936491\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n14973\n0.596394\n-0.702103\n0.907630\n-0.308620\n-0.435925\n-0.693771\n-0.373485\n1.171942\n1.0\n0.0\n0.0\n0.0\n0.0\n\n\n3785\n-1.203098\n1.276119\n0.351428\n-0.712240\n-0.760709\n-0.788768\n-0.775727\n-0.759789\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n14689\n1.231216\n-0.884924\n-0.919891\n0.702262\n0.742306\n0.383175\n0.731375\n-0.850281\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n20507\n0.711362\n-0.875549\n0.589800\n0.790125\n1.595753\n0.444376\n1.755263\n-0.180365\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nYou now want to create a single pipeline that will perform all the transformations we’ve experimented with up to now. Let’s recap what the pipeline will do and why:\n\nMissing values in numerical features will be imputed by replacing them with the median, as most ML algorithms don’t expect missing values. In categorical features, missing values will be replaced by the most frequent category.\nThe categorical feature will be one-hot encoded, as most ML algorithms only accept numerical inputs.\nA few ratio features will be computed and added: bedrooms_ratio, rooms_per_house, and people_per_house. Hopefully these will better correlate with the median house value, and thereby help the ML models.\nA few cluster similarity features will also be added. These will likely be more useful to the model than latitude and longitude.\nFeatures with a long tail will be replaced by their logarithm, as most models prefer features with roughly uniform or Gaussian distributions.\nAll numerical features will be standardized, as most ML algorithms prefer when all features have roughly the same scale.\n\nThe code that builds the pipeline to do all of this should look familiar to you by now:\n\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\ndef ratio_name(function_transformer, feature_names_in):\n    return [\"ratio\"]  # feature names out\n\ndef ratio_pipeline():\n    return make_pipeline(\n        SimpleImputer(strategy=\"median\"),\n        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n        StandardScaler())\n\nlog_pipeline = make_pipeline(\n    SimpleImputer(strategy=\"median\"),\n    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n    StandardScaler())\n\ncluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n\ndefault_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n                                     StandardScaler())\n\npreprocessing = ColumnTransformer([\n        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n        (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n    ],\n    remainder=default_num_pipeline)  # one column remaining: housing_median_age\n\n\nhousing_prepared = preprocessing.fit_transform(housing)\nhousing_prepared.shape\n\n(16512, 24)\n\n\n\npreprocessing.get_feature_names_out()\n\narray(['bedrooms__ratio', 'rooms_per_house__ratio',\n       'people_per_house__ratio', 'log__total_bedrooms',\n       'log__total_rooms', 'log__population', 'log__households',\n       'log__median_income', 'geo__Cluster 0 similarity',\n       'geo__Cluster 1 similarity', 'geo__Cluster 2 similarity',\n       'geo__Cluster 3 similarity', 'geo__Cluster 4 similarity',\n       'geo__Cluster 5 similarity', 'geo__Cluster 6 similarity',\n       'geo__Cluster 7 similarity', 'geo__Cluster 8 similarity',\n       'geo__Cluster 9 similarity', 'cat__ocean_proximity_&lt;1H OCEAN',\n       'cat__ocean_proximity_INLAND', 'cat__ocean_proximity_ISLAND',\n       'cat__ocean_proximity_NEAR BAY', 'cat__ocean_proximity_NEAR OCEAN',\n       'remainder__housing_median_age'], dtype=object)\n\n\n\nhousing_prepared_fr = pd.DataFrame(\n    housing_prepared,\n    columns=preprocessing.get_feature_names_out(),\n    index=housing.index)\nhousing_prepared_fr.head()\n\n\n\n\n\n\n\n\nbedrooms__ratio\nrooms_per_house__ratio\npeople_per_house__ratio\nlog__total_bedrooms\nlog__total_rooms\nlog__population\nlog__households\nlog__median_income\ngeo__Cluster 0 similarity\ngeo__Cluster 1 similarity\n...\ngeo__Cluster 6 similarity\ngeo__Cluster 7 similarity\ngeo__Cluster 8 similarity\ngeo__Cluster 9 similarity\ncat__ocean_proximity_&lt;1H OCEAN\ncat__ocean_proximity_INLAND\ncat__ocean_proximity_ISLAND\ncat__ocean_proximity_NEAR BAY\ncat__ocean_proximity_NEAR OCEAN\nremainder__housing_median_age\n\n\n\n\n13096\n1.846624\n-0.866027\n-0.330204\n1.324114\n0.637892\n0.456906\n1.310369\n-1.071522\n4.581829e-01\n1.241847e-14\n...\n8.489216e-04\n9.770322e-01\n2.382191e-08\n3.819126e-18\n0.0\n0.0\n0.0\n1.0\n0.0\n1.861119\n\n\n14973\n-0.508121\n0.024550\n-0.253616\n-0.252671\n-0.063576\n-0.711654\n-0.142030\n1.194712\n6.511495e-10\n9.579596e-01\n...\n5.614049e-27\n1.260964e-13\n1.103491e-01\n3.547610e-01\n1.0\n0.0\n0.0\n0.0\n0.0\n0.907630\n\n\n3785\n-0.202155\n-0.041193\n-0.051041\n-0.925266\n-0.859927\n-0.941997\n-0.913030\n-0.756981\n3.432506e-01\n4.261141e-15\n...\n5.641131e-03\n7.303265e-01\n2.508224e-08\n2.669659e-18\n0.0\n1.0\n0.0\n0.0\n0.0\n0.351428\n\n\n14689\n-0.149006\n-0.034858\n-0.141475\n0.952773\n0.943475\n0.670700\n0.925373\n-0.912253\n2.244844e-15\n2.704823e-01\n...\n5.913326e-35\n5.201263e-20\n1.712982e-03\n8.874598e-01\n0.0\n1.0\n0.0\n0.0\n0.0\n-0.919891\n\n\n20507\n0.963208\n-0.666554\n-0.306148\n1.437622\n1.003590\n0.719093\n1.481464\n0.034537\n1.090228e-11\n9.422206e-01\n...\n5.421817e-30\n1.048030e-15\n2.568824e-02\n5.279506e-01\n0.0\n0.0\n0.0\n0.0\n1.0\n0.589800\n\n\n\n\n5 rows × 24 columns\n\n\n\n\n\n\n1.3.6 Select and Train a Model\n\n1.3.6.1 Train and Evaluate on the Training Set\nYou decide to train a very basic linear regression model to get started:\n\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg = make_pipeline(preprocessing, LinearRegression())\nlin_reg.fit(housing, housing_labels)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x000...\n                                                   'median_income']),\n                                                 ('geo',\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC265E50&gt;)])),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('columntransformer', ...), ('linearregression', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('bedrooms', ...), ('rooms_per_house', ...), ...]\n\n\n\nremainder \nPipeline(step...ardScaler())])\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    bedrooms['total_bedrooms', 'total_rooms']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    rooms_per_house['total_rooms', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    people_per_house['population', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    logFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;ufunc 'log'&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n'one-to-one'\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    geo['latitude', 'longitude']ClusterSimilarity\n        \n            \n                Parameters\n                \n\n\n\n\nn_clusters \n10\n\n\n\ngamma \n1.0\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC265E50&gt;SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'most_frequent'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    remainder['housing_median_age']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nhousing_predictions = lin_reg.predict(housing)\nhousing_predictions[:5].round(-2)  # -2 = rounded to the nearest hundred\n\narray([246000., 372700., 135700.,  91400., 330900.])\n\n\n\nhousing_labels.iloc[:5].values\n\narray([458300., 483800., 101700.,  96100., 361800.])\n\n\nWell, it works, but not always: the first prediction is way off (by over $ 200,000!), while the other predictions are better: two are off by about 25%, and two are off by less than 10%. Remember that you chose to use the RMSE as your performance measure, so you want to measure this regression model’s RMSE on the whole training set using Scikit-Learn’s root_mean_squared_error() function:\n\nfrom sklearn.metrics import root_mean_squared_error\nlin_rmse = root_mean_squared_error(housing_labels, housing_predictions)\nlin_rmse\n\n68972.88910758478\n\n\nThis is better than nothing, but clearly not a great score: the median_housing_values of most districts range between $120,000 and $265,000, so a typical prediction error of $68,973 is really not very satisfying. This is an example of a model underfitting the training data. When this happens it can mean that the features do not provide enough information to make good predictions, or that the model is not powerful enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, which rules out the last option. You could try to add more features, but first you want to try a more complex model to see how it does.\nYou decide to try a DecisionTreeRegressor, as this is a fairly powerful model capable of finding complex nonlinear relationships in the data\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\ntree_reg.fit(housing, housing_labels)\n\nPipeline(steps=[('columntransformer',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x000...\n                                                  ClusterSimilarity(random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC265E50&gt;)])),\n                ('decisiontreeregressor',\n                 DecisionTreeRegressor(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('columntransformer', ...), ('decisiontreeregressor', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    columntransformer: ColumnTransformer?Documentation for columntransformer: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('bedrooms', ...), ('rooms_per_house', ...), ...]\n\n\n\nremainder \nPipeline(step...ardScaler())])\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    bedrooms['total_bedrooms', 'total_rooms']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    rooms_per_house['total_rooms', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    people_per_house['population', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    logFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;ufunc 'log'&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n'one-to-one'\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    geo['latitude', 'longitude']ClusterSimilarity\n        \n            \n                Parameters\n                \n\n\n\n\nn_clusters \n10\n\n\n\ngamma \n1.0\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC265E50&gt;SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'most_frequent'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    remainder['housing_median_age']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    DecisionTreeRegressor?Documentation for DecisionTreeRegressor\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'squared_error'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \n42\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\nhousing_predictions = tree_reg.predict(housing)\ntree_rmse = root_mean_squared_error(housing_labels, housing_predictions)\ntree_rmse\n\n0.0\n\n\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As you saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training and part of it for model validation.\n\n\n1.3.6.2 Better Evaluation Using Cross-Validation\nOne way to evaluate the decision tree model would be to use the train_​test_split() function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. It’s a bit of effort, but nothing too difficult, and it would work fairly well.\nA great alternative is to use Scikit-Learn’s k-fold cross-validation feature. You split the training set into k nonoverlapping subsets called folds, then you train and evaluate your model k times, picking a different fold for evaluation every time (i.e., the validation fold) and using the other k – 1 folds for training. This process produces k evaluation scores.\n\n\n\nk-fold cross-validation, with k = 10\n\n\nScikit-Learn provides a convenient cross_val_score() function that does just that, and it returns an array containing the k evaluation scores. For example, let’s use it to evaluate our tree regressor, using \\(k\\) = 10:\n\nfrom sklearn.model_selection import cross_val_score\n\ntree_rmses = -cross_val_score(tree_reg, housing, housing_labels,\n                              scoring=\"neg_root_mean_squared_error\", cv=10)\npd.Series(tree_rmses).describe()\n\ncount       10.000000\nmean     66573.734600\nstd       1103.402323\nmin      64607.896046\n25%      66204.731788\n50%      66388.272499\n75%      66826.257468\nmax      68532.210664\ndtype: float64\n\n\nNow the decision tree doesn’t look as good as it did earlier. In fact, it seems to perform almost as poorly as the linear regression model! Notice that cross-validation allows you to get not only an estimate of the performance of your model, but also a measure of how precise this estimate is (i.e., its standard deviation). The decision tree has an RMSE of about 66,573, with a standard deviation of about 1,103. You would not have this information if you just used one validation set. But cross-validation comes at the cost of training the model several times, so it is not always feasible.\nIf you compute the same metric for the linear regression model, you will find that the mean RMSE is 70,003 and the standard deviation is 4,182. So the decision tree model seems to perform very slightly better than the linear model, but the difference is minimal due to severe overfitting. We know there’s an overfitting problem because the training error is low (actually zero) while the validation error is high.\nLet’s try one last model now: the RandomForestRegressor. Random forests work by training many decision trees on random subsets of the features, then averaging out their predictions. Such models composed of many other models are called ensembles: if the underlying models are very diverse, then their errors will not be very correlated, and therefore averaging out the predictions will smooth out the errors, reduce overfitting, and improve the overall performance. The code is much the same as earlier:\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nforest_reg = make_pipeline(preprocessing,\n                           RandomForestRegressor(random_state=42))\nforest_rmses = -cross_val_score(forest_reg, housing, housing_labels,\n                                scoring=\"neg_root_mean_squared_error\", cv=10)\npd.Series(forest_rmses).describe()\n\ncount       10.000000\nmean     47038.092799\nstd       1021.491757\nmin      45495.976649\n25%      46510.418013\n50%      47118.719249\n75%      47480.519175\nmax      49140.832210\ndtype: float64\n\n\nWow, this is much better: random forests really look very promising for this task! However, if you train a RandomForestRegressor and measure the RMSE on the training set, you will find roughly 17,551: that’s much lower, meaning that there’s still quite a lot of overfitting going on. Possible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.\n\n\n\n1.3.7 Fine-Tune Your Model\n\n1.3.7.1 Grid Search\nOne option would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.\nInstead, you can use Scikit-Learn’s GridSearchCV class to search for you. All you need to do is tell it which hyperparameters you want it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor:\n\nfrom sklearn.model_selection import GridSearchCV\n\nfull_pipeline = Pipeline([\n    (\"preprocessing\", preprocessing),\n    (\"random_forest\", RandomForestRegressor(random_state=42)),\n])\nparam_grid = [\n    {'preprocessing__geo__n_clusters': [5, 8, 10],\n     'random_forest__max_features': [4, 6, 8]},\n    {'preprocessing__geo__n_clusters': [10, 15],\n     'random_forest__max_features': [6, 8, 10]},\n]\ngrid_search = GridSearchCV(full_pipeline, param_grid, cv=3,\n                           scoring='neg_root_mean_squared_error')\ngrid_search.fit(housing, housing_labels)\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('preprocessing',\n                                        ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                     SimpleImputer(strategy='median')),\n                                                                                    ('standardscaler',\n                                                                                     StandardScaler())]),\n                                                          transformers=[('bedrooms',\n                                                                         Pipeline(steps=[('simpleimputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('functiontransformer',\n                                                                                          FunctionTransformer(feature_names_out=&lt;f...\n                                                                         &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC265E50&gt;)])),\n                                       ('random_forest',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid=[{'preprocessing__geo__n_clusters': [5, 8, 10],\n                          'random_forest__max_features': [4, 6, 8]},\n                         {'preprocessing__geo__n_clusters': [10, 15],\n                          'random_forest__max_features': [6, 8, 10]}],\n             scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nPipeline(step...m_state=42))])\n\n\n\nparam_grid \n[{'preprocessing__geo__n_clusters': [5, 8, ...], 'random_forest__max_features': [4, 6, ...]}, {'preprocessing__geo__n_clusters': [10, 15], 'random_forest__max_features': [6, 8, ...]}]\n\n\n\nscoring \n'neg_root_mean_squared_error'\n\n\n\nn_jobs \nNone\n\n\n\nrefit \nTrue\n\n\n\ncv \n3\n\n\n\nverbose \n0\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nerror_score \nnan\n\n\n\nreturn_train_score \nFalse\n\n\n\n\n            \n        \n    best_estimator_: Pipelinepreprocessing: ColumnTransformer?Documentation for preprocessing: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('bedrooms', ...), ('rooms_per_house', ...), ...]\n\n\n\nremainder \nPipeline(step...ardScaler())])\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    bedrooms['total_bedrooms', 'total_rooms']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    rooms_per_house['total_rooms', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    people_per_house['population', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    logFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;ufunc 'log'&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n'one-to-one'\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    geo['latitude', 'longitude']ClusterSimilarity\n        \n            \n                Parameters\n                \n\n\n\n\nn_clusters \n15\n\n\n\ngamma \n1.0\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC2E8BF0&gt;SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'most_frequent'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    remainder['housing_median_age']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    RandomForestRegressor?Documentation for RandomForestRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n6\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\nNotice that you can refer to any hyperparameter of any estimator in a pipeline, even if this estimator is nested deep inside several pipelines and column transformers. For example, when Scikit-Learn sees “preprocessing__geo__n_clusters”, it splits this string at the double underscores, then it looks for an estimator named “preprocessing” in the pipeline and finds the preprocessing ColumnTransformer. Next, it looks for a transformer named “geo” inside this ColumnTransformer and finds the ClusterSimilarity transformer we used on the latitude and longitude attributes. Then it finds this transformer’s n_clusters hyperparameter. Similarly, random_forest__max_features refers to the max_features hyperparameter of the estimator named “random_forest”, which is of course the RandomForestRegressor model.\nThere are two dictionaries in this param_grid, so GridSearchCV will first evaluate all 3 × 3 = 9 combinations of n_clusters and max_features hyperparameter values specified in the first dict, then it will try all 2 × 3 = 6 combinations of hyperparameter values in the second dict. So in total the grid search will explore 9 + 6 = 15 combinations of hyperparameter values, and it will train the pipeline 3 times per combination, since we are using 3-fold cross validation. This means there will be a grand total of 15 × 3 = 45 rounds of training! It may take a while, but when it is done you can get the best combination of parameters like this:\n\ngrid_search.best_params_\n\n{'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}\n\n\n\ngrid_search.best_estimator_\n\nPipeline(steps=[('preprocessing',\n                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                              SimpleImputer(strategy='median')),\n                                                             ('standardscaler',\n                                                              StandardScaler())]),\n                                   transformers=[('bedrooms',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('functiontransformer',\n                                                                   FunctionTransformer(feature_names_out=&lt;function ratio_name at 0x0000025...\n                                                  ClusterSimilarity(n_clusters=15,\n                                                                    random_state=42),\n                                                  ['latitude', 'longitude']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('simpleimputer',\n                                                                   SimpleImputer(strategy='most_frequent')),\n                                                                  ('onehotencoder',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  &lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC2E8BF0&gt;)])),\n                ('random_forest',\n                 RandomForestRegressor(max_features=6, random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('preprocessing', ...), ('random_forest', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    preprocessing: ColumnTransformer?Documentation for preprocessing: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('bedrooms', ...), ('rooms_per_house', ...), ...]\n\n\n\nremainder \nPipeline(step...ardScaler())])\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    bedrooms['total_bedrooms', 'total_rooms']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    rooms_per_house['total_rooms', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    people_per_house['population', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    logFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;ufunc 'log'&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n'one-to-one'\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    geo['latitude', 'longitude']ClusterSimilarity\n        \n            \n                Parameters\n                \n\n\n\n\nn_clusters \n15\n\n\n\ngamma \n1.0\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC2E8BF0&gt;SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'most_frequent'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    remainder['housing_median_age']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    RandomForestRegressor?Documentation for RandomForestRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n6\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ncv_res = pd.DataFrame(grid_search.cv_results_)\ncv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n\n# extra code – these few lines of code just make the DataFrame look nicer\ncv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n                 \"param_random_forest__max_features\", \"split0_test_score\",\n                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\nscore_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\ncv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\ncv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n\ncv_res.head()\n\n\n\n\n\n\n\n\nn_clusters\nmax_features\nsplit0\nsplit1\nsplit2\nmean_test_rmse\n\n\n\n\n12\n15\n6\n42725\n43708\n44335\n43590\n\n\n13\n15\n8\n43486\n43820\n44900\n44069\n\n\n6\n10\n4\n43798\n44036\n44961\n44265\n\n\n9\n10\n6\n43710\n44163\n44967\n44280\n\n\n7\n10\n6\n43710\n44163\n44967\n44280\n\n\n\n\n\n\n\nThe mean test RMSE score for the best model is 43,590, which is better than the score you got earlier using the default hyperparameter values (which was 47,038). Congratulations, you have successfully fine-tuned your best model!\n\n\n1.3.7.2 Randomized Search\nThe grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but RandomizedSearchCV is often preferable, especially when the hyperparameter search space is large. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combinations it evaluates a fixed number of combinations, selecting a random value for each hyperparameter at every iteration.\nFor each hyperparameter, you must provide either a list of possible values, or a probability distribution:\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),\n                  'random_forest__max_features': randint(low=2, high=20)}\n\nrnd_search = RandomizedSearchCV(\n    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,\n    scoring='neg_root_mean_squared_error', random_state=42)\n\nrnd_search.fit(housing, housing_labels)\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('preprocessing',\n                                              ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n                                                                                           SimpleImputer(strategy='median')),\n                                                                                          ('standardscaler',\n                                                                                           StandardScaler())]),\n                                                                transformers=[('bedrooms',\n                                                                               Pipeline(steps=[('simpleimputer',\n                                                                                                SimpleImputer(strategy='median')),\n                                                                                               ('functiontransformer',\n                                                                                                FunctionTransformer(feature_names_...\n                                             ('random_forest',\n                                              RandomForestRegressor(random_state=42))]),\n                   param_distributions={'preprocessing__geo__n_clusters': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025B96C7D550&gt;,\n                                        'random_forest__max_features': &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x0000025BFC266AD0&gt;},\n                   random_state=42, scoring='neg_root_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nPipeline(step...m_state=42))])\n\n\n\nparam_distributions \n{'preprocessing__geo__n_clusters': &lt;scipy.stats....0025B96C7D550&gt;, 'random_forest__max_features': &lt;scipy.stats....0025BFC266AD0&gt;}\n\n\n\nn_iter \n10\n\n\n\nscoring \n'neg_root_mean_squared_error'\n\n\n\nn_jobs \nNone\n\n\n\nrefit \nTrue\n\n\n\ncv \n3\n\n\n\nverbose \n0\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nrandom_state \n42\n\n\n\nerror_score \nnan\n\n\n\nreturn_train_score \nFalse\n\n\n\n\n            \n        \n    best_estimator_: Pipelinepreprocessing: ColumnTransformer?Documentation for preprocessing: ColumnTransformer\n        \n            \n                Parameters\n                \n\n\n\n\ntransformers \n[('bedrooms', ...), ('rooms_per_house', ...), ...]\n\n\n\nremainder \nPipeline(step...ardScaler())])\n\n\n\nsparse_threshold \n0.3\n\n\n\nn_jobs \nNone\n\n\n\ntransformer_weights \nNone\n\n\n\nverbose \nFalse\n\n\n\nverbose_feature_names_out \nTrue\n\n\n\nforce_int_remainder_cols \n'deprecated'\n\n\n\n\n            \n        \n    bedrooms['total_bedrooms', 'total_rooms']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    rooms_per_house['total_rooms', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    people_per_house['population', 'households']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    column_ratioFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;function col...0025BF9C1CA40&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n&lt;function rat...0025BF9C1CB80&gt;\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    log['total_bedrooms', 'total_rooms', 'population', 'households', 'median_income']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    logFunctionTransformer?Documentation for FunctionTransformer\n        \n            \n                Parameters\n                \n\n\n\n\nfunc \n&lt;ufunc 'log'&gt;\n\n\n\ninverse_func \nNone\n\n\n\nvalidate \nFalse\n\n\n\naccept_sparse \nFalse\n\n\n\ncheck_inverse \nTrue\n\n\n\nfeature_names_out \n'one-to-one'\n\n\n\nkw_args \nNone\n\n\n\ninv_kw_args \nNone\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    geo['latitude', 'longitude']ClusterSimilarity\n        \n            \n                Parameters\n                \n\n\n\n\nn_clusters \n45\n\n\n\ngamma \n1.0\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    cat&lt;sklearn.compose._column_transformer.make_column_selector object at 0x0000025BFC2EBD70&gt;SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'most_frequent'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    OneHotEncoder?Documentation for OneHotEncoder\n        \n            \n                Parameters\n                \n\n\n\n\ncategories \n'auto'\n\n\n\ndrop \nNone\n\n\n\nsparse_output \nTrue\n\n\n\ndtype \n&lt;class 'numpy.float64'&gt;\n\n\n\nhandle_unknown \n'ignore'\n\n\n\nmin_frequency \nNone\n\n\n\nmax_categories \nNone\n\n\n\nfeature_name_combiner \n'concat'\n\n\n\n\n            \n        \n    remainder['housing_median_age']SimpleImputer?Documentation for SimpleImputer\n        \n            \n                Parameters\n                \n\n\n\n\nmissing_values \nnan\n\n\n\nstrategy \n'median'\n\n\n\nfill_value \nNone\n\n\n\ncopy \nTrue\n\n\n\nadd_indicator \nFalse\n\n\n\nkeep_empty_features \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    RandomForestRegressor?Documentation for RandomForestRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n100\n\n\n\ncriterion \n'squared_error'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n9\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\ncv_res = pd.DataFrame(rnd_search.cv_results_)\ncv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\ncv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n                 \"param_random_forest__max_features\", \"split0_test_score\",\n                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\ncv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\ncv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\ncv_res.head()\n\n\n\n\n\n\n\n\nn_clusters\nmax_features\nsplit0\nsplit1\nsplit2\nmean_test_rmse\n\n\n\n\n1\n45\n9\n41342\n42242\n43057\n42214\n\n\n8\n32\n7\n41825\n42275\n43241\n42447\n\n\n0\n41\n16\n42238\n42938\n43354\n42843\n\n\n5\n42\n4\n41869\n43362\n43664\n42965\n\n\n2\n23\n8\n42490\n42928\n43718\n43046\n\n\n\n\n\n\n\n\n\n1.3.7.3 Analysing the Best Models and Their Errors\nYou will often gain good insights on the problem by inspecting the best models. For example, the RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions:\n\nfinal_model = rnd_search.best_estimator_  # includes preprocessing\nfeature_importances = final_model[\"random_forest\"].feature_importances_\nfeature_importances.round(2)\n\narray([0.07, 0.05, 0.05, 0.01, 0.01, 0.01, 0.01, 0.19, 0.01, 0.02, 0.01,\n       0.01, 0.01, 0.  , 0.01, 0.02, 0.01, 0.02, 0.01, 0.  , 0.01, 0.02,\n       0.01, 0.01, 0.01, 0.  , 0.02, 0.01, 0.01, 0.  , 0.01, 0.01, 0.01,\n       0.03, 0.01, 0.01, 0.01, 0.01, 0.04, 0.01, 0.02, 0.01, 0.02, 0.01,\n       0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.07,\n       0.  , 0.  , 0.  , 0.01])\n\n\nLet’s sort these importance scores in descending order and display them next to their corresponding attribute names:\n\nsorted(zip(feature_importances,\n           final_model[\"preprocessing\"].get_feature_names_out()),\n       reverse=True)\n\n[(np.float64(0.18599734460509473), 'log__median_income'),\n (np.float64(0.07338850855844488), 'cat__ocean_proximity_INLAND'),\n (np.float64(0.06556941990883974), 'bedrooms__ratio'),\n (np.float64(0.05364871007672531), 'rooms_per_house__ratio'),\n (np.float64(0.04598870861894748), 'people_per_house__ratio'),\n (np.float64(0.04175269214442518), 'geo__Cluster 30 similarity'),\n (np.float64(0.025976797232869678), 'geo__Cluster 25 similarity'),\n (np.float64(0.023595895886342252), 'geo__Cluster 36 similarity'),\n (np.float64(0.02021056221732893), 'geo__Cluster 9 similarity'),\n (np.float64(0.018606917076661445), 'geo__Cluster 34 similarity'),\n (np.float64(0.01813798837462886), 'geo__Cluster 37 similarity'),\n (np.float64(0.017404353166326745), 'geo__Cluster 18 similarity'),\n (np.float64(0.01677838614384489), 'geo__Cluster 1 similarity'),\n (np.float64(0.015459009666188978), 'geo__Cluster 7 similarity'),\n (np.float64(0.015325731028175922), 'geo__Cluster 32 similarity'),\n (np.float64(0.015073772015038346), 'geo__Cluster 13 similarity'),\n (np.float64(0.014272160962173803), 'geo__Cluster 35 similarity'),\n (np.float64(0.014180636461860477), 'geo__Cluster 0 similarity'),\n (np.float64(0.01374636449823899), 'geo__Cluster 3 similarity'),\n (np.float64(0.013572305708469519), 'geo__Cluster 28 similarity'),\n (np.float64(0.012940349694228718), 'geo__Cluster 26 similarity'),\n (np.float64(0.012738123746761943), 'geo__Cluster 31 similarity'),\n (np.float64(0.011654237215152623), 'geo__Cluster 19 similarity'),\n (np.float64(0.011628003598059721), 'geo__Cluster 6 similarity'),\n (np.float64(0.011134113333125396), 'geo__Cluster 24 similarity'),\n (np.float64(0.011042979326385047), 'remainder__housing_median_age'),\n (np.float64(0.010907388443940416), 'geo__Cluster 43 similarity'),\n (np.float64(0.010847192663592164), 'geo__Cluster 44 similarity'),\n (np.float64(0.010592244492858262), 'geo__Cluster 10 similarity'),\n (np.float64(0.010512467290844922), 'geo__Cluster 23 similarity'),\n (np.float64(0.010458665615386447), 'geo__Cluster 41 similarity'),\n (np.float64(0.010261910692851671), 'geo__Cluster 40 similarity'),\n (np.float64(0.00975730698309749), 'geo__Cluster 2 similarity'),\n (np.float64(0.009659933222114479), 'geo__Cluster 12 similarity'),\n (np.float64(0.009574969190852867), 'geo__Cluster 14 similarity'),\n (np.float64(0.008199144719918424), 'geo__Cluster 20 similarity'),\n (np.float64(0.008141941480860804), 'geo__Cluster 33 similarity'),\n (np.float64(0.00759676121996469), 'geo__Cluster 8 similarity'),\n (np.float64(0.007576298012849029), 'geo__Cluster 22 similarity'),\n (np.float64(0.007346290789504318), 'geo__Cluster 39 similarity'),\n (np.float64(0.006898774333063982), 'geo__Cluster 4 similarity'),\n (np.float64(0.006794731845079839), 'log__total_rooms'),\n (np.float64(0.006514889773323567), 'log__population'),\n (np.float64(0.006350528211987124), 'geo__Cluster 27 similarity'),\n (np.float64(0.0063375587499023365), 'geo__Cluster 16 similarity'),\n (np.float64(0.006231053672395538), 'geo__Cluster 38 similarity'),\n (np.float64(0.006121348345871485), 'log__households'),\n (np.float64(0.00584984200158211), 'log__total_bedrooms'),\n (np.float64(0.005678310466685012), 'geo__Cluster 15 similarity'),\n (np.float64(0.005479729990673466), 'geo__Cluster 29 similarity'),\n (np.float64(0.005348325088535127), 'geo__Cluster 42 similarity'),\n (np.float64(0.004866251452445484), 'geo__Cluster 17 similarity'),\n (np.float64(0.004495340541933026), 'geo__Cluster 11 similarity'),\n (np.float64(0.004418821635620684), 'geo__Cluster 5 similarity'),\n (np.float64(0.003534473250529128), 'geo__Cluster 21 similarity'),\n (np.float64(0.0018324246573418507), 'cat__ocean_proximity_&lt;1H OCEAN'),\n (np.float64(0.0015282226447271793), 'cat__ocean_proximity_NEAR OCEAN'),\n (np.float64(0.00043259703422473606), 'cat__ocean_proximity_NEAR BAY'),\n (np.float64(3.0190221102670295e-05), 'cat__ocean_proximity_ISLAND')]\n\n\nWith this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others).\nYou should also look at the specific errors that your system makes, then try to understand why it makes them and what could fix the problem: adding extra features or getting rid of uninformative ones, cleaning up outliers, etc.\nNow is also a good time to check model fairness: it should not only work well on average, but also on various categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. This requires a detailed bias analysis: creating subsets of your validation set for each category, and analyzing your model’s performance on them. That’s a lot of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is resolved, or at least it should not be used to make predictions for that category, as it may do more harm than good.\n\n\n1.3.7.4 Evaluate Your System on the Test Set\nAfter tweaking your models for a while, you eventually have a system that performs sufficiently well. You are ready to evaluate the final model on the test set. There is nothing special about this process; just get the predictors and the labels from your test set and run your final_model to transform the data and make predictions, then evaluate these predictions:\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nfinal_predictions = final_model.predict(X_test)\n\nfinal_rmse = root_mean_squared_error(y_test, final_predictions)\nprint(final_rmse)\n\n41445.533268606625\n\n\nWe can compute a 95% confidence interval for the test RMSE using SciPy’s bootstrap() function:\n\nfrom scipy import stats\n\ndef rmse(squared_errors):\n    return np.sqrt(np.mean(squared_errors))\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nboot_result = stats.bootstrap([squared_errors], rmse,\n                              confidence_level=confidence, random_state=42)\nrmse_lower, rmse_upper = boot_result.confidence_interval\n\nprint(f\"95% CI for RMSE: ({rmse_lower:.4f}, {rmse_upper:.4f})\")\n\n95% CI for RMSE: (39520.9572, 43701.7681)\n\n\n\n\n\n1.3.8 Launch, Monitor, and Maintain the System\nIt’s often a good idea to save every model you experiment with so that you can come back easily to any model you want. You may also save the cross-validation scores and perhaps the actual predictions on the validation set. This will allow you to easily compare scores across model types, and compare the types of errors they make.\nThen you can deploy your model to your production environment. The most basic way to do this is just to save the best model you trained, transfer the file to your production environment, and load it. To save the model, you can use the joblib library like this:\n\nimport joblib\n\njoblib.dump(final_model, \"my_california_housing_model.pkl\")\n\n['my_california_housing_model.pkl']\n\n\nNow you can deploy this model to production. For example, the following code could be a script that would run in production:\n\nimport joblib\n\n# extra code – excluded for conciseness\nfrom sklearn.cluster import KMeans\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics.pairwise import rbf_kernel\n\ndef column_ratio(X):\n    return X[:, [0]] / X[:, [1]]\n\n#class ClusterSimilarity(BaseEstimator, TransformerMixin):\n#    [...]\n\nfinal_model_reloaded = joblib.load(\"my_california_housing_model.pkl\")\n\nnew_data = housing.iloc[:5]  # pretend these are new districts\npredictions = final_model_reloaded.predict(new_data)\npredictions\n\narray([441046.12, 454713.09, 104832.  , 101316.  , 336181.05])\n\n\nPerhaps the model will be used within a website: the user will type in some data about a new district and click the Estimate Price button. This will send a query containing the data to the web server, which will forward it to your web application, and finally your code will simply call the model’s predict() method (you want to load the model upon server startup, rather than every time the model is used). Alternatively, you can wrap the model within a dedicated web service that your web application can query through a REST API. This makes it easier to upgrade your model to new versions without interrupting the main application. It also simplifies scaling, since you can start as many web services as needed and load-balance the requests coming from your web application across these web services. Moreover, it allows your web application to use any programming language, not just Python.\n\n\n\nA model deployed as a web service and used by a web application\n\n\nIf the data keeps evolving, you will need to update your datasets and retrain your model regularly. You should probably automate the whole process as much as possible. Here are a few things you can automate:\n\nCollect fresh data regularly and label it (e.g., using human raters).\nWrite a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.\nWrite another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.\n\nYou should also make sure you evaluate the model’s input data quality. Sometimes performance will degrade slightly because of a poor-quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your model’s inputs, you may catch this earlier. For example, you could trigger an alert if more and more inputs are missing a feature, or the mean or standard deviation drifts too far from the training set, or a categorical feature starts containing new categories.\nFinally, make sure you keep backups of every model you create and have the process and tools in place to roll back to a previous model quickly, in case the new model starts failing badly for some reason. Having backups also makes it possible to easily compare new models with previous ones. Similarly, you should keep backups of every version of your datasets so that you can roll back to a previous dataset if the new one ever gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of outliers). Having backups of your datasets also allows you to evaluate any model against any previous dataset.\nAs you can see, machine learning involves quite a lot of infrastructure. This is a very broad topic called ML Operations (MLOps), which deserves its own book. So don’t be surprised if your first ML project takes a lot of effort and time to build and deploy to production. Fortunately, once all the infrastructure is in place, going from idea to production will be much faster.\n\nAssignment 1:\nTackle a regression task of your choice by following the process you learned in this chapter. For example, you can try tackling the Vehicle dataset, where the goal is to predict the selling price of a used car, based on its age, the number of kilometers it has driven, its make and model, and more. Another good dataset to try is the Bike Sharing dataset: the objective is to predict the number of bikes rented within a period of time (column cnt), based on the day of the week, the time, and the weather conditions.\nSome alternatives you can try (not mandatory):\n\nTry a support vector machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\"linear\" (with various values for the C hyperparameter) or kernel=\"rbf\" (with various values for the C and gamma hyperparameters). Note that support vector machines don’t scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. Don’t worry about what the hyperparameters mean for now.\nTry replacing the GridSearchCV with a RandomizedSearchCV.\n\nFinally, explain your results and what you encountered during the process.\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapter 2 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, 3rd edition"
  },
  {
    "objectID": "IDS/1_1_fundamentals.html",
    "href": "IDS/1_1_fundamentals.html",
    "title": "1. Introduction to Data Science",
    "section": "",
    "text": "Open In Colab"
  },
  {
    "objectID": "IDS/1_1_fundamentals.html#basic-concepts-and-definitions",
    "href": "IDS/1_1_fundamentals.html#basic-concepts-and-definitions",
    "title": "1. Introduction to Data Science",
    "section": "1.1 Basic Concepts and Definitions",
    "text": "1.1 Basic Concepts and Definitions\n\n1.1.1 What is Data Science?\nThere’s a common joke that a data scientist is someone who knows more statistics than a computer scientist and more computer science than a statistician. (Whether or not it’s funny is up for debate.) Like all good jokes, though, it contains a kernel of truth. Some data scientists resemble statisticians who happen to code, while others look more like software engineers who dabble in analytics. Some are cutting-edge machine learning experts, while others focus primarily on business reporting or experimentation. There are PhDs with lengthy publication lists, and there are practitioners who have never opened an academic paper. In short, “data science” is notoriously hard to pin down, because almost any definition risks excluding someone who legitimately practices it.\nStill, the attempt at definition is worthwhile. At its core, we can say that data science is about extracting insight from messy, real-world data. The people who do this go by many titles—data scientist, data analyst, machine learning engineer, statistician, or business intelligence analyst. Even software engineers, researchers, and domain experts frequently find themselves doing the work of data science, whether or not it appears in their job description.\n\n\n\nVenn diagram showing overlap of skills in Data Science\n\n\nThe first diagram helps explain why data science is so difficult to define. It shows that data science sits at the intersection of computer science, mathematics & statistics, and domain knowledge. A successful data scientist combines all three: they need programming and software skills to manipulate data at scale, statistical and mathematical understanding to interpret results rigorously, and domain expertise to make those results meaningful in context. Add in machine learning, and you see how easily the field overlaps with multiple other disciplines.\n\n\n\nVenn diagram showing overlap of skills in Data Roles\n\n\nThe second diagram adds further clarity by mapping how different data-related roles emphasize different skill sets. Software engineers lean toward programming and system design, while data engineers focus on databases, pipelines, and scalable architectures. Data analysts center on reporting, visualization, and business understanding. Data scientists, meanwhile, sit in the overlap: they must balance coding ability, mathematical rigor, and communication skills to bridge technical work with actionable insight.\nThis overlap also underscores another key truth: data science is collaborative by nature. No one person masters the full breadth of skills in both diagrams. Data scientists usually work closely with engineers, analysts, domain experts, and business stakeholders. Technical proficiency alone is not enough—effective communication, critical thinking, and teamwork are equally essential to transform raw data into decisions that matter.\nIn this sense, data science is best seen not as a rigid job title, but as a mode of problem-solving that blends disciplines. The diversity of backgrounds among practitioners isn’t a weakness—it’s the very thing that makes the field so dynamic and impactful.\n\n\n1.1.1 Methodology of Data Science\nThe data science process is an iterative and interdisciplinary workflow that transforms raw data into actionable knowledge. While the exact steps may vary depending on the project, industry, or team, the process generally includes the following key stages:\n\nProblem Definition: The process begins with a clear understanding of the problem to be solved or the question to be answered. This step often requires close collaboration with domain experts and stakeholders to ensure the problem is framed in a way that is both meaningful and solvable with data. A poorly defined problem can derail the entire process, making this stage critical.\nData Collection: Relevant data must then be gathered from diverse sources, which may include relational databases, APIs, web scraping, sensors, surveys, or third-party datasets. At this stage, considerations such as data accessibility, privacy, and quality play a major role in determining the project’s feasibility.\nData Cleaning and Preprocessing: Raw data is rarely analysis-ready. This step involves addressing missing values, correcting inconsistencies, removing duplicates, and transforming variables into usable formats. Depending on the project, preprocessing may also involve scaling, normalization, text parsing, or image transformation. High-quality preprocessing is essential, since poor data hygiene undermines all subsequent stages.\nExploratory Data Analysis (EDA): Before formal modeling, data scientists perform EDA to uncover structure, detect anomalies, visualize distributions, and generate initial hypotheses. This step combines statistical methods, visualization tools, and domain knowledge to shape an intuitive understanding of the dataset and guide later decisions about modeling.\nFeature Engineering: Raw data rarely contains all the information needed for predictive modeling. Data scientists therefore create new variables or transform existing ones to capture relevant patterns. Examples include aggregating time-series signals, encoding categorical variables, or constructing interaction terms. Good feature engineering often determines whether a model succeeds or fails.\nModel Selection and Training: With features prepared, data scientists select suitable algorithms based on the problem type (classification, regression, clustering, etc.), the data structure, and performance trade-offs. The chosen models are then trained on the dataset, often using cross-validation to tune hyperparameters and prevent overfitting.\nModel Evaluation: Trained models must be rigorously evaluated using appropriate metrics, such as accuracy, precision, recall, F1-score, ROC-AUC, or mean squared error, depending on the task. Evaluation also includes robustness testing, fairness assessments, and comparisons with baseline methods to ensure the model adds genuine value.\nModel Deployment: Once validated, the model is deployed into a production environment where it can generate predictions on new, unseen data. Deployment may involve integrating the model into applications, dashboards, or automated decision systems, often requiring collaboration with software engineers and DevOps teams.\nMonitoring and Maintenance: The data science process does not end with deployment. Models degrade over time due to changing data distributions, user behavior, or external factors—a phenomenon known as data drift. Continuous monitoring, retraining, and updating ensure the model remains reliable and relevant.\n\nThis cycle is iterative rather than strictly linear. Insights from later stages (such as poor model performance) often lead to revisiting earlier steps (such as redefining features or collecting additional data). Just as the diagrams showed, successful data science requires more than technical execution: it depends on problem framing, communication, domain expertise, and collaboration across multiple roles."
  },
  {
    "objectID": "IDS/1_1_fundamentals.html#fundamentals-for-data-scientists",
    "href": "IDS/1_1_fundamentals.html#fundamentals-for-data-scientists",
    "title": "1. Introduction to Data Science",
    "section": "1.2 Fundamentals for Data Scientists",
    "text": "1.2 Fundamentals for Data Scientists\n\n1.2.1 Visualizing Data\nA fundamental part of the data scientist’s toolkit is data visualization. Although it is very easy to create visualizations, it’s much harder to produce good ones. There are two primary uses for data visualization:\n\nTo explore the data and find patterns, trends, and anomalies.\nTo communicate results to others.\n\nA wide variety of tools exist for visualizing data. We will be using matplotlib, which is a popular Python library for creating visualizations. It provides a wide range of tools and functions to create various types of plots and charts. To install Matplotlib, you can use pip, the Python package manager. Open your terminal or command prompt and run the following command:\npip install matplotlib\nWe will be using the matplotlib.pyplot module. In it’s simplest use, pyplot maintains an internal state in which you build up a visualization step by step. Once you’re done, you can save it with savefig or display it with show. For example, making simple plots is pretty simple:\n\nimport matplotlib.pyplot as plt\n\nyears = [1950,  1960,  1970,  1980,  1990,  2000,  2010]\ngdp = [300.2,  543.3,  1075.9,  2862.5,  5979.6,  10289.7,  14958.3]\n\n# create a line chart, years on x-axis, gdp on y-axis\nplt.plot(years,  gdp,  color='green',  marker='o',  linestyle='solid')\n\n# add a title\nplt.title(\"Nominal GDP\")\n\n# add a label to the y-axis\nplt.ylabel(\"Billions of $\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_gdp.png')\nplt.show()\n\n\n\n\n\n\n\n\nMaking plots that look publication-quality good is more complicated. There are many ways you can customize your charts with, for example, axis labels, line styles, and point markers. Rather than attempt a comprehensive treatment of these options, we’ll just use (and call attention to) some of them in our examples. Although we won’t be using much of this functionality, matplotlib is capable of producing complicated plots within plots, sophisticated formatting, and interactive visualizations. Check out the matplotlib documentation if you want to go deeper.\n\nBar Charts\nA bar chart is a good choice when you want to show how some quantity varies among some discrete set of items. For instance, the next figure shows how many Academy Awards were won by each of a variety of movies:\n\nmovies = [\"Annie Hall\", \"Ben-Hur\", \"Casablanca\", \"Gandhi\", \"West Side Story\"]\nnum_oscars = [5, 11, 3, 8, 10]\n\n# plot bars with left x-coordinates [0, 1, 2, 3, 4], heights [num_oscars]\nplt.bar(range(len(movies)), num_oscars)\n\nplt.title(\"My Favorite Movies\")     # add a title\nplt.ylabel(\"# of Academy Awards\")   # label the y-axis\n\n# label x-axis with movie names at bar centers\nplt.xticks(range(len(movies)), movies)\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_movies.png')\nplt.show()\n\n\n\n\n\n\n\n\nA bar chart can also be a good choice for plotting histograms of bucketed numeric values, as in the next figure , in order to visually explore how the values are distributed:\n\nfrom collections import Counter\ngrades = [83, 95, 91, 87, 70, 0, 85, 82, 100, 67, 73, 77, 0]\n\n# Bucket grades by decile, but put 100 in with the 90s\nhistogram = Counter(min(grade // 10 * 10, 90) for grade in grades)\n\nplt.bar([x + 5 for x in histogram.keys()],  # Shift bars right by 5\n        list(histogram.values()),           # Give each bar its correct height\n        10,                                 # Give each bar a width of 8\n        edgecolor=(0, 0, 0))                # Black edges for each bar\n\nplt.axis((-5, 105, 0, 5))                  # x-axis from -5 to 105,\n                                           # y-axis from 0 to 5\n\nplt.xticks([10 * i for i in range(11)])    # x-axis labels at 0, 10, ..., 100\nplt.xlabel(\"Decile\")\nplt.ylabel(\"# of Students\")\nplt.title(\"Distribution of Exam 1 Grades\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_grades.png')\nplt.show()\n\n\n\n\n\n\n\n\nThe third argument to plt.bar specifies the bar width. Here we chose a width of 10, to fill the entire decile. We also shifted the bars right by 5, so that, for example, the “10” bar (which corresponds to the decile 10–20) would have its center at 15 and hence occupy the correct range. We also added a black edge to eacch bar to make them visually distinct.\nThe call to plt.axis indicates that we want the x-axis to range from –5 to 105 (just to leave a little space on the left and right), and that the y-axis should range from 0 to 5. And the call to plt.xticks puts x-axis labels at 0, 10, 20, …, 100. Be judicious when using plt.axis. When creating bar charts it is considered especially bad form for your y-axis not to start at 0, since this is an easy way to mislead people:\n\nmentions = [500, 505]\nyears = [2017, 2018]\n\nplt.bar(years, mentions, 0.8)\nplt.xticks(years)\nplt.ylabel(\"# of times I heard someone say 'data science'\")\n\n# if you don't do this, matplotlib will label the x-axis 0, 1\n# and then add a +2.013e3 off in the corner (bad matplotlib!)\nplt.ticklabel_format(useOffset=False)\n\n# misleading y-axis only shows the part above 500\nplt.axis((2016.5, 2018.5, 499, 506))\nplt.title(\"Look at the 'Huge' Increase!\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_misleading_y_axis.png')\nplt.show()\n\n\n\n\n\n\n\n\nHere we use more sensible axes, and it looks far less impressive:\n\nplt.bar(years, mentions, 0.8)\nplt.xticks(years)\nplt.ylabel(\"# of times I heard someone say 'data science'\")\nplt.ticklabel_format(useOffset=False)\n\nplt.axis((2016.5, 2018.5, 0, 550))\nplt.title(\"Not So Huge Anymore\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_non_misleading_y_axis.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLine Charts\nAs we saw already, we can make line charts using plt.plot. These are a good choice for showing trends:\n\nvariance = [1, 2, 4, 8, 16, 32, 64, 128, 256]\nbias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]\ntotal_error = [x + y for x, y in zip(variance, bias_squared)]\nxs = [i for i, _ in enumerate(variance)]\n\n# We can make multiple calls to plt.plot\n# to show multiple series on the same chart\nplt.plot(xs, variance,     'g-',  label='variance')    # green solid line\nplt.plot(xs, bias_squared, 'r-.', label='bias^2')      # red dot-dashed line\nplt.plot(xs, total_error,  'b:',  label='total error')  # blue dotted line\n\n# Because we've assigned labels to each series,\n# we can get a legend for free (loc=9 means \"top center\")\nplt.legend(loc=9)\nplt.xlabel(\"model complexity\")\nplt.xticks([])\nplt.title(\"The Bias-Variance Tradeoff\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_line_chart.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nScatterplots\nA scatterplot is the right choice for visualizing the relationship between two paired sets of data. For example, the next visualization illustrates the relationship between the number of friends your users have and the number of minutes they spend on the site every day:\n\nfriends = [ 70,  65,  72,  63,  71,  64,  60,  64,  67]\nminutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]\nlabels =  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n\nplt.scatter(friends, minutes)\n\n# label each point\nfor label, friend_count, minute_count in zip(labels, friends, minutes):\n    plt.annotate(label,\n        xy=(friend_count, minute_count), # Put the label with its point\n        xytext=(5, -5),                  # but slightly offset\n        textcoords='offset points')\n\nplt.title(\"Daily Minutes vs. Number of Friends\")\nplt.xlabel(\"# of friends\")\nplt.ylabel(\"daily minutes spent on the site\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_scatterplot.png')\nplt.show()\n\n\n\n\n\n\n\n\nIf you’re scattering comparable variables, you might get a misleading picture if you let matplotlib choose the scale.\n\ntest_1_grades = [ 99, 90, 85, 97, 80]\ntest_2_grades = [100, 85, 60, 90, 70]\n\nplt.scatter(test_1_grades, test_2_grades)\nplt.title(\"Axes Aren't Comparable\")\nplt.xlabel(\"test 1 grade\")\nplt.ylabel(\"test 2 grade\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_scatterplot_axes_not_comparable.png')\nplt.show()\n\n\n\n\n\n\n\n\nIf we include a call to plt.axis(\"equal\"), the plot more accurately shows that most of the variation occurs on test 2.\n\ntest_1_grades = [99, 90, 85, 97, 80]\ntest_2_grades = [100, 85, 60, 90, 70]\n\nplt.scatter(test_1_grades, test_2_grades)\nplt.title(\"Axes Are Comparable\")\nplt.axis(\"equal\")\nplt.xlabel(\"test 1 grade\")\nplt.ylabel(\"test 2 grade\")\n\n# save the plot as a PNG file\n# plt.savefig('img/viz_scatterplot_axes_comparable.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFor Further Exploration\n\nMatplotlib Gallery will give you a good idea of the sorts of things you can do with matplotlib (and how to do them).\nSeaborn is a higher-level visualization library built on top of matplotlib that provides a more user-friendly interface and additional features for creating attractive and informative statistical graphics.\nAltair is a declarative statistical visualization library for Python. It allows you to create complex visualizations with concise and expressive code.\nPlotly is another popular visualization library that allows for interactive and web-based visualizations. It provides a wide range of chart types and customization options.\nD3.js is a JavaScript library for creating interactive and dynamic visualizations on the web. While it is not a Python library, it is widely used for data visualization and can be integrated with Python using libraries like Bokeh or Dash.\n\n\n\n\n1.2.2 Linear Algebra\n\nVectors\nA vector is a mathematical object that represents both magnitude (size) and direction. In data science, we think of vectors as ordered lists of numbers that can represent data points, features, or measurements. Geometrically, a vector can be visualized as an arrow pointing from the origin to a specific point in space.\nVectors are fundamental building blocks in linear algebra and essential for:\n\nStoring and manipulating data\nRepresenting features in machine learning\nPerforming mathematical operations on datasets\nComputing distances and similarities between data points\n\nA vector has countless practical applications. In physics, a vector is often thought of as a direction and magnitude. In math, it is a direction and scale on an XY plane, kind of like a movement. In computer science, it is an array of numbers storing data. The computer science context is the one we will become the most familiar with as data science professionals.\nMathematical Notation:\nWe represent vectors using lowercase letters with arrows: \\(\\vec{v}\\), \\(\\vec{w}\\), or in column form:\n\\[\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\]\nTypes of Vectors\nRow Vector: Numbers arranged horizontally \\[\\vec{v} = [v_1, v_2, v_3, ..., v_n]\\]\nColumn Vector: Numbers arranged vertically \\[\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\]\nUnit Vector: A vector with magnitude (length) equal to 1\nZero Vector: A vector where all components are zero \\[\\vec{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\]\nTo emphasize again, the purpose of the vector is to visually represent a piece of data. If you have a data record for the square footage of a house 18,000 square feet and its valuation $260,000, we could express that as a vector [18000, 2600000], stepping 18,000 steps in the horizontal direction and 260,000 steps in the vertical direction.\nWe can declare a vector mathematically like this:\n\\(\\vec{v} = \\begin{bmatrix} 3 , 2 \\end{bmatrix}\\)\nWe can declare a vector using a simple Python collection, like a Python list:\n\nv = [3, 2]\nprint(v)\n\n[3, 2]\n\n\nHowever, when we start doing mathematical computations with vectors, especially when doing tasks like machine learning, we should probably use the numpy library as it is more efficient than plain Python. You can also use sympy to perform linear algebra operations; however, numpy is what you will likely use in practice so that is what we will mainly stick to. To declare a vector, you can use NumPy’s array() function and then can pass a collection of numbers to it\n\nimport numpy as np\n\n# Creating a basic 2D vector\nv = np.array([3, 3])\nprint(\"2D Vector:\")\nprint(f\"v = {v}\")\nprint(f\"Type: {type(v)}\")\nprint(f\"Shape: {v.shape}\")\nprint(f\"Dimension: {v.ndim}\")\n\n2D Vector:\nv = [3 3]\nType: &lt;class 'numpy.ndarray'&gt;\nShape: (2,)\nDimension: 1\n\n\nNote also vectors can exist on more than two dimensions. Next we declare a three- dimensional vector along axes \\(x\\), \\(y\\), and \\(z\\):\n\\(\\vec{v}  = \\begin{bmatrix} 4 , 1 , 2 \\end{bmatrix}\\)\nNaturally, we can express this three-dimensional vector in Python using three numeric values:\n\n# 3D vector\nv3d = np.array([4, 1, 2])\nprint(\"3D Vector:\")\nprint(f\"v3d = {v3d}\")\n\n3D Vector:\nv3d = [4 1 2]\n\n\nOr something like:\n\\(\\vec{v} = \\begin{bmatrix} 6 , 1 , 5 , 8 , 3 \\end{bmatrix}\\)\n\n# Higher dimensional vector (5D)\nv5d = np.array([6, 1, 5, 8, 3])\nprint(\"\\n5D Vector:\")\nprint(f\"v5d = {v5d}\")\n\n\n5D Vector:\nv5d = [6 1 5 8 3]\n\n\n\nVector properties and operations\n\n# Calculate vector magnitude (length)\nv = np.array([3, 4])\nmagnitude = np.linalg.norm(v)\nprint(f\"Vector v = {v}\")\nprint(f\"Magnitude of v = {magnitude}\")\n\n# Create unit vector (normalize)\nunit_v = v / magnitude\nprint(f\"Unit vector = {unit_v}\")\nprint(f\"Unit vector magnitude = {np.linalg.norm(unit_v)}\")\n\n# Zero vector\nzero_vector = np.zeros(3)\nprint(f\"Zero vector: {zero_vector}\")\n\n# Vector of ones\nones_vector = np.ones(4)\nprint(f\"Ones vector: {ones_vector}\")\n\nVector v = [3 4]\nMagnitude of v = 5.0\nUnit vector = [0.6 0.8]\nUnit vector magnitude = 1.0\nZero vector: [0. 0. 0.]\nOnes vector: [1. 1. 1. 1.]\n\n\n\n# Vector addition and subtraction\n\nv1 = np.array([2, 3, 1])\nv2 = np.array([1, -1, 4])\n\n# Addition\nv_sum = v1 + v2\nprint(f\"v1 = {v1}\")\nprint(f\"v2 = {v2}\")\nprint(f\"v1 + v2 = {v_sum}\")\n\n# Subtraction\nv_diff = v1 - v2\nprint(f\"v1 - v2 = {v_diff}\")\n\n# Element-wise operations\nv_mult = v1 * v2  # Element-wise multiplication\nprint(f\"v1 * v2 (element-wise) = {v_mult}\")\n\nv1 = [2 3 1]\nv2 = [ 1 -1  4]\nv1 + v2 = [3 2 5]\nv1 - v2 = [ 1  4 -3]\nv1 * v2 (element-wise) = [ 2 -3  4]\n\n\n\n# Scalar multiplication (scaling)\n\nv = np.array([2, -3, 1])\nscalar = 2.5\n\n# Scale the vector\nscaled_v = scalar * v\nprint(f\"Original vector: {v}\")\nprint(f\"Scalar: {scalar}\")\nprint(f\"Scaled vector: {scaled_v}\")\n\n# Scaling changes magnitude but not direction (unless scalar is negative)\nprint(f\"Original magnitude: {np.linalg.norm(v):.2f}\")\nprint(f\"Scaled magnitude: {np.linalg.norm(scaled_v):.2f}\")\nprint(f\"Magnitude ratio: {np.linalg.norm(scaled_v) / np.linalg.norm(v):.2f}\")\n\nOriginal vector: [ 2 -3  1]\nScalar: 2.5\nScaled vector: [ 5.  -7.5  2.5]\nOriginal magnitude: 3.74\nScaled magnitude: 9.35\nMagnitude ratio: 2.50\n\n\n\n# Dot product (scalar product)\n\nv1 = np.array([1, 2, 3])\nv2 = np.array([4, 5, 6])\n\n# Calculate dot product\ndot_product = np.dot(v1, v2)\nprint(f\"v1 = {v1}\")\nprint(f\"v2 = {v2}\")\nprint(f\"Dot product v1 · v2 = {dot_product}\")\n\n# Alternative syntax\ndot_product_alt = v1 @ v2  # Matrix multiplication operator\nprint(f\"Alternative syntax: v1 @ v2 = {dot_product_alt}\")\n\n# Geometric interpretation: dot product relates to angle between vectors\nangle_cos = dot_product / (np.linalg.norm(v1) * np.linalg.norm(v2))\nangle_rad = np.arccos(angle_cos)\nangle_deg = np.degrees(angle_rad)\nprint(f\"Angle between vectors: {angle_deg:.2f} degrees\")\n\nv1 = [1 2 3]\nv2 = [4 5 6]\nDot product v1 · v2 = 32\nAlternative syntax: v1 @ v2 = 32\nAngle between vectors: 12.93 degrees\n\n\n\n# Vector indexing and slicing\n\ndata = np.array([10, 20, 30, 40, 50, 60])\nprint(f\"Original vector: {data}\")\n\n# Access single elements\nprint(f\"First element: {data[0]}\")\nprint(f\"Last element: {data[-1]}\")\nprint(f\"Third element: {data[2]}\")\n\n# Slicing\nprint(f\"First three elements: {data[:3]}\")\nprint(f\"Last two elements: {data[-2:]}\")\nprint(f\"Middle elements: {data[2:4]}\")\nprint(f\"Every second element: {data[::2]}\")\n\n# Boolean indexing\nmask = data &gt; 30\nprint(f\"Elements greater than 30: {data[mask]}\")\n\n# Find indices where condition is true\nindices = np.where(data &gt; 30)\nprint(f\"Indices where data &gt; 30: {indices[0]}\")\n\nOriginal vector: [10 20 30 40 50 60]\nFirst element: 10\nLast element: 60\nThird element: 30\nFirst three elements: [10 20 30]\nLast two elements: [50 60]\nMiddle elements: [30 40]\nEvery second element: [10 30 50]\nElements greater than 30: [40 50 60]\nIndices where data &gt; 30: [3 4 5]\n\n\n\n# Creating special vectors\n\n# Range-based vectors\nrange_vector = np.arange(0, 10, 2)  # Start, stop, step\nprint(f\"Range vector: {range_vector}\")\n\n# Linearly spaced vectors\nlinear_space = np.linspace(0, 1, 5)  # Start, stop, number of points\nprint(f\"Linear space: {linear_space}\")\n\n# Random vectors\nnp.random.seed(42)  # For reproducible results\nrandom_vector = np.random.randint(1, 10, size=5)\nprint(f\"Random integers: {random_vector}\")\n\nrandom_normal = np.random.normal(0, 1, 5)  # Mean=0, std=1, size=5\nprint(f\"Random normal: {random_normal}\")\n\n# Vector from list\nlist_data = [1.5, 2.7, 3.1, 4.8, 5.2]\nvector_from_list = np.array(list_data)\nprint(f\"From list: {vector_from_list}\")\n\nRange vector: [0 2 4 6 8]\nLinear space: [0.   0.25 0.5  0.75 1.  ]\nRandom integers: [7 4 8 5 7]\nRandom normal: [-0.91682684 -0.12414718 -2.01096289 -0.49280342  0.39257975]\nFrom list: [1.5 2.7 3.1 4.8 5.2]\n\n\nManipulating Data Is Manipulating Vectors: Every data operation can be thought of in terms of vectors, even simple averages. Take scaling, for example. Let’s say we were trying to get the average house value and average square footage for an entire neighborhood. We would add the vectors together to combine their value and square footage respectively, giving us one giant vector containing both total value and total square footage. We then scale down the vector by dividing by the number of houses N , which really is multiplying by 1/ N . We now have a vector containing the average house value and average square footage.\n\n# Vector operations for data analysis\n\n# Sales data over 6 months\nsales_q1 = np.array([10000, 12000, 15000])  # Jan, Feb, Mar\nsales_q2 = np.array([18000, 20000, 22000])  # Apr, May, Jun\n\nprint(\"Sales Data:\")\nprint(f\"Q1 Sales: {sales_q1}\")\nprint(f\"Q2 Sales: {sales_q2}\")\n\n# Calculate quarterly growth\ngrowth = sales_q2 - sales_q1\nprint(f\"Monthly Growth: {growth}\")\n\n# Calculate growth percentage\ngrowth_percentage = (growth / sales_q1) * 100\nprint(f\"Growth Percentage: {growth_percentage}%\")\n\n# Total sales for each quarter\nq1_total = np.sum(sales_q1)\nq2_total = np.sum(sales_q2)\nprint(f\"\\nQ1 Total: ${q1_total:,}\")\nprint(f\"Q2 Total: ${q2_total:,}\")\n\n# Average monthly sales\nq1_avg = np.mean(sales_q1)\nq2_avg = np.mean(sales_q2)\nprint(f\"Q1 Average: ${q1_avg:,.0f}\")\nprint(f\"Q2 Average: ${q2_avg:,.0f}\")\n\nSales Data:\nQ1 Sales: [10000 12000 15000]\nQ2 Sales: [18000 20000 22000]\nMonthly Growth: [8000 8000 7000]\nGrowth Percentage: [80.         66.66666667 46.66666667]%\n\nQ1 Total: $37,000\nQ2 Total: $60,000\nQ1 Average: $12,333\nQ2 Average: $20,000\n\n\n\n\n\nMatrices\nA matrix is a rectangular array of numbers arranged in rows and columns. Matrices are fundamental structures in linear algebra and serve as powerful tools for representing and manipulating data in data science. Think of a matrix as a collection of vectors arranged side by side, or as a way to organize data in a tabular format.\nMathematically, we represent a matrix using capital letters like \\(A\\), \\(B\\), or \\(M\\). A matrix with \\(m\\) rows and \\(n\\) columns is called an \\(m \\times n\\) matrix:\n\\[A = \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}\\]\nFor example, a \\(2 \\times 3\\) matrix looks like:\n\\[A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{bmatrix}\\]\nIn data science, matrices are everywhere. A dataset with multiple features for multiple observations is essentially a matrix where each row represents an observation and each column represents a feature.\nTypes of Matrices\nSquare Matrix: A matrix where the number of rows equals the number of columns (\\(n \\times n\\)).\n\\[A = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{bmatrix}\\]\nIdentity Matrix: A special square matrix with 1s on the diagonal and 0s elsewhere. It’s the matrix equivalent of the number 1 in multiplication.\n\\[I = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\\]\nZero Matrix: A matrix where all elements are zero.\n\\[O = \\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix}\\]\nDiagonal Matrix: A square matrix where all non-diagonal elements are zero.\n\\[D = \\begin{bmatrix}\n3 & 0 & 0 \\\\\n0 & 5 & 0 \\\\\n0 & 0 & 2\n\\end{bmatrix}\\]\nLet’s see how to work with these matrices using NumPy:\n\n# Creating a basic matrix (2x3)\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"Shape: {A.shape}\")\nprint(f\"Dimension: {A.ndim}\")\nprint(f\"Type: {type(A)}\")\n\nMatrix A:\n[[1 2 3]\n [4 5 6]]\nShape: (2, 3)\nDimension: 2\nType: &lt;class 'numpy.ndarray'&gt;\n\n\n\n# Creating a square matrix (3x3)\nB = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\nprint(\"Square Matrix B:\")\nprint(B)\nprint(f\"Shape: {B.shape}\")\n\nSquare Matrix B:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nShape: (3, 3)\n\n\n\n# Creating an identity matrix\nI = np.eye(3)  # 3x3 identity matrix\nprint(\"Identity Matrix:\")\nprint(I)\n\nIdentity Matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n# Creating a zero matrix\nzeros = np.zeros((2, 4))  # 2x4 zero matrix\nprint(\"Zero Matrix:\")\nprint(zeros)\n\nZero Matrix:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n\n\n\n# Creating a diagonal matrix\ndiagonal_values = [3, 5, 2]\nD = np.diag(diagonal_values)\nprint(\"Diagonal Matrix:\")\nprint(D)\n\nDiagonal Matrix:\n[[3 0 0]\n [0 5 0]\n [0 0 2]]\n\n\nMatrix operations\n\n# Matrix addition (matrices must have the same dimensions)\nA1 = np.array([[1, 2], \n               [3, 4]])\nA2 = np.array([[5, 6], \n               [7, 8]])\n\nmatrix_sum = A1 + A2\nprint(\"Matrix Addition:\")\nprint(f\"A1 + A2 = \\n{matrix_sum}\")\n\nMatrix Addition:\nA1 + A2 = \n[[ 6  8]\n [10 12]]\n\n\n\n# Scalar multiplication\nscalar = 3\nscaled_matrix = scalar * A1\nprint(\"Scalar Multiplication:\")\nprint(f\"3 * A1 = \\n{scaled_matrix}\")\n\nScalar Multiplication:\n3 * A1 = \n[[ 3  6]\n [ 9 12]]\n\n\n\n# Matrix transpose (swap rows and columns)\noriginal = np.array([[1, 2, 3],\n                     [4, 5, 6]])\ntransposed = original.T  # or np.transpose(original)\nprint(\"Original Matrix:\")\nprint(original)\nprint(\"Transposed Matrix:\")\nprint(transposed)\n\nOriginal Matrix:\n[[1 2 3]\n [4 5 6]]\nTransposed Matrix:\n[[1 4]\n [2 5]\n [3 6]]\n\n\n\n# Accessing matrix elements\nmatrix = np.array([[10, 20, 30],\n                   [40, 50, 60],\n                   [70, 80, 90]])\n\nprint(\"Full matrix:\")\nprint(matrix)\nprint(f\"Element at row 1, column 2: {matrix[1, 2]}\")  # Remember: 0-indexed\nprint(f\"First row: {matrix[0, :]}\")\nprint(f\"Second column: {matrix[:, 1]}\")\nprint(f\"Submatrix (first 2x2): \\n{matrix[:2, :2]}\")\n\nFull matrix:\n[[10 20 30]\n [40 50 60]\n [70 80 90]]\nElement at row 1, column 2: 60\nFirst row: [10 20 30]\nSecond column: [20 50 80]\nSubmatrix (first 2x2): \n[[10 20]\n [40 50]]\n\n\n\n# Practical example: Student grades matrix\n# Rows represent students, columns represent subjects\ngrades = np.array([[85, 92, 78, 90],  # Student 1: Math, Science, English, History\n                   [79, 85, 88, 92],  # Student 2\n                   [92, 88, 85, 87],  # Student 3\n                   [88, 90, 92, 85]]) # Student 4\n\nsubjects = ['Math', 'Science', 'English', 'History']\nstudents = ['Alice', 'Bob', 'Charlie', 'Diana']\n\nprint(\"Student Grades Matrix:\")\nprint(grades)\nprint(f\"Shape: {grades.shape} (4 students, 4 subjects)\")\n\n# Calculate average grade per student\nstudent_averages = np.mean(grades, axis=1)  # axis=1 means across columns\nprint(\"\\nAverage grades per student:\")\nfor i, student in enumerate(students):\n    print(f\"{student}: {student_averages[i]:.2f}\")\n\n# Calculate average grade per subject\nsubject_averages = np.mean(grades, axis=0)  # axis=0 means across rows\nprint(\"\\nAverage grades per subject:\")\nfor i, subject in enumerate(subjects):\n    print(f\"{subject}: {subject_averages[i]:.2f}\")\n\nStudent Grades Matrix:\n[[85 92 78 90]\n [79 85 88 92]\n [92 88 85 87]\n [88 90 92 85]]\nShape: (4, 4) (4 students, 4 subjects)\n\nAverage grades per student:\nAlice: 86.25\nBob: 86.00\nCharlie: 88.00\nDiana: 88.75\n\nAverage grades per subject:\nMath: 86.00\nScience: 88.75\nEnglish: 85.75\nHistory: 88.50\n\n\n\n\nDeterminants\nA determinant is a scalar value that can be calculated from a square matrix. It provides important information about the matrix’s properties and has numerous applications in linear algebra, geometry, and data science. The determinant tells us whether a matrix is invertible, how it transforms areas and volumes, and is crucial for solving systems of linear equations.\nMathematical Notation:\nFor a square matrix \\(A\\), we denote its determinant as \\(\\det(A)\\) or \\(|A|\\).\nKey Properties of Determinants:\n\nOnly square matrices have determinants\nIf \\(\\det(A) = 0\\), the matrix is singular (non-invertible)\nIf \\(\\det(A) \\neq 0\\), the matrix is invertible\nThe determinant represents the scaling factor of the linear transformation\nFor a 2D matrix, it represents the area scaling factor; for 3D, the volume scaling factor\n\nGeometric Interpretation:\nThe absolute value of the determinant tells us how much the matrix scales areas (in 2D) or volumes (in 3D). A negative determinant indicates that the transformation reverses orientation.\nApplications in Data Science:\n\nChecking if a system of equations has a unique solution\nComputing matrix inverses\nPrincipal Component Analysis (PCA)\nMeasuring multicollinearity in regression analysis\nVolume calculations in high-dimensional spaces\n\n\n# Calculate determinant of a 2x2 matrix\nA_2x2 = np.array([[1, 2],\n                  [2, 5]])\n\ndet_A = np.linalg.det(A_2x2)\nprint(\"2x2 Matrix:\")\nprint(A_2x2)\nprint(f\"Determinant: {det_A}\")\n\n# Manual calculation for 2x2: ad - bc\nmanual_det = A_2x2[0,0] * A_2x2[1,1] - A_2x2[0,1] * A_2x2[1,0]\nprint(f\"Manual calculation: {manual_det}\")\n\n2x2 Matrix:\n[[1 2]\n [2 5]]\nDeterminant: 1.0\nManual calculation: 1\n\n\n\n# Calculate determinant of a 3x3 matrix\nA_3x3 = np.array([[1, 2, 3],\n                  [0, 1, 4],\n                  [5, 6, 0]])\n\ndet_A_3x3 = np.linalg.det(A_3x3)\nprint(\"3x3 Matrix:\")\nprint(A_3x3)\nprint(f\"Determinant: {det_A_3x3:.2f}\")\n\n3x3 Matrix:\n[[1 2 3]\n [0 1 4]\n [5 6 0]]\nDeterminant: 1.00\n\n\n\n# Special matrices and their determinants\n\n# Identity matrix - determinant is always 1\nI = np.eye(3)\nprint(\"Identity Matrix:\")\nprint(I)\nprint(f\"Determinant: {np.linalg.det(I)}\")\n\n# Diagonal matrix - determinant is product of diagonal elements\nD = np.diag([4, 4, 2])\nprint(\"\\nDiagonal Matrix:\")\nprint(D)\nprint(f\"Determinant: {np.linalg.det(D)}\")\nprint(f\"Product of diagonal elements: {4 * 4 * 2}\")\n\nIdentity Matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\nDeterminant: 1.0\n\nDiagonal Matrix:\n[[4 0 0]\n [0 4 0]\n [0 0 2]]\nDeterminant: 32.0\nProduct of diagonal elements: 32\n\n\n\n# Singular matrix (determinant = 0)\nsingular_matrix = np.array([[1, 2, 3],\n                           [2, 4, 6],\n                           [1, 1, 1]])\n\ndet_singular = np.linalg.det(singular_matrix)\nprint(\"Singular Matrix (rows are linearly dependent):\")\nprint(singular_matrix)\nprint(f\"Determinant: {det_singular:.10f}\")  # Should be very close to 0\n\n# This matrix is not invertible\ntry:\n    inverse = np.linalg.inv(singular_matrix)\nexcept np.linalg.LinAlgError as e:\n    print(f\"Cannot invert: {e}\")\n\nSingular Matrix (rows are linearly dependent):\n[[1 2 3]\n [2 4 6]\n [1 1 1]]\nDeterminant: 0.0000000000\nCannot invert: Singular matrix\n\n\n\n# Properties of determinants\n\nA = np.array([[2, 1],\n              [3, 4]])\n\nB = np.array([[1, 2],\n              [0, 3]])\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"det(A) = {np.linalg.det(A)}\")\n\nprint(\"\\nMatrix B:\")\nprint(B)\nprint(f\"det(B) = {np.linalg.det(B)}\")\n\n# Property: det(AB) = det(A) * det(B)\nAB = A @ B\nprint(f\"\\nA @ B:\")\nprint(AB)\nprint(f\"det(AB) = {np.linalg.det(AB)}\")\nprint(f\"det(A) * det(B) = {np.linalg.det(A) * np.linalg.det(B)}\")\n\n# Property: det(A^T) = det(A)\nA_transpose = A.T\nprint(f\"\\ndet(A^T) = {np.linalg.det(A_transpose)}\")\nprint(f\"det(A) = {np.linalg.det(A)}\")\n\nMatrix A:\n[[2 1]\n [3 4]]\ndet(A) = 5.000000000000001\n\nMatrix B:\n[[1 2]\n [0 3]]\ndet(B) = 3.0000000000000004\n\nA @ B:\n[[ 2  7]\n [ 3 18]]\ndet(AB) = 15.0\ndet(A) * det(B) = 15.000000000000005\n\ndet(A^T) = 5.000000000000001\ndet(A) = 5.000000000000001\n\n\n\n# Practical example: Checking system solvability\n\n# System of equations: \n# 2x + 3y = 7\n# 4x + 6y = 14\n\n# Coefficient matrix\ncoeff_matrix = np.array([[2, 3],\n                        [4, 6]])\n\n# Constants vector\nconstants = np.array([7, 14])\n\ndet_coeff = np.linalg.det(coeff_matrix)\nprint(\"Coefficient Matrix:\")\nprint(coeff_matrix)\nprint(f\"Determinant: {det_coeff}\")\n\nif abs(det_coeff) &lt; 1e-10:  # Close to zero\n    print(\"System has no unique solution (infinite solutions or no solution)\")\n    # Check if system is consistent\n    augmented = np.column_stack([coeff_matrix, constants])\n    rank_coeff = np.linalg.matrix_rank(coeff_matrix)\n    rank_augmented = np.linalg.matrix_rank(augmented)\n    \n    if rank_coeff == rank_augmented:\n        print(\"System has infinite solutions\")\n    else:\n        print(\"System has no solution\")\nelse:\n    print(\"System has a unique solution\")\n    solution = np.linalg.solve(coeff_matrix, constants)\n    print(f\"Solution: x = {solution[0]}, y = {solution[1]}\")\n\nCoefficient Matrix:\n[[2 3]\n [4 6]]\nDeterminant: 0.0\nSystem has no unique solution (infinite solutions or no solution)\nSystem has infinite solutions\n\n\n\n\nEigenvectors and Eigenvalues\nEigenvectors and eigenvalues are fundamental concepts in linear algebra that reveal the intrinsic properties of linear transformations. They are crucial for understanding how matrices act on vectors and have profound applications in data science, particularly in dimensionality reduction, principal component analysis (PCA), and machine learning.\nMathematical Definition:\nFor a square matrix \\(A\\), an eigenvector \\(\\vec{v}\\) is a non-zero vector that, when multiplied by \\(A\\), results in a scalar multiple of itself:\n\\[A\\vec{v} = \\lambda\\vec{v}\\]\nWhere: - \\(\\vec{v}\\) is the eigenvector (direction that doesn’t change) - \\(\\lambda\\) (lambda) is the eigenvalue (scaling factor) - \\(A\\) is the square matrix\nGeometric Interpretation:\nWhen a matrix transforms a vector, most vectors change both direction and magnitude. However, eigenvectors are special - they only change in magnitude (scaled by the eigenvalue) but maintain their direction. Think of it as finding the “natural” directions of a transformation.\nKey Properties:\n\nA matrix can have multiple eigenvector-eigenvalue pairs\nEigenvectors corresponding to different eigenvalues are orthogonal\nThe eigenvalues tell us how much the matrix stretches or shrinks in each eigenvector direction\nIf eigenvalue λ &gt; 1: stretching, if 0 &lt; λ &lt; 1: shrinking, if λ &lt; 0: reflection\n\nApplications in Data Science:\n\nPrincipal Component Analysis (PCA): Finding directions of maximum variance\nDimensionality Reduction: Identifying most important features\nGoogle PageRank: Ranking web pages using eigenvectors\nImage Compression: Using eigenfaces for face recognition\nStability Analysis: Determining system behavior in dynamic models\nSpectral Clustering: Graph-based clustering algorithms\n\n\n# Basic example: Finding eigenvalues and eigenvectors\nA = np.array([[3, 1],\n              [0, 2]])\n\n# Calculate eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"Matrix A:\")\nprint(A)\nprint(f\"\\nEigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors:\\n{eigenvectors}\")\n\n# Verify the eigenvalue equation: Av = λv\nfor i in range(len(eigenvalues)):\n    v = eigenvectors[:, i]  # i-th eigenvector\n    lambda_val = eigenvalues[i]  # i-th eigenvalue\n    \n    Av = A @ v\n    lambda_v = lambda_val * v\n    \n    print(f\"\\nEigenvector {i+1}: {v}\")\n    print(f\"A * v = {Av}\")\n    print(f\"λ * v = {lambda_v}\")\n    print(f\"Are they equal? {np.allclose(Av, lambda_v)}\")\n\nMatrix A:\n[[3 1]\n [0 2]]\n\nEigenvalues: [3. 2.]\nEigenvectors:\n[[ 1.         -0.70710678]\n [ 0.          0.70710678]]\n\nEigenvector 1: [1. 0.]\nA * v = [3. 0.]\nλ * v = [3. 0.]\nAre they equal? True\n\nEigenvector 2: [-0.70710678  0.70710678]\nA * v = [-1.41421356  1.41421356]\nλ * v = [-1.41421356  1.41421356]\nAre they equal? True\n\n\n\n# Symmetric matrix example (real eigenvalues, orthogonal eigenvectors)\nS = np.array([[4, 2],\n              [2, 3]])\n\neigenvals_s, eigenvecs_s = np.linalg.eig(S)\n\nprint(\"Symmetric Matrix S:\")\nprint(S)\nprint(f\"Eigenvalues: {eigenvals_s}\")\nprint(f\"Eigenvectors:\\n{eigenvecs_s}\")\n\n# Check orthogonality of eigenvectors\ndot_product = np.dot(eigenvecs_s[:, 0], eigenvecs_s[:, 1])\nprint(f\"\\nDot product of eigenvectors: {dot_product:.10f}\")\nprint(f\"Eigenvectors are orthogonal: {abs(dot_product) &lt; 1e-10}\")\n\nSymmetric Matrix S:\n[[4 2]\n [2 3]]\nEigenvalues: [5.56155281 1.43844719]\nEigenvectors:\n[[ 0.78820544 -0.61541221]\n [ 0.61541221  0.78820544]]\n\nDot product of eigenvectors: 0.0000000000\nEigenvectors are orthogonal: True\n\n\n\n# 3D example\nA_3d = np.array([[6, -2, 2],\n                 [-2, 3, -1],\n                 [2, -1, 3]])\n\neigenvals_3d, eigenvecs_3d = np.linalg.eig(A_3d)\n\nprint(\"3D Matrix:\")\nprint(A_3d)\nprint(f\"\\nEigenvalues: {eigenvals_3d}\")\nprint(f\"\\nEigenvectors:\")\nfor i in range(len(eigenvals_3d)):\n    print(f\"λ{i+1} = {eigenvals_3d[i]:.3f}, v{i+1} = {eigenvecs_3d[:, i]}\")\n\n3D Matrix:\n[[ 6 -2  2]\n [-2  3 -1]\n [ 2 -1  3]]\n\nEigenvalues: [8. 2. 2.]\n\nEigenvectors:\nλ1 = 8.000, v1 = [ 0.81649658 -0.40824829  0.40824829]\nλ2 = 2.000, v2 = [-0.57735027 -0.57735027  0.57735027]\nλ3 = 2.000, v3 = [-0.11547005  0.57735027  0.80829038]\n\n\n\n# Practical example: Principal Component Analysis (PCA) basics\nnp.random.seed(42)\n\n# Generate correlated 2D data\nn_samples = 100\nx1 = np.random.normal(0, 2, n_samples)\nx2 = 1.5 * x1 + np.random.normal(0, 1, n_samples)\ndata = np.column_stack([x1, x2])\n\nprint(\"Original data shape:\", data.shape)\n\n# Center the data\ndata_centered = data - np.mean(data, axis=0)\n\n# Calculate covariance matrix\ncov_matrix = np.cov(data_centered.T)\nprint(\"Covariance Matrix:\")\nprint(cov_matrix)\n\n# Find principal components (eigenvectors of covariance matrix)\neigenvals_pca, eigenvecs_pca = np.linalg.eig(cov_matrix)\n\n# Sort by eigenvalue (descending order)\nidx = np.argsort(eigenvals_pca)[::-1]\neigenvals_pca = eigenvals_pca[idx]\neigenvecs_pca = eigenvecs_pca[:, idx]\n\nprint(f\"\\nPrincipal Component eigenvalues: {eigenvals_pca}\")\nprint(f\"Principal Component eigenvectors:\\n{eigenvecs_pca}\")\n\n# The first eigenvector is the direction of maximum variance\nprint(f\"\\nFirst PC explains {eigenvals_pca[0]/(eigenvals_pca[0]+eigenvals_pca[1])*100:.1f}% of variance\")\nprint(f\"Second PC explains {eigenvals_pca[1]/(eigenvals_pca[0]+eigenvals_pca[1])*100:.1f}% of variance\")\n\nOriginal data shape: (100, 2)\nCovariance Matrix:\n[[3.29907957 4.71231098]\n [4.71231098 7.62348838]]\n\nPrincipal Component eigenvalues: [10.64597323  0.27659473]\nPrincipal Component eigenvectors:\n[[-0.53989052 -0.84173525]\n [-0.84173525  0.53989052]]\n\nFirst PC explains 97.5% of variance\nSecond PC explains 2.5% of variance\n\n\n\n\nFor Further Exploration\n\n3Blue1Brown’s Linear Algebra Series provides intuitive visual explanations of linear algebra concepts, including eigenvectors and eigenvalues.\n\n\n\n\n1.2.3 Probability and Statistics\nProbability and statistics form the mathematical foundation of data science. While probability helps us model uncertainty and make predictions about future events, statistics provides tools to analyze data, test hypotheses, and draw meaningful conclusions. These concepts are essential for exploratory data analysis, hypothesis testing, and building robust machine learning models.\nKey Areas We’ll Cover:\n\nBasic probability concepts and distributions\nDescriptive statistics and data summarization\nStatistical inference and hypothesis testing\nBayes’ theorem and its applications\nCorrelation and causation\nStatistical distributions commonly used in data science\n\nUnderstanding these concepts is crucial for:\n\nExploratory Data Analysis (EDA): Summarizing and visualizing data patterns\nStatistical Inference: Making conclusions about populations from samples\nMachine Learning: Understanding model assumptions and uncertainty\nRisk Assessment: Quantifying uncertainty in business decisions\n\n\nBasic Probability Concepts\nProbability is a measure of the likelihood that an event will occur, expressed as a number between 0 and 1, where:\n\n0 means the event is impossible\n1 means the event is certain\n0.5 means the event has equal chances of occurring or not\n\nKey Definitions:\nSample Space (S): The set of all possible outcomes of an experiment\n\nRolling a dice: S = \\(\\{1, 2, 3, 4, 5, 6\\}\\)\nFlipping a coin: S = \\(\\{Heads, Tails\\}\\)\n\nEvent (E): A subset of the sample space\n\nRolling an even number: E = \\(\\{2, 4, 6\\}\\)\nGetting heads: E = \\(\\{Heads\\}\\)\n\nProbability of an Event: P(E) = (Number of favorable outcomes) / (Total number of possible outcomes)\nFundamental Rules:\n\nAddition Rule: P(A ∪ B) = P(A) + P(B) - P(A ∩ B)\nMultiplication Rule: P(A ∩ B) = P(A) × P(B|A)\nComplement Rule: P(A’) = 1 - P(A)\n\nConditional Probability: P(A|B) = P(A ∩ B) / P(B) The probability of event A given that event B has occurred.\nIndependence: Two events A and B are independent if P(A|B) = P(A) Or equivalently: P(A ∩ B) = P(A) × P(B)\n\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\n# Set style for better visualizations\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Basic probability examples\nnp.random.seed(42)\n\n# Simulate coin flips\nn_flips = 1000\ncoin_flips = np.random.choice(['Heads', 'Tails'], n_flips, p=[0.5, 0.5])\n\n# Calculate probabilities\nheads_count = np.sum(coin_flips == 'Heads')\nprob_heads = heads_count / n_flips\n\nprint(\"Coin Flip Simulation:\")\nprint(f\"Number of flips: {n_flips}\")\nprint(f\"Heads: {heads_count}\")\nprint(f\"Tails: {n_flips - heads_count}\")\nprint(f\"Probability of Heads: {prob_heads:.3f}\")\nprint(f\"Expected Probability: 0.500\")\n\n# Visualize convergence to true probability\ncumulative_prob = np.cumsum(coin_flips == 'Heads') / np.arange(1, n_flips + 1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(cumulative_prob, linewidth=2, label='Observed Probability')\nplt.axhline(y=0.5, color='red', linestyle='--', label='True Probability (0.5)')\nplt.xlabel('Number of Flips')\nplt.ylabel('Probability of Heads')\nplt.title('Law of Large Numbers: Coin Flip Convergence')\nplt.legend()\nplt.grid(True, alpha=0.3)\n# plt.savefig('img/probability_convergence.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nCoin Flip Simulation:\nNumber of flips: 1000\nHeads: 503\nTails: 497\nProbability of Heads: 0.503\nExpected Probability: 0.500\n\n\n\n\n\n\n\n\n\n\n# Dice rolling example - multiple events\nn_rolls = 10000\ndice_rolls = np.random.randint(1, 7, n_rolls)\n\n# Calculate various probabilities\nprob_even = np.sum(dice_rolls % 2 == 0) / n_rolls\nprob_greater_4 = np.sum(dice_rolls &gt; 4) / n_rolls\nprob_1_or_6 = np.sum((dice_rolls == 1) | (dice_rolls == 6)) / n_rolls\n\nprint(\"Dice Rolling Simulation:\")\nprint(f\"Number of rolls: {n_rolls}\")\nprint(f\"P(Even number): {prob_even:.3f} (Expected: 0.500)\")\nprint(f\"P(&gt; 4): {prob_greater_4:.3f} (Expected: 0.333)\")\nprint(f\"P(1 or 6): {prob_1_or_6:.3f} (Expected: 0.333)\")\n\n# Visualize dice roll distribution\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nvalues, counts = np.unique(dice_rolls, return_counts=True)\nplt.bar(values, counts/n_rolls, alpha=0.7, color='skyblue')\nplt.axhline(y=1/6, color='red', linestyle='--', label='Expected Probability (1/6)')\nplt.xlabel('Dice Value')\nplt.ylabel('Probability')\nplt.title('Dice Roll Distribution')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\ntwo_dice = []\nfor i in range(1, 7):\n    for j in range(1, 7):\n        two_dice.append(i + j)\n\nplt.hist(two_dice, bins=range(2, 14), density=True, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Sum of Two Dice')\nplt.ylabel('Probability')\nplt.title('Sum of Two Dice Distribution')\nplt.xticks(range(2, 13))\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('img/dice_probability.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Calculate P(sum = 7)\nsum_7_count = len([x for x in two_dice if x == 7])\nprob_sum_7 = sum_7_count / len(two_dice)\nprint(f\"\\nP(Sum = 7 with two dice): {prob_sum_7:.3f}\")\n\nDice Rolling Simulation:\nNumber of rolls: 10000\nP(Even number): 0.501 (Expected: 0.500)\nP(&gt; 4): 0.333 (Expected: 0.333)\nP(1 or 6): 0.334 (Expected: 0.333)\n\n\n\n\n\n\n\n\n\n\nP(Sum = 7 with two dice): 0.167\n\n\n\n\nDescriptive Statistics\nDescriptive statistics help us summarize and understand the main characteristics of a dataset. They provide insight into the center, spread, and shape of data distributions.\nMeasures of Central Tendency:\nMean (μ or x̄): The arithmetic average \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]\nMedian: The middle value when data is ordered\n\nRobust to outliers\nBetter than mean for skewed distributions\n\nMode: The most frequently occurring value(s)\nMeasures of Dispersion:\nVariance (σ² or s²): Average of squared differences from the mean \\[\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2\\]\nStandard Deviation (σ or s): Square root of variance \\[\\sigma = \\sqrt{\\sigma^2}\\]\nRange: Difference between maximum and minimum values\nInterquartile Range (IQR): Difference between 75th and 25th percentiles\n\nQ3 - Q1\nMeasures spread of middle 50% of data\n\nMeasures of Shape:\nSkewness: Measure of asymmetry\n\nPositive skew: tail extends to the right\nNegative skew: tail extends to the left\n\nKurtosis: Measure of tail heaviness\n\nHigh kurtosis: heavy tails, more outliers\nLow kurtosis: light tails, fewer outliers\n\n\n# Generate sample datasets for statistical analysis\nnp.random.seed(42)\n\n# Create different types of distributions\nnormal_data = np.random.normal(100, 15, 1000)  # Mean=100, std=15\nskewed_data = np.random.exponential(2, 1000)   # Exponential distribution\nuniform_data = np.random.uniform(0, 100, 1000) # Uniform distribution\n\n# Create a DataFrame\ndf = pd.DataFrame({\n    'Normal': normal_data,\n    'Skewed': skewed_data,\n    'Uniform': uniform_data\n})\n\nprint(\"Descriptive Statistics Summary:\")\nprint(\"=\" * 50)\nprint(df.describe())\n\n# Calculate additional statistics\nprint(\"\\nAdditional Statistics:\")\nprint(\"=\" * 30)\nfor col in df.columns:\n    data = df[col]\n    print(f\"\\n{col} Distribution:\")\n    print(f\"  Median: {np.median(data):.2f}\")\n    print(f\"  Mode: {stats.mode(data)[0]:.2f}\")\n    print(f\"  Variance: {np.var(data, ddof=1):.2f}\")\n    print(f\"  Skewness: {stats.skew(data):.2f}\")\n    print(f\"  Kurtosis: {stats.kurtosis(data):.2f}\")\n    print(f\"  IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}\")\n\nDescriptive Statistics Summary:\n==================================================\n            Normal       Skewed      Uniform\ncount  1000.000000  1000.000000  1000.000000\nmean    100.289981     2.015972    49.449499\nstd      14.688239     2.005977    28.891967\nmin      51.380990     0.006447     0.001163\n25%      90.286145     0.567909    25.652351\n50%     100.379509     1.451862    49.171060\n75%     109.719158     2.743369    73.862888\nmax     157.790972    14.883446    99.782086\n\nAdditional Statistics:\n==============================\n\nNormal Distribution:\n  Median: 100.38\n  Mode: 51.38\n  Variance: 215.74\n  Skewness: 0.12\n  Kurtosis: 0.07\n  IQR: 19.43\n\nSkewed Distribution:\n  Median: 1.45\n  Mode: 0.01\n  Variance: 4.02\n  Skewness: 1.98\n  Kurtosis: 5.38\n  IQR: 2.18\n\nUniform Distribution:\n  Median: 49.17\n  Mode: 0.00\n  Variance: 834.75\n  Skewness: 0.01\n  Kurtosis: -1.18\n  IQR: 48.21\n\n\n\n# Visualize distributions and their statistics\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Row 1: Histograms\nfor i, col in enumerate(df.columns):\n    axes[0, i].hist(df[col], bins=50, density=True, alpha=0.7, color=f'C{i}')\n    axes[0, i].axvline(df[col].mean(), color='red', linestyle='--', \n                      linewidth=2, label=f'Mean: {df[col].mean():.1f}')\n    axes[0, i].axvline(df[col].median(), color='green', linestyle='--', \n                      linewidth=2, label=f'Median: {df[col].median():.1f}')\n    axes[0, i].set_title(f'{col} Distribution')\n    axes[0, i].set_xlabel('Value')\n    axes[0, i].set_ylabel('Density')\n    axes[0, i].legend()\n    axes[0, i].grid(True, alpha=0.3)\n\n# Row 2: Box plots\nfor i, col in enumerate(df.columns):\n    axes[1, i].boxplot(df[col])\n    axes[1, i].set_title(f'{col} Box Plot')\n    axes[1, i].set_ylabel('Value')\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('img/descriptive_stats_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStatistical Distributions\nStatistical distributions are mathematical functions that describe the probability of different outcomes in a dataset. Understanding common distributions is crucial for data analysis and modeling.\nNormal Distribution (Gaussian)\n\nBell-shaped, symmetric distribution\nDefined by mean (μ) and standard deviation (σ)\n68-95-99.7 rule (empirical rule)\nMany natural phenomena follow normal distribution\n\nProperties:\n\nMean = Median = Mode\n68% of data within 1σ of mean\n95% of data within 2σ of mean\n\n99.7% of data within 3σ of mean\n\nOther Important Distributions:\n\nUniform Distribution: All outcomes equally likely\nExponential Distribution: Models time between events\nPoisson Distribution: Models count of events in fixed intervals\nBinomial Distribution: Models number of successes in n trials\nChi-square Distribution: Used in hypothesis testing\nt-Distribution: Used when sample size is small\n\n\n# Demonstrate various statistical distributions\nfig, axes = plt.subplots(3, 3, figsize=(15, 12))\nfig.suptitle('Common Statistical Distributions', fontsize=16)\n\n# 1. Normal Distribution\nx = np.linspace(-4, 4, 100)\nnormal_pdf = stats.norm.pdf(x, 0, 1)\naxes[0, 0].plot(x, normal_pdf, 'b-', linewidth=2, label='μ=0, σ=1')\naxes[0, 0].fill_between(x, normal_pdf, alpha=0.3)\naxes[0, 0].set_title('Normal Distribution')\naxes[0, 0].set_xlabel('Value')\naxes[0, 0].set_ylabel('Probability Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Uniform Distribution\nx_uniform = np.linspace(-0.5, 3.5, 100)\nuniform_pdf = stats.uniform.pdf(x_uniform, 0, 3)\naxes[0, 1].plot(x_uniform, uniform_pdf, 'r-', linewidth=2, label='a=0, b=3')\naxes[0, 1].fill_between(x_uniform, uniform_pdf, alpha=0.3)\naxes[0, 1].set_title('Uniform Distribution')\naxes[0, 1].set_xlabel('Value')\naxes[0, 1].set_ylabel('Probability Density')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Exponential Distribution\nx_exp = np.linspace(0, 5, 100)\nexp_pdf = stats.expon.pdf(x_exp, scale=1)\naxes[0, 2].plot(x_exp, exp_pdf, 'g-', linewidth=2, label='λ=1')\naxes[0, 2].fill_between(x_exp, exp_pdf, alpha=0.3)\naxes[0, 2].set_title('Exponential Distribution')\naxes[0, 2].set_xlabel('Value')\naxes[0, 2].set_ylabel('Probability Density')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# 4. Poisson Distribution\nx_poisson = np.arange(0, 15)\npoisson_pmf = stats.poisson.pmf(x_poisson, mu=3)\naxes[1, 0].bar(x_poisson, poisson_pmf, alpha=0.7, color='purple', label='λ=3')\naxes[1, 0].set_title('Poisson Distribution')\naxes[1, 0].set_xlabel('Number of Events')\naxes[1, 0].set_ylabel('Probability Mass')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# 5. Binomial Distribution\nx_binomial = np.arange(0, 21)\nbinomial_pmf = stats.binom.pmf(x_binomial, n=20, p=0.3)\naxes[1, 1].bar(x_binomial, binomial_pmf, alpha=0.7, color='orange', label='n=20, p=0.3')\naxes[1, 1].set_title('Binomial Distribution')\naxes[1, 1].set_xlabel('Number of Successes')\naxes[1, 1].set_ylabel('Probability Mass')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# 6. Chi-square Distribution\nx_chi2 = np.linspace(0, 15, 100)\nchi2_pdf = stats.chi2.pdf(x_chi2, df=4)\naxes[1, 2].plot(x_chi2, chi2_pdf, 'brown', linewidth=2, label='df=4')\naxes[1, 2].fill_between(x_chi2, chi2_pdf, alpha=0.3)\naxes[1, 2].set_title('Chi-square Distribution')\naxes[1, 2].set_xlabel('Value')\naxes[1, 2].set_ylabel('Probability Density')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\n# 7. t-Distribution\nx_t = np.linspace(-4, 4, 100)\nt_pdf = stats.t.pdf(x_t, df=5)\nnormal_pdf_comp = stats.norm.pdf(x_t, 0, 1)\naxes[2, 0].plot(x_t, t_pdf, 'red', linewidth=2, label='t-dist (df=5)')\naxes[2, 0].plot(x_t, normal_pdf_comp, 'blue', linewidth=2, linestyle='--', label='Normal')\naxes[2, 0].set_title('t-Distribution vs Normal')\naxes[2, 0].set_xlabel('Value')\naxes[2, 0].set_ylabel('Probability Density')\naxes[2, 0].legend()\naxes[2, 0].grid(True, alpha=0.3)\n\n# 8. Beta Distribution\nx_beta = np.linspace(0, 1, 100)\nbeta_pdf = stats.beta.pdf(x_beta, a=2, b=5)\naxes[2, 1].plot(x_beta, beta_pdf, 'pink', linewidth=2, label='α=2, β=5')\naxes[2, 1].fill_between(x_beta, beta_pdf, alpha=0.3)\naxes[2, 1].set_title('Beta Distribution')\naxes[2, 1].set_xlabel('Value')\naxes[2, 1].set_ylabel('Probability Density')\naxes[2, 1].legend()\naxes[2, 1].grid(True, alpha=0.3)\n\n# 9. Gamma Distribution\nx_gamma = np.linspace(0, 10, 100)\ngamma_pdf = stats.gamma.pdf(x_gamma, a=2, scale=1)\naxes[2, 2].plot(x_gamma, gamma_pdf, 'cyan', linewidth=2, label='α=2, β=1')\naxes[2, 2].fill_between(x_gamma, gamma_pdf, alpha=0.3)\naxes[2, 2].set_title('Gamma Distribution')\naxes[2, 2].set_xlabel('Value')\naxes[2, 2].set_ylabel('Probability Density')\naxes[2, 2].legend()\naxes[2, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('img/statistical_distributions.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Real-world applications of distributions with simulated data\nprint(\"Distribution Applications in Data Science:\")\nprint(\"=\" * 50)\n\n# 1. Normal Distribution - Height data\nheights = np.random.normal(170, 10, 1000)  # Heights in cm\nprint(f\"Height Data (Normal Distribution):\")\nprint(f\"  Mean height: {np.mean(heights):.1f} cm\")\nprint(f\"  Std deviation: {np.std(heights):.1f} cm\")\nprint(f\"  % between 160-180 cm: {np.sum((heights &gt;= 160) & (heights &lt;= 180))/len(heights)*100:.1f}%\")\n\n# 2. Exponential Distribution - Time between customer arrivals\narrival_times = np.random.exponential(5, 1000)  # Average 5 minutes between arrivals\nprint(f\"\\nCustomer Arrival Times (Exponential Distribution):\")\nprint(f\"  Average time between arrivals: {np.mean(arrival_times):.1f} minutes\")\nprint(f\"  % of arrivals within 2 minutes: {np.sum(arrival_times &lt;= 2)/len(arrival_times)*100:.1f}%\")\n\n# 3. Poisson Distribution - Number of website visits per hour\nvisits_per_hour = np.random.poisson(50, 24*7)  # 50 average visits per hour for a week\nprint(f\"\\nWebsite Visits (Poisson Distribution):\")\nprint(f\"  Average visits per hour: {np.mean(visits_per_hour):.1f}\")\nprint(f\"  Max visits in an hour: {np.max(visits_per_hour)}\")\nprint(f\"  Hours with &gt;60 visits: {np.sum(visits_per_hour &gt; 60)}\")\n\n# 4. Binomial Distribution - A/B Testing\n# 100 users, 15% conversion rate\nconversions = np.random.binomial(100, 0.15, 1000)  # 1000 experiments\nprint(f\"\\nA/B Test Conversions (Binomial Distribution):\")\nprint(f\"  Average conversions per 100 users: {np.mean(conversions):.1f}\")\nprint(f\"  95% confidence interval: [{np.percentile(conversions, 2.5):.0f}, {np.percentile(conversions, 97.5):.0f}]\")\n\n# Visualize these applications\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Heights\naxes[0, 0].hist(heights, bins=50, density=True, alpha=0.7, color='skyblue')\nx_heights = np.linspace(heights.min(), heights.max(), 100)\naxes[0, 0].plot(x_heights, stats.norm.pdf(x_heights, np.mean(heights), np.std(heights)), \n                'r-', linewidth=2, label='Normal Fit')\naxes[0, 0].set_title('Heights Distribution')\naxes[0, 0].set_xlabel('Height (cm)')\naxes[0, 0].set_ylabel('Density')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Arrival times\naxes[0, 1].hist(arrival_times, bins=50, density=True, alpha=0.7, color='lightgreen')\nx_arrivals = np.linspace(0, arrival_times.max(), 100)\naxes[0, 1].plot(x_arrivals, stats.expon.pdf(x_arrivals, scale=np.mean(arrival_times)), \n                'r-', linewidth=2, label='Exponential Fit')\naxes[0, 1].set_title('Time Between Customer Arrivals')\naxes[0, 1].set_xlabel('Time (minutes)')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Website visits\nx_visits = np.arange(0, visits_per_hour.max()+1)\naxes[1, 0].hist(visits_per_hour, bins=30, density=True, alpha=0.7, color='lightcoral')\naxes[1, 0].plot(x_visits, stats.poisson.pmf(x_visits, np.mean(visits_per_hour)), \n                'ro-', linewidth=2, markersize=4, label='Poisson Fit')\naxes[1, 0].set_title('Website Visits per Hour')\naxes[1, 0].set_xlabel('Number of Visits')\naxes[1, 0].set_ylabel('Density')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Conversions\nx_conv = np.arange(0, conversions.max()+1)\naxes[1, 1].hist(conversions, bins=20, density=True, alpha=0.7, color='gold')\naxes[1, 1].plot(x_conv, stats.binom.pmf(x_conv, 100, np.mean(conversions)/100), \n                'ro-', linewidth=2, markersize=3, label='Binomial Fit')\naxes[1, 1].set_title('A/B Test Conversions')\naxes[1, 1].set_xlabel('Number of Conversions')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\n# plt.savefig('img/distribution_applications.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nDistribution Applications in Data Science:\n==================================================\nHeight Data (Normal Distribution):\n  Mean height: 170.1 cm\n  Std deviation: 10.3 cm\n  % between 160-180 cm: 65.7%\n\nCustomer Arrival Times (Exponential Distribution):\n  Average time between arrivals: 5.0 minutes\n  % of arrivals within 2 minutes: 33.4%\n\nWebsite Visits (Poisson Distribution):\n  Average visits per hour: 50.7\n  Max visits in an hour: 75\n  Hours with &gt;60 visits: 14\n\nA/B Test Conversions (Binomial Distribution):\n  Average conversions per 100 users: 14.9\n  95% confidence interval: [9, 22]\n\n\n\n\n\n\n\n\n\n\n\nBayes’ Theorem\nBayes’ theorem is a fundamental principle in probability theory that describes how to update the probability of a hypothesis based on new evidence. It’s the foundation of Bayesian statistics and has numerous applications in machine learning, medical diagnosis, spam filtering, and decision-making.\nMathematical Formula:\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nWhere:\n\nP(A|B): Posterior probability - probability of A given B\nP(B|A): Likelihood - probability of B given A\n\nP(A): Prior probability - initial probability of A\nP(B): Marginal probability - total probability of B\n\nIn words: Posterior = (Likelihood × Prior) / Evidence\nKey Components:\n\nPrior P(A): Our initial belief about the probability of A before seeing evidence\nLikelihood P(B|A): How likely we are to observe evidence B if A is true\nEvidence P(B): Total probability of observing evidence B\nPosterior P(A|B): Updated probability of A after observing evidence B\n\nApplications in Data Science:\n\nMedical Diagnosis: Update disease probability based on test results\nSpam Detection: Classify emails based on word patterns\nMachine Learning: Naive Bayes classifier\n\n\n# Bayes' Theorem Examples\n\nprint(\"Bayes' Theorem Applications\")\nprint(\"=\" * 40)\n\n# Example 1: Medical Diagnosis\nprint(\"Example 1: Medical Diagnosis\")\nprint(\"-\" * 30)\n\n# Disease affects 1% of population\nprior_disease = 0.01\nprior_no_disease = 1 - prior_disease\n\n# Test accuracy: 95% sensitivity, 90% specificity\nsensitivity = 0.95  # P(Test+ | Disease)\nspecificity = 0.90  # P(Test- | No Disease)\nfalse_positive_rate = 1 - specificity  # P(Test+ | No Disease)\n\n# Evidence: P(Test+)\nevidence_test_positive = (sensitivity * prior_disease) + (false_positive_rate * prior_no_disease)\n\n# Posterior: P(Disease | Test+)\nposterior_disease_given_positive = (sensitivity * prior_disease) / evidence_test_positive\n\nprint(f\"Prior probability of disease: {prior_disease:.1%}\")\nprint(f\"Test sensitivity: {sensitivity:.1%}\")\nprint(f\"Test specificity: {specificity:.1%}\")\nprint(f\"Probability of positive test: {evidence_test_positive:.1%}\")\nprint(f\"Probability of disease given positive test: {posterior_disease_given_positive:.1%}\")\n\n# Example 2: Spam Detection\nprint(f\"\\nExample 2: Spam Detection\")\nprint(\"-\" * 30)\n\n# Prior probabilities\nprior_spam = 0.3  # 30% of emails are spam\nprior_ham = 0.7   # 70% of emails are legitimate\n\n# Likelihood of word \"free\" appearing\nlikelihood_free_given_spam = 0.8   # 80% of spam contains \"free\"\nlikelihood_free_given_ham = 0.1    # 10% of legitimate emails contain \"free\"\n\n# Evidence: P(\"free\")\nevidence_free = (likelihood_free_given_spam * prior_spam) + (likelihood_free_given_ham * prior_ham)\n\n# Posterior: P(Spam | \"free\")\nposterior_spam_given_free = (likelihood_free_given_spam * prior_spam) / evidence_free\n\nprint(f\"Prior probability of spam: {prior_spam:.1%}\")\nprint(f\"P('free' | spam): {likelihood_free_given_spam:.1%}\")\nprint(f\"P('free' | legitimate): {likelihood_free_given_ham:.1%}\")\nprint(f\"P(spam | 'free'): {posterior_spam_given_free:.1%}\")\n\nBayes' Theorem Applications\n========================================\nExample 1: Medical Diagnosis\n------------------------------\nPrior probability of disease: 1.0%\nTest sensitivity: 95.0%\nTest specificity: 90.0%\nProbability of positive test: 10.8%\nProbability of disease given positive test: 8.8%\n\nExample 2: Spam Detection\n------------------------------\nPrior probability of spam: 30.0%\nP('free' | spam): 80.0%\nP('free' | legitimate): 10.0%\nP(spam | 'free'): 77.4%\n\n\n\n\nCorrelation and Causation\nUnderstanding the relationship between variables is crucial in data analysis. Correlation measures the strength and direction of a linear relationship between two variables, while causation implies that one variable directly influences another.\nKey Principle: “Correlation does not imply causation”\nCorrelation Coefficient (Pearson’s r): \\[r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}\\]\nInterpretation:\n\nr = +1: Perfect positive correlation\nr = 0: No linear correlation\n\nr = -1: Perfect negative correlation\n|r| &gt; 0.7: Strong correlation\n0.3 &lt; |r| &lt; 0.7: Moderate correlation\n|r| &lt; 0.3: Weak correlation\n\nTypes of Relationships:\n\nNo relationship: Variables are independent\nLinear relationship: Variables change at a constant rate\nNon-linear relationship: Variables are related but not linearly\nSpurious correlation: Variables appear related due to confounding factors\n\nEstablishing Causation:\n\nTemporal precedence: Cause must precede effect\nCovariation: Cause and effect must be correlated\nNon-spuriousness: Relationship isn’t due to third variable\nMechanism: Logical explanation for how cause leads to effect\n\n\n# Generate data to demonstrate correlation vs causation\nnp.random.seed(42)\nn = 500\n\n# Example 1: Strong positive correlation\nx1 = np.random.normal(0, 1, n)\ny1 = 2 * x1 + np.random.normal(0, 0.5, n)  # y = 2x + noise\n\n# Example 2: No correlation\nx2 = np.random.normal(0, 1, n)\ny2 = np.random.normal(0, 1, n)  # Independent variables\n\n# Example 3: Non-linear relationship\nx3 = np.random.uniform(-3, 3, n)\ny3 = x3**2 + np.random.normal(0, 1, n)  # Quadratic relationship\n\n# Example 4: Spurious correlation (both depend on third variable)\nz = np.random.normal(0, 1, n)  # Hidden variable\nx4 = z + np.random.normal(0, 0.5, n)\ny4 = z + np.random.normal(0, 0.5, n)\n\n# Calculate correlation coefficients\ncorr1 = np.corrcoef(x1, y1)[0, 1]\ncorr2 = np.corrcoef(x2, y2)[0, 1]\ncorr3 = np.corrcoef(x3, y3)[0, 1]\ncorr4 = np.corrcoef(x4, y4)[0, 1]\n\nprint(\"Correlation Analysis Examples:\")\nprint(\"=\" * 40)\nprint(f\"1. Linear relationship: r = {corr1:.3f}\")\nprint(f\"2. No relationship: r = {corr2:.3f}\")\nprint(f\"3. Non-linear relationship: r = {corr3:.3f}\")\nprint(f\"4. Spurious correlation: r = {corr4:.3f}\")\n\n# Create comprehensive correlation visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nfig.suptitle('Correlation vs Causation Examples', fontsize=16)\n\n# Example 1: Strong positive correlation\naxes[0, 0].scatter(x1, y1, alpha=0.6, color='blue')\nz1 = np.polyfit(x1, y1, 1)\np1 = np.poly1d(z1)\naxes[0, 0].plot(x1, p1(x1), \"r--\", alpha=0.8, linewidth=2)\naxes[0, 0].set_title(f'Strong Positive Correlation\\nr = {corr1:.3f}')\naxes[0, 0].set_xlabel('X Variable')\naxes[0, 0].set_ylabel('Y Variable')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Example 2: No correlation\naxes[0, 1].scatter(x2, y2, alpha=0.6, color='green')\naxes[0, 1].set_title(f'No Correlation\\nr = {corr2:.3f}')\naxes[0, 1].set_xlabel('X Variable')\naxes[0, 1].set_ylabel('Y Variable')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Example 3: Non-linear relationship\naxes[0, 2].scatter(x3, y3, alpha=0.6, color='red')\n# Fit quadratic curve\nz3 = np.polyfit(x3, y3, 2)\np3 = np.poly1d(z3)\nx3_sorted = np.linspace(x3.min(), x3.max(), 100)\naxes[0, 2].plot(x3_sorted, p3(x3_sorted), \"orange\", linewidth=2, label='Quadratic fit')\naxes[0, 2].set_title(f'Non-linear Relationship\\nPearson r = {corr3:.3f}')\naxes[0, 2].set_xlabel('X Variable')\naxes[0, 2].set_ylabel('Y Variable')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# Example 4: Spurious correlation\naxes[1, 0].scatter(x4, y4, alpha=0.6, color='purple')\nz4 = np.polyfit(x4, y4, 1)\np4 = np.poly1d(z4)\naxes[1, 0].plot(x4, p4(x4), \"orange\", alpha=0.8, linewidth=2)\naxes[1, 0].set_title(f'Spurious Correlation\\nr = {corr4:.3f}')\naxes[1, 0].set_xlabel('X Variable')\naxes[1, 0].set_ylabel('Y Variable')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Create correlation heatmap for multiple variables\n# Real-world example: student performance data\nnp.random.seed(42)\nstudy_hours = np.random.gamma(2, 3, n)  # Hours studied per week\nsleep_hours = 8 - 0.1 * study_hours + np.random.normal(0, 1, n)  # Sleep affected by study\nsleep_hours = np.clip(sleep_hours, 4, 12)  # Reasonable sleep range\ntest_score = 50 + 2 * study_hours + 3 * sleep_hours + np.random.normal(0, 10, n)\ntest_score = np.clip(test_score, 0, 100)  # Score between 0-100\nstress_level = 100 - test_score + np.random.normal(0, 15, n)  # Stress inversely related to score\nstress_level = np.clip(stress_level, 0, 100)\n\n# Create DataFrame\nstudent_data = pd.DataFrame({\n    'Study_Hours': study_hours,\n    'Sleep_Hours': sleep_hours,\n    'Test_Score': test_score,\n    'Stress_Level': stress_level\n})\n\n# Correlation matrix\ncorr_matrix = student_data.corr()\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\nsns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n            square=True, ax=axes[1, 1], cbar_kws={\"shrink\": .8})\naxes[1, 1].set_title('Student Performance\\nCorrelation Matrix')\n\n# Pairplot equivalent using scatter plots\naxes[1, 2].scatter(student_data['Study_Hours'], student_data['Test_Score'], \n                  alpha=0.6, color='darkblue')\naxes[1, 2].set_title('Study Hours vs Test Score\\nCausal Relationship?')\naxes[1, 2].set_xlabel('Study Hours per Week')\naxes[1, 2].set_ylabel('Test Score')\naxes[1, 2].grid(True, alpha=0.3)\n\n# Add trend line\nz_study = np.polyfit(student_data['Study_Hours'], student_data['Test_Score'], 1)\np_study = np.poly1d(z_study)\naxes[1, 2].plot(student_data['Study_Hours'], p_study(student_data['Study_Hours']), \n               \"red\", alpha=0.8, linewidth=2)\n\nplt.tight_layout()\n# plt.savefig('img/correlation_causation_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# Print correlation analysis\nprint(f\"\\nStudent Performance Correlations:\")\nprint(\"-\" * 35)\nfor i, var1 in enumerate(student_data.columns):\n    for var2 in student_data.columns[i+1:]:\n        corr = student_data[var1].corr(student_data[var2])\n        print(f\"{var1} vs {var2}: r = {corr:.3f}\")\n\nCorrelation Analysis Examples:\n========================================\n1. Linear relationship: r = 0.969\n2. No relationship: r = -0.022\n3. Non-linear relationship: r = -0.021\n4. Spurious correlation: r = 0.820\n\n\n\n\n\n\n\n\n\n\nStudent Performance Correlations:\n-----------------------------------\nStudy_Hours vs Sleep_Hours: r = -0.323\nStudy_Hours vs Test_Score: r = 0.556\nStudy_Hours vs Stress_Level: r = -0.288\nSleep_Hours vs Test_Score: r = 0.005\nSleep_Hours vs Stress_Level: r = -0.008\nTest_Score vs Stress_Level: r = -0.600\n\n\n\n\nFor Further Exploration\nCheck out these resources to deepen your understanding of probability and statistics:\n\nNormal Distribution\nCorrelation\nBayes’ Theorem\nCentral Limit Theorem\n\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapters 3-6 Data Science from Scratch by Joel Grus, 2nd edition\nChapters 2-4 Essential Math for Data Science by Thomas Nield, 1st edition"
  },
  {
    "objectID": "IDS/1_4_dimensionality_reduction.html#dimensionality-reduction",
    "href": "IDS/1_4_dimensionality_reduction.html#dimensionality-reduction",
    "title": "1. Introduction to Data Science",
    "section": "1.4 Dimensionality Reduction",
    "text": "1.4 Dimensionality Reduction\n\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\n\n1.4.1 The Curse of Dimensionality\nWe are so used to living in three dimensions that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our minds, let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.\n\n\n\nPoint, segment, square, cube, and tesseract (0D to 4D hypercubes)\n\n\nAs a result, high-dimensional datasets are often very sparse: most training instances are likely to be far away from each other, so training methods based on distance or similarity (such as k-nearest neighbors) will be much less effective. And some types of models will not be usable at all because they scale poorly with the dataset’s dimensionality (e.g., SVMs or dense neural networks). And new instances will likely be far away from any training instance, making predictions much less reliable than in lower dimensions since they will be based on much larger extrapolations. Since patterns in the data will become harder to identify, models will tend to fit the noise more frequently than in lower dimensions; regularization will become all the more important. Lastly, models will become even harder to interpret.\nIn theory, some of these issues can be resolved by increasing the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features, all ranging from 0 to 1, you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions.\nIn most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.\n\n# extra code\n\nimport numpy as np\nfrom scipy.spatial.transform import Rotation\n\nm = 60\nX = np.zeros((m, 3))  # initialize 3D dataset\nrng = np.random.default_rng(seed=42)\nangles = (rng.random(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\nX[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\nX += 0.28 * rng.standard_normal((m, 3))  # add more noise\nX = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\nX += [0.2, 0, 0.2]  # shift a bit\n\n\n# extra code\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX2D = pca.fit_transform(X)  # dataset reduced to 2D\nX3D_inv = pca.inverse_transform(X2D)  # 3D position of the projected samples\nX_centered = X - X.mean(axis=0)\nU, s, Vt = np.linalg.svd(X_centered)\n\naxes = [-1.4, 1.4, -1.4, 1.4, -1.1, 1.1]\nx1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 10),\n                     np.linspace(axes[2], axes[3], 10))\nw1, w2 = np.linalg.solve(Vt[:2, :2], Vt[:2, 2])  # projection plane coefs\nz = w1 * (x1 - pca.mean_[0]) + w2 * (x2 - pca.mean_[1]) - pca.mean_[2]  # plane\nX3D_above = X[X[:, 2] &gt;= X3D_inv[:, 2]]  # samples above plane\nX3D_below = X[X[:, 2] &lt; X3D_inv[:, 2]]  # samples below plane\n\nfig = plt.figure(figsize=(9, 9))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# plot samples and projection lines below plane first\nax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"ro\", alpha=0.3)\nfor i in range(m):\n    if X[i, 2] &lt; X3D_inv[i, 2]:\n        ax.plot([X[i][0], X3D_inv[i][0]],\n                [X[i][1], X3D_inv[i][1]],\n                [X[i][2], X3D_inv[i][2]], \":\", color=\"#F88\")\n\nax.plot_surface(x1, x2, z, alpha=0.1, color=\"b\")  # projection plane\nax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b+\")  # projected samples\nax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b.\")\n\n# now plot projection lines and samples above plane\nfor i in range(m):\n    if X[i, 2] &gt;= X3D_inv[i, 2]:\n        ax.plot([X[i][0], X3D_inv[i][0]],\n                [X[i][1], X3D_inv[i][1]],\n                [X[i][2], X3D_inv[i][2]], \"r--\")\n\nax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"ro\")\n\ndef set_xyz_axes(ax, axes):\n    ax.xaxis.set_rotate_label(False)\n    ax.yaxis.set_rotate_label(False)\n    ax.zaxis.set_rotate_label(False)\n    ax.set_xlabel(\"$x_1$\", labelpad=8, rotation=0)\n    ax.set_ylabel(\"$x_2$\", labelpad=8, rotation=0)\n    ax.set_zlabel(\"$x_3$\", labelpad=8, rotation=0)\n    ax.set_xlim(axes[0:2])\n    ax.set_ylim(axes[2:4])\n    ax.set_zlim(axes[4:6])\n\nset_xyz_axes(ax, axes)\nax.set_zticks([-1, -0.5, 0, 0.5, 1])\n\n\nplt.show()\n\n\n\n\n\n\n\n\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the higher-dimensional (3D) space. If we project every training instance perpendicularly onto this subspace (as represented by the short dashed lines connecting the instances to the plane), we get a new 2D dataset. Ta-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that the axes correspond to new features \\(z_1\\) and \\(z_2\\): they are the coordinates of the projections on the plane.\n\n# extra code\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1, aspect='equal')\nax.plot(X2D[:, 0], X2D[:, 1], \"b+\")\nax.plot(X2D[:, 0], X2D[:, 1], \"b.\")\nax.plot([0], [0], \"bo\")\nax.arrow(0, 0, 1, 0, head_width=0.05, length_includes_head=True,\n         head_length=0.1, fc='b', ec='b', linewidth=4)\nax.arrow(0, 0, 0, 1, head_width=0.05, length_includes_head=True,\n         head_length=0.1, fc='b', ec='b', linewidth=1)\nax.set_xlabel(\"$z_1$\")\nax.set_yticks([-0.5, 0, 0.5, 1])\nax.set_ylabel(\"$z_2$\", rotation=0)\nax.set_axisbelow(True)\nax.grid(True)\n\n\n\n\n\n\n\n\n\n\n1.4.2 Unsupervised dimensionality reduction: Principal Component Analysis (PCA)\nPrincipal component analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it. PCA (Principal Component Analysis) is a feature extraction algorithm. The main difference is that in a selection algorithm, the original features are preserved, whereas in an extraction algorithm, the data is transformed or projected onto a new feature space.\n\n# extra code\n\nangle = np.pi / 5\nstretch = 5\nm = 200\n\nrng = np.random.default_rng(seed=3)\nX_line = rng.standard_normal((m, 2)) / 10\nX_line = X_line @ np.array([[stretch, 0], [0, 1]])  # stretch\nX_line = X_line @ [[np.cos(angle), np.sin(angle)],\n                   [np.sin(angle), np.cos(angle)]]  # rotate\n\nu1 = np.array([np.cos(angle), np.sin(angle)])\nu2 = np.array([np.cos(angle - 2 * np.pi / 6), np.sin(angle - 2 * np.pi / 6)])\nu3 = np.array([np.cos(angle - np.pi / 2), np.sin(angle - np.pi / 2)])\n\nX_proj1 = X_line @ u1.reshape(-1, 1)\nX_proj2 = X_line @ u2.reshape(-1, 1)\nX_proj3 = X_line @ u3.reshape(-1, 1)\n\nplt.figure(figsize=(8, 4))\nplt.subplot2grid((3, 2), (0, 0), rowspan=3)\nplt.plot([-1.4, 1.4], [-1.4 * u1[1] / u1[0], 1.4 * u1[1] / u1[0]], \"k-\",\n         linewidth=2)\nplt.plot([-1.4, 1.4], [-1.4 * u2[1] / u2[0], 1.4 * u2[1] / u2[0]], \"k--\",\n         linewidth=2)\nplt.plot([-1.4, 1.4], [-1.4 * u3[1] / u3[0], 1.4 * u3[1] / u3[0]], \"k:\",\n         linewidth=2)\nplt.plot(X_line[:, 0], X_line[:, 1], \"ro\", alpha=0.5)\nplt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=4, alpha=0.9,\n          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\nplt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=1, alpha=0.9,\n          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\nplt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", color=\"blue\")\nplt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", color=\"blue\")\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\", rotation=0)\nplt.axis([-1.4, 1.4, -1.4, 1.4])\nplt.grid()\n\nplt.subplot2grid((3, 2), (0, 1))\nplt.plot([-2, 2], [0, 0], \"k-\", linewidth=2)\nplt.plot(X_proj1[:, 0], np.zeros(m), \"ro\", alpha=0.3)\nplt.gca().get_yaxis().set_ticks([])\nplt.gca().get_xaxis().set_ticklabels([])\nplt.axis([-2, 2, -1, 1])\nplt.grid()\n\nplt.subplot2grid((3, 2), (1, 1))\nplt.plot([-2, 2], [0, 0], \"k--\", linewidth=2)\nplt.plot(X_proj2[:, 0], np.zeros(m), \"ro\", alpha=0.3)\nplt.gca().get_yaxis().set_ticks([])\nplt.gca().get_xaxis().set_ticklabels([])\nplt.axis([-2, 2, -1, 1])\nplt.grid()\n\nplt.subplot2grid((3, 2), (2, 1))\nplt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\nplt.plot(X_proj3[:, 0], np.zeros(m), \"ro\", alpha=0.3)\nplt.gca().get_yaxis().set_ticks([])\nplt.axis([-2, 2, -1, 1])\nplt.xlabel(\"$z_1$\")\nplt.grid()\n\n\nplt.show()\n\n\n\n\n\n\n\n\nPCA helps to identify patterns in data based on the correlation between features; that is, it attempts to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with an equal or smaller number of dimensions than the original. The algorithm consists of the following steps:\n\nAlgorithm 1.1 Principal Component Analysis\n\nStandardize the \\(d\\)-dimensional dataset.\nObtain the covariance matrix.\nDecompose the covariance matrix into its eigenvalues and eigenvectors.\nSort the eigenvalues in descending order along with their corresponding eigenvectors.\nSelect the \\(k\\) eigenvectors that correspond to the \\(k\\) largest eigenvalues; \\(k\\) is the dimension of the new feature subspace (\\(k &lt; d\\)).\nConstruct a projection matrix \\(\\mathbf{W}\\) from the top \\(k\\) eigenvectors.\nTransform the \\(d\\)-dimensional input dataset \\(\\mathbf{X}\\) using the projection matrix \\(\\mathbf{W}\\) to obtain the new \\(k\\)-dimensional feature subspace.\n\nPCA identifies the axis that accounts for the largest amount of variance in the training set. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of the remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset.\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX2D = pca.fit_transform(X)\n\nAfter fitting the PCA transformer to the dataset, its components_ attribute holds the transpose of \\(\\mathbf{W_d}\\): it contains one row for each of the first \\(d\\) principal components.\n\npca.components_\n\narray([[ 0.66824153,  0.73208333,  0.13231495],\n       [ 0.74374636, -0.66151587, -0.09611511]])\n\n\nAnother useful piece of information is the explained variance ratio of each principal component, available via the explained_variance_ratio_ variable. The ratio indicates the proportion of the dataset’s variance that lies along each principal component. For example, let’s look at the explained variance ratios of the first two components of the 3D dataset\n\npca.explained_variance_ratio_\n\narray([0.82279334, 0.10821224])\n\n\nThis output tells us that about 82% of the dataset’s variance lies along the first PC, and about 11% lies along the second PC. This leaves about 7% for the third PC, so it is reasonable to assume that the third PC probably carries little information.\n\n\n1.4.3 Choosing the Right number of Dimensions\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance—say, 95%. (An exception to this rule, of course, is if you are reducing dimensionality for data visualization, in which case you will want to reduce the dimensionality down to 2 or 3.)\nThe following code loads and splits the MNIST dataset and performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:\n\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', as_frame=False)\nX_train, y_train = mnist.data[:60_000], mnist.target[:60_000]\nX_test, y_test = mnist.data[60_000:], mnist.target[60_000:]\n\npca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum &gt;= 0.95) + 1  # d equals 154\n\n\nprint(f\"Number of dimensions to preserve 95% of the variance: {d}\")\n\nNumber of dimensions to preserve 95% of the variance: 154\n\n\nYou could then set n_components=d and run PCA again, but there’s a better option. Instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:\n\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)\n\n\npca.n_components_\n\nnp.int64(154)\n\n\n\npca.explained_variance_ratio_.sum()\n\nnp.float64(0.9501960192613034)\n\n\nYet another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum). There will usually be an elbow in the curve, where the explained variance stops growing fast. In this case, you can see that reducing the dimensionality down to about 100 dimensions wouldn’t lose too much explained variance.\n\n# extra code\n\nplt.figure(figsize=(6, 4))\nplt.plot(cumsum, linewidth=3)\nplt.axis([0, 400, 0, 1])\nplt.xlabel(\"Dimensions\")\nplt.ylabel(\"Explained Variance\")\nplt.plot([d, d], [0, 0.95], \"k:\")\nplt.plot([0, d], [0.95, 0.95], \"k:\")\nplt.plot(d, 0.95, \"ko\")\nplt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n             arrowprops=dict(arrowstyle=\"-&gt;\"))\nplt.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, if you are using dimensionality reduction as a preprocessing step for a supervised learning task (e.g., classification), then you can tune the number of dimensions as you would any other hyperparameter. For example, the following code example creates a two-step pipeline, first reducing dimensionality using PCA, then classifying using a random forest. Next, it uses RandomizedSearchCV to find a good combination of hyperparameters for both PCA and the random forest classifier. This example does a quick search, tuning only 2 hyperparameters, training on just 1,000 instances, and running for just 10 iterations, but feel free to do a more thorough search if you have the time:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\n\nclf = make_pipeline(PCA(random_state=42),\n                    RandomForestClassifier(random_state=42))\nparam_distrib = {\n    \"pca__n_components\": np.arange(10, 80),\n    \"randomforestclassifier__n_estimators\": np.arange(50, 500)\n}\nrnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3,\n                                random_state=42)\nrnd_search.fit(X_train[:1000], y_train[:1000])\n\nRandomizedSearchCV(cv=3,\n                   estimator=Pipeline(steps=[('pca', PCA(random_state=42)),\n                                             ('randomforestclassifier',\n                                              RandomForestClassifier(random_state=42))]),\n                   param_distributions={'pca__n_components': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n       27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n       44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n       6...\n       414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426,\n       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n       492, 493, 494, 495, 496, 497, 498, 499])},\n                   random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCV?Documentation for RandomizedSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nPipeline(step...m_state=42))])\n\n\n\nparam_distributions \n{'pca__n_components': array([10, 11... 78, 79]), 'randomforestclassifier__n_estimators': array([ 50, ...97, 498, 499])}\n\n\n\nn_iter \n10\n\n\n\nscoring \nNone\n\n\n\nn_jobs \nNone\n\n\n\nrefit \nTrue\n\n\n\ncv \n3\n\n\n\nverbose \n0\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nrandom_state \n42\n\n\n\nerror_score \nnan\n\n\n\nreturn_train_score \nFalse\n\n\n\n\n            \n        \n    best_estimator_: PipelinePCA?Documentation for PCA\n        \n            \n                Parameters\n                \n\n\n\n\nn_components \nnp.int64(57)\n\n\n\ncopy \nTrue\n\n\n\nwhiten \nFalse\n\n\n\nsvd_solver \n'auto'\n\n\n\ntol \n0.0\n\n\n\niterated_power \n'auto'\n\n\n\nn_oversamples \n10\n\n\n\npower_iteration_normalizer \n'auto'\n\n\n\nrandom_state \n42\n\n\n\n\n            \n        \n    RandomForestClassifier?Documentation for RandomForestClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \nnp.int64(475)\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \n42\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\nrnd_search.best_params_\n\n{'randomforestclassifier__n_estimators': np.int64(475),\n 'pca__n_components': np.int64(57)}\n\n\nIt’s interesting to note how low the optimal number of components is: we reduced a 784-dimensional dataset to just 57 dimensions! This is tied to the fact that we used a random forest, which is a pretty powerful model. If we used a linear model instead, such as an SGDClassifier, the search would find that we need to preserve more dimensions (about 70).\n\n\n1.4.4 PCA for Compression\nAfter dimensionality reduction, the training set takes up much less space. For example, after applying PCA to the MNIST dataset while preserving 95% of its variance, we are left with 154 features, instead of the original 784 features. So the dataset is now less than 20% of its original size, and we only lost 5% of its variance! This is a reasonable compression ratio, and it’s easy to see how such a size reduction would speed up a classification algorithm tremendously.\nIt is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. This won’t give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error.\nThe inverse_transform() method lets us decompress the reduced MNIST dataset back to 784 dimensions:\n\npca = PCA(0.95)\nX_reduced = pca.fit_transform(X_train, y_train)\n\n\nX_recovered = pca.inverse_transform(X_reduced)\n\n\n# extra code\n\nplt.figure(figsize=(7, 4))\nfor idx, X in enumerate((X_train[::2100], X_recovered[::2100])):\n    plt.subplot(1, 2, idx + 1)\n    plt.title([\"Original\", \"Compressed\"][idx])\n    for row in range(5):\n        for col in range(5):\n            plt.imshow(X[row * 5 + col].reshape(28, 28), cmap=\"binary\",\n                       vmin=0, vmax=255, extent=(row, row + 1, col, col + 1))\n            plt.axis([0, 5, 0, 5])\n            plt.axis(\"off\")\n\n\n\n\n\n\n\n\n\n\n1.4.5 Supervised dimensionality reduction: Linear Discriminant Analysis (LDA)\nLinear discriminant analysis (LDA) is a linear classification algorithm that, during training, learns the most discriminative axes between the classes. These axes can then be used to define a hyperplane onto which to project the data. The benefit of this approach is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm (unless LDA alone is sufficient).\nIn general, the concepts behind LDA are very similar to PCA: while PCA seeks the orthogonal components of maximum variance, the objective of LDA is to find a feature subspace that optimizes class separability.\nThe algorithm consists of the following steps:\n\nAlgorithm 1.2 Linear Discriminant Analysis\n\nStandardize the \\(d\\)-dimensional dataset.\nFor each class, compute its \\(d\\)-dimensional mean vector.\nObtain the between-class scatter matrix \\(S_B\\) and the within-class scatter matrix \\(S_W\\).\nDetermine the eigenvalues and corresponding eigenvectors for the matrix \\(S_W^{-1}S_B\\).\nSort the eigenvalues in descending order along with their corresponding eigenvectors.\nSelect the \\(k\\) eigenvectors that correspond to the \\(k\\) largest eigenvalues to construct a \\(d \\times k\\)-dimensional projection matrix \\(\\mathbf{W}\\); the eigenvectors are the columns of this matrix.\nProject the samples onto the new feature subspace using the projection matrix \\(\\mathbf{W}\\).\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components=2)\nX_reduced = lda.fit_transform(X_train, y_train)\n\n\n1.4.6 Nonlinear dimensionality reduction: Kernel Principal Component Analysis (KPCA)\nSome machine learning algorithms assume linear separability of the input data; for example, the perceptron requires perfect separability to guarantee convergence. On the other hand, there are algorithms that assume the lack of linear separability is due to noise; for example, logistic regression.\nHowever, if we have a nonlinear problem, which is very common in real-world applications, linear transformation techniques for dimensionality reduction (PCA, LDA) may not be a good option. This section presents a kernelized version of PCA: Kernel Principal Component Analysis (KPCA), used to transform data that is not linearly separable into a new, lower-dimensional subspace that is suitable for linear classifiers.\n\nKernel functions\nWhen we have nonlinear problems, they can be addressed by projecting them into a higher-dimensional feature space where the classes become linearly separable. To transform the samples \\(x \\in \\mathbb{R}^d\\) onto a higher \\(k\\)-dimensional space, a mapping function \\(\\phi\\) must be defined:\n\\[\\phi : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k; (k \\gg d)\\]\n\\(\\phi\\) can be seen as a function that generates nonlinear combinations of the original features from the original \\(d\\)-dimensional dataset onto a \\(k\\)-dimensional feature space. For example, for a two-dimensional vector \\(x \\in \\mathbb{R}^2\\), a potential mapping to the 3-dimensional space \\(\\mathbb{R}^3\\) could be:\n\\[x = [x_1, x_2]^T\\] \\[\\downarrow \\phi\\] \\[z = [x_1^2, \\sqrt{2}x_1x_2, x_2^2]^T\\]\nIn this way, PCA can be used on a higher-order space and then projected onto a lower-dimensional space where the samples can be separated by a linear classifier.\nTo implement a KPCA with a Radial Basis Function (RBF) or Gaussian function as the kernel, the following steps are performed:\n\nAlgorithm 1.3 Kernel Principal Component Analysis (RBF Kernel)\n\nObtain the kernel matrix K, calculated as:\n\\[\nk\\bigl(x^{(i)}, x^{(j)}\\bigr) = \\exp\\left(-\\gamma \\|x^{(i)} - x^{(j)}\\|^2\\right); \\quad \\gamma = \\frac{1}{2\\sigma}\n\\]\nfor each pair of samples:\n\\[\n\\mathbf{K} =\n\\begin{bmatrix}\nk\\bigl(x^{(1)}, x^{(1)}\\bigr) & k\\bigl(x^{(1)}, x^{(2)}\\bigr) & \\cdots & k\\bigl(x^{(1)}, x^{(n)}\\bigr) \\\\\nk\\bigl(x^{(2)}, x^{(1)}\\bigr) & k\\bigl(x^{(2)}, x^{(2)}\\bigr) & \\cdots & k\\bigl(x^{(2)}, x^{(n)}\\bigr) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk\\bigl(x^{(n)}, x^{(1)}\\bigr) & k\\bigl(x^{(n)}, x^{(2)}\\bigr) & \\cdots & k\\bigl(x^{(n)}, x^{(n)}\\bigr)\n\\end{bmatrix}\n\\]\nFor example, if we have 100 training samples, the kernel matrix will have dimensions \\(100 \\times 100\\).\nCenter the matrix K using:\n\\[\n\\mathbf{K}' = \\mathbf{K} - \\mathbf{1}_n \\mathbf{K} - \\mathbf{K}\\mathbf{1}_n + \\mathbf{1}_n \\mathbf{K}\\mathbf{1}_n\n\\]\nwhere \\(\\mathbf{1}_n\\) is an \\(n \\times n\\) matrix in which all entries are \\(\\frac{1}{n}\\).\nSelect the (k) eigenvectors of the centered matrix that correspond to the \\(k\\) largest eigenvalues.\n\n\nCentering the matrix (step 2) is required because it is not possible to guarantee that the new space is centered at zero. Other commonly used kernel functions include the polynomial kernel and the hyperbolic tangent kernel (sigmoid).\n\n\nExample: PCA vs Kernel PCA (RBF) on make_moons and make_circles\nLet’s look at a couple classic non-linear problems: concentric circles. Linear PCA will fail to separate these, as there is no linear projection that can place the two circles apart. Kernel PCA, by mapping the data to a higher dimension, can effectively “unroll” the circles.\n\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, random_state=123)\nplt.scatter(X[y==0,0], X[y==0,1], color='red', marker='^', alpha=0.5)\nplt.scatter(X[y==1,0], X[y==1,1], color='blue', marker='o', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n# It is not necessary to standardize because the axes have similar ranges but it is a good practice to do so\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nax[0].scatter(X_pca[y==0,0], X_pca[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_pca[y==1,0], X_pca[y==1,1], color='blue', marker='o', alpha=0.5)\nax[1].scatter(X_pca[y==0,0], np.zeros((50,1))+0.02, color='red', marker='^', alpha=0.5)\nax[1].scatter(X_pca[y==1,0], np.zeros((50,1))-0.02, color='blue', marker='o', alpha=0.5)\nax[0].set_ylabel('PC2')\nax[0].set_xlabel('PC1')\nax[1].set_xlabel('PC1')\nax[1].set_ylim((-1,1))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import KernelPCA\n\nX, y = make_moons(n_samples=100, random_state=123)\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\nX_kpca = kpca.fit_transform(X)\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nax[0].scatter(X_kpca[y==0,0], X_kpca[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_kpca[y==1,0], X_kpca[y==1,1], color='blue', marker='o', alpha=0.5)\nax[1].scatter(X_kpca[y==0,0], np.zeros((50,1))+0.02, color='red', marker='^', alpha=0.5)\nax[1].scatter(X_kpca[y==1,0], np.zeros((50,1))-0.02, color='blue', marker='o', alpha=0.5)\nax[0].set_ylabel('PC2')\nax[0].set_xlabel('PC1')\nax[1].set_xlabel('PC1')\nax[1].set_ylim((-1,1))\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\nplt.scatter(X[y==0,0], X[y==0,1], color='red', marker='^', alpha=0.5)\nplt.scatter(X[y==1,0], X[y==1,1], color='blue', marker='o', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nax[0].scatter(X_pca[y==0,0], X_pca[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_pca[y==1,0], X_pca[y==1,1], color='blue', marker='o', alpha=0.5)\nax[1].scatter(X_pca[y==0,0], np.zeros((500,1))+0.02, color='red', marker='^', alpha=0.5)\nax[1].scatter(X_pca[y==1,0], np.zeros((500,1))-0.02, color='blue', marker='o', alpha=0.5)\nax[0].set_ylabel('PC2')\nax[0].set_xlabel('PC1')\nax[1].set_xlabel('PC1')\nax[1].set_ylim((-1,1))\nplt.show()\n\n\n\n\n\n\n\n\n\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\nX_kpca = kpca.fit_transform(X)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nax[0].scatter(X_kpca[y==0,0], X_kpca[y==0,1], color='red', marker='^', alpha=0.5)\nax[0].scatter(X_kpca[y==1,0], X_kpca[y==1,1], color='blue', marker='o', alpha=0.5)\nax[1].scatter(X_kpca[y==0,0], np.zeros((500,1))+0.02, color='red', marker='^', alpha=0.5)\nax[1].scatter(X_kpca[y==1,0], np.zeros((500,1))-0.02, color='blue', marker='o', alpha=0.5)\nax[0].set_ylabel('PC2')\nax[0].set_xlabel('PC1')\nax[1].set_xlabel('PC1')\nax[1].set_ylim((-1,1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFor Further Exploration\nCheck out these resources to deepen your understanding of dimensionality reduction and related concepts:\n\nPCA Main ideas\nPCA Explained Step by Step\nPCA Practical Tips\nPCA Visually explained\nLDA Explained\nThe Kernel Trick\nPCA Step by Step Explanation (Reading)\n\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapter 7 Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron, 3rd edition"
  },
  {
    "objectID": "ML/2_1_intro.html#what-is-ml",
    "href": "ML/2_1_intro.html#what-is-ml",
    "title": "What is Machine Learning?",
    "section": "What is ML?",
    "text": "What is ML?\nMachine learning is the science (and art) of programming computers so they can learn from data.\nHere is a slightly more general definition:\n[Machine learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\n\nArthur Samuel, 1959"
  },
  {
    "objectID": "ML/2_1_intro.html#what-is-ml-1",
    "href": "ML/2_1_intro.html#what-is-ml-1",
    "title": "What is Machine Learning?",
    "section": "What is ML?",
    "text": "What is ML?\nML offers a new way to solve problems, answer complex questions, and create new content. ML can predict the weather, estimate travel times, recommend songs, auto-complete sentences, summarize articles, and generate never-seen-before images.\nIn basic terms, ML is the process of training a piece of software, called a model, to make useful predictions or generate content (like text, images, audio, or video) from data."
  },
  {
    "objectID": "ML/2_1_intro.html#what-is-ml-2",
    "href": "ML/2_1_intro.html#what-is-ml-2",
    "title": "What is Machine Learning?",
    "section": "What is ML?",
    "text": "What is ML?\nFor example, suppose we wanted to create an app to predict rainfall. We could use either a traditional approach or an ML approach. Using a traditional approach, we’d create a physics-based representation of the Earth’s atmosphere and surface, computing massive amounts of fluid dynamics equations. This is incredibly difficult."
  },
  {
    "objectID": "ML/2_1_intro.html#what-is-ml-3",
    "href": "ML/2_1_intro.html#what-is-ml-3",
    "title": "What is Machine Learning?",
    "section": "What is ML?",
    "text": "What is ML?\nUsing an ML approach, we would give an ML model enormous amounts of weather data until the ML model eventually learned the mathematical relationship between weather patterns that produce differing amounts of rain. We would then give the model the current weather data, and it would predict the amount of rain."
  },
  {
    "objectID": "ML/2_1_intro.html#why-use-ml",
    "href": "ML/2_1_intro.html#why-use-ml",
    "title": "What is Machine Learning?",
    "section": "Why use ML?",
    "text": "Why use ML?\n\nThe traditional approach requires explicitly programming a solution."
  },
  {
    "objectID": "ML/2_1_intro.html#why-use-ml-1",
    "href": "ML/2_1_intro.html#why-use-ml-1",
    "title": "What is Machine Learning?",
    "section": "Why use ML?",
    "text": "Why use ML?\n\nThe ML approach learns a solution from data."
  },
  {
    "objectID": "ML/2_1_intro.html#why-use-ml-2",
    "href": "ML/2_1_intro.html#why-use-ml-2",
    "title": "What is Machine Learning?",
    "section": "Why use ML?",
    "text": "Why use ML?\n\nAutomatically adapting to change."
  },
  {
    "objectID": "ML/2_1_intro.html#why-use-ml-3",
    "href": "ML/2_1_intro.html#why-use-ml-3",
    "title": "What is Machine Learning?",
    "section": "Why use ML?",
    "text": "Why use ML?\n\nMachine Learning can help humans learn"
  },
  {
    "objectID": "ML/2_1_intro.html#video-explanation",
    "href": "ML/2_1_intro.html#video-explanation",
    "title": "What is Machine Learning?",
    "section": "Video Explanation",
    "text": "Video Explanation"
  },
  {
    "objectID": "ML/2_1_intro.html#types-of-ml-systems",
    "href": "ML/2_1_intro.html#types-of-ml-systems",
    "title": "What is Machine Learning?",
    "section": "Types of ML Systems",
    "text": "Types of ML Systems\nML systems fall into one or more of the following categories based on how they learn to make predictions or generate content:\n\nSupervised learning\nUnsupervised learning\nReinforcement learning\nGenerative AI"
  },
  {
    "objectID": "ML/2_1_intro.html#supervised-learning",
    "href": "ML/2_1_intro.html#supervised-learning",
    "title": "What is Machine Learning?",
    "section": "Supervised Learning",
    "text": "Supervised Learning\nSupervised learning models can make predictions after seeing lots of data with the correct answers and then discovering the connections between the elements in the data that produce the correct answers. This is like a student learning new material by studying old exams that contain both questions and answers. Once the student has trained on enough old exams, the student is well prepared to take a new exam. These ML systems are “supervised” in the sense that a human gives the ML system data with the known correct results.\nTwo of the most common use cases for supervised learning are regression and classification."
  },
  {
    "objectID": "ML/2_1_intro.html#supervised-learning-1",
    "href": "ML/2_1_intro.html#supervised-learning-1",
    "title": "What is Machine Learning?",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nA labeled training set for spam classification (an example of supervised learning)"
  },
  {
    "objectID": "ML/2_1_intro.html#regression",
    "href": "ML/2_1_intro.html#regression",
    "title": "What is Machine Learning?",
    "section": "Regression",
    "text": "Regression\nA regression model predicts a numeric value.\n\nA regression problem: predict a value, given an input feature (there are usually multiple input features, and sometimes multiple output values)"
  },
  {
    "objectID": "ML/2_1_intro.html#regression-1",
    "href": "ML/2_1_intro.html#regression-1",
    "title": "What is Machine Learning?",
    "section": "Regression",
    "text": "Regression\n\n\n\n\n\n\n\n\nScenario\nPossible Input Data\nNumeric Prediction\n\n\n\n\nFuture ride time\nHistorical traffic conditions (gathered from smartphones, traffic sensors, ride-hailing and other navigation applications), distance from destination, and weather conditions.\nThe time in minutes and seconds to arrive at a destination"
  },
  {
    "objectID": "ML/2_1_intro.html#regression-2",
    "href": "ML/2_1_intro.html#regression-2",
    "title": "What is Machine Learning?",
    "section": "Regression",
    "text": "Regression\n\n\n\n\n\n\n\n\nScenario\nPossible Input Data\nNumeric Prediction\n\n\n\n\nFuture house price\nSquare footage, zip code, number of bedrooms and bathrooms, lot size, mortgage interest rate, property tax rate, construction costs, and number of homes for sale in the area\nThe price of the home"
  },
  {
    "objectID": "ML/2_1_intro.html#classification",
    "href": "ML/2_1_intro.html#classification",
    "title": "What is Machine Learning?",
    "section": "Classification",
    "text": "Classification\nClassification models predict the likelihood that something belongs to a category. Unlike regression models, whose output is a number, classification models output a value that states whether or not something belongs to a particular category. For example, classification models are used to predict if an email is spam or if a photo contains a cat."
  },
  {
    "objectID": "ML/2_1_intro.html#classification-1",
    "href": "ML/2_1_intro.html#classification-1",
    "title": "What is Machine Learning?",
    "section": "Classification",
    "text": "Classification\nClassification models are divided into two groups: binary classification and multiclass classification. Binary classification models output a value from a class that contains only two values, for example, a model that outputs either rain or no rain. Multiclass classification models output a value from a class that contains more than two values, for example, a model that can output either rain, hail, snow, or sleet."
  },
  {
    "objectID": "ML/2_1_intro.html#supervised-machine-learning",
    "href": "ML/2_1_intro.html#supervised-machine-learning",
    "title": "What is Machine Learning?",
    "section": "Supervised Machine Learning",
    "text": "Supervised Machine Learning"
  },
  {
    "objectID": "ML/2_1_intro.html#unsupervised-learning",
    "href": "ML/2_1_intro.html#unsupervised-learning",
    "title": "What is Machine Learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nUnsupervised learning models make predictions by being given data that does not contain any correct answers. An unsupervised learning model’s goal is to identify meaningful patterns among the data. In other words, the model has no hints on how to categorize each piece of data, but instead it must infer its own rules.\nA commonly used unsupervised learning model employs a technique called clustering. The model finds data points that demarcate natural groupings."
  },
  {
    "objectID": "ML/2_1_intro.html#unsupervised-learning-1",
    "href": "ML/2_1_intro.html#unsupervised-learning-1",
    "title": "What is Machine Learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nFigure 1. An ML model clustering similar data point"
  },
  {
    "objectID": "ML/2_1_intro.html#unsupervised-learning-2",
    "href": "ML/2_1_intro.html#unsupervised-learning-2",
    "title": "What is Machine Learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nFigure 2. Groups of clusters with natural demarcations"
  },
  {
    "objectID": "ML/2_1_intro.html#unsupervised-learning-3",
    "href": "ML/2_1_intro.html#unsupervised-learning-3",
    "title": "What is Machine Learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\nClustering differs from classification because the categories aren’t defined by you. For example, an unsupervised model might cluster a weather dataset based on temperature, revealing segmentations that define the seasons. You might then attempt to name those clusters based on your understanding of the dataset."
  },
  {
    "objectID": "ML/2_1_intro.html#unsupervised-learning-4",
    "href": "ML/2_1_intro.html#unsupervised-learning-4",
    "title": "What is Machine Learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nFigure 3. An ML model clustering similar weather patterns."
  },
  {
    "objectID": "ML/2_1_intro.html#unsupervised-learning-5",
    "href": "ML/2_1_intro.html#unsupervised-learning-5",
    "title": "What is Machine Learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nFigure 4. Clusters of weather patterns labeled as snow, sleet, rain, and no rain."
  },
  {
    "objectID": "ML/2_1_intro.html#reinforcement-learning",
    "href": "ML/2_1_intro.html#reinforcement-learning",
    "title": "What is Machine Learning?",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\nReinforcement learning models make predictions by getting rewards or penalties based on actions performed within an environment. A reinforcement learning system generates a policy that defines the best strategy for getting the most rewards.\nReinforcement learning is used to train robots to perform tasks, like walking around a room, and software programs like AlphaGo to play the game of Go."
  },
  {
    "objectID": "ML/2_1_intro.html#reinforcement-learning-1",
    "href": "ML/2_1_intro.html#reinforcement-learning-1",
    "title": "What is Machine Learning?",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\nA robot learning to walk through reinforcement learning."
  },
  {
    "objectID": "ML/2_1_intro.html#reinforcement-learning-2",
    "href": "ML/2_1_intro.html#reinforcement-learning-2",
    "title": "What is Machine Learning?",
    "section": "Reinforcement Learning",
    "text": "Reinforcement Learning\n\n\nVideo\nAtari breakout game where an ML model is playing the game."
  },
  {
    "objectID": "ML/2_1_intro.html#generative-ai",
    "href": "ML/2_1_intro.html#generative-ai",
    "title": "What is Machine Learning?",
    "section": "Generative AI",
    "text": "Generative AI\nGenerative AI is a class of models that creates content from user input. For example, generative AI can create unique images, music compositions, and jokes; it can summarize articles, explain how to perform a task, or edit a photo.\nGenerative AI can take a variety of inputs and create a variety of outputs, like text, images, audio, and video. It can also take and create combinations of these. For example, a model can take an image as input and create an image and text as output, or take an image and text as input and create a video as output."
  },
  {
    "objectID": "ML/2_1_intro.html#generative-ai-1",
    "href": "ML/2_1_intro.html#generative-ai-1",
    "title": "What is Machine Learning?",
    "section": "Generative AI",
    "text": "Generative AI\nWe can discuss generative models by their inputs and outputs, typically written as “type of input”-to-“type of output.” For example, the following is a partial list of some inputs and outputs for generative models:\n\nText-to-text\nText-to-image\nText-to-video\nText-to-code\nText-to-speech\nImage and text-to-image"
  },
  {
    "objectID": "ML/2_1_intro.html#text-to-text",
    "href": "ML/2_1_intro.html#text-to-text",
    "title": "What is Machine Learning?",
    "section": "Text-to-text",
    "text": "Text-to-text\n\n\n\n\n\n\n\nInput\nExample Output\n\n\n\n\nWho invented the Le Mans race?\nThe 24 Hours of Le Mans was founded by the Automobile Club de l’Ouest (ACO), a motoring organization founded in 1906. The first race was held on 26 and 27 May 1923 on the Sarthe Circuit, a 13.426-kilometre (8.345-mile) road circuit located near the town of Le Mans in France."
  },
  {
    "objectID": "ML/2_1_intro.html#text-to-image",
    "href": "ML/2_1_intro.html#text-to-image",
    "title": "What is Machine Learning?",
    "section": "Text-to-image",
    "text": "Text-to-image\n\n\n\n\n\n\n\nInput\nExample Output\n\n\n\n\nAn alien octopus floats through a portal while reading a newspaper"
  },
  {
    "objectID": "ML/2_1_intro.html#text-to-video",
    "href": "ML/2_1_intro.html#text-to-video",
    "title": "What is Machine Learning?",
    "section": "Text-to-video",
    "text": "Text-to-video\n\n\n\n\n\n\n\nInput\nExample Output\n\n\n\n\nA photorealistic teddy bear is swimming in the ocean at San Francisco. The teddy bear goes under water. The teddy bear keeps swimming under the water with colorful fishes. A panda bear is swimming under water."
  },
  {
    "objectID": "ML/2_1_intro.html#text-to-code",
    "href": "ML/2_1_intro.html#text-to-code",
    "title": "What is Machine Learning?",
    "section": "Text-to-code",
    "text": "Text-to-code\n\n\n\n\n\n\n\nInput\nExample Output\n\n\n\n\nWrite a Python loop that loops over a list of numbers and prints the prime numbers.\n\n\n\n\nfor number in numbers:\n  # Check if the number is prime.\n  is_prime = True\n  for i in range(2, number):\n    if number % i == 0:\n        is_prime = False\n        break\n  # If the number is prime, print it.\n  if is_prime:\n    print(number)"
  },
  {
    "objectID": "ML/2_1_intro.html#image-to-text",
    "href": "ML/2_1_intro.html#image-to-text",
    "title": "What is Machine Learning?",
    "section": "Image-to-text",
    "text": "Image-to-text\n\n\n\n\n\n\n\nInput\nExample Output\n\n\n\n\n\nThis is a flamingo. They are found in the Caribbean."
  },
  {
    "objectID": "ML/2_1_intro.html#generative-ai-2",
    "href": "ML/2_1_intro.html#generative-ai-2",
    "title": "What is Machine Learning?",
    "section": "Generative AI",
    "text": "Generative AI\nHow does generative AI work? At a high-level, generative models learn patterns in data with the goal to produce new but similar data. Generative models are like the following:\n\nComedians who learn to imitate others by observing people’s behaviors and style of speaking\nArtists who learn to paint in a particular style by studying lots of paintings in that style\nCover bands that learn to sound like a specific music group by listening to lots of music by that group"
  },
  {
    "objectID": "ML/2_1_intro.html#generative-ai-3",
    "href": "ML/2_1_intro.html#generative-ai-3",
    "title": "What is Machine Learning?",
    "section": "Generative AI",
    "text": "Generative AI\nTo produce unique and creative outputs, generative models are initially trained using an unsupervised approach, where the model learns to mimic the data it’s trained on. The model is sometimes trained further using supervised or reinforcement learning on specific data related to tasks the model might be asked to perform, for example, summarize an article or edit a photo.\nGenerative AI is a quickly evolving technology with new use cases constantly being discovered. For example, generative models are helping businesses refine their ecommerce product images by automatically removing distracting backgrounds or improving the quality of low-resolution images."
  },
  {
    "objectID": "ML/2_3_2_classification_algorithms.html",
    "href": "ML/2_3_2_classification_algorithms.html",
    "title": "2. Machine Learning",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\nimport seaborn as sns"
  },
  {
    "objectID": "ML/2_3_2_classification_algorithms.html#classification",
    "href": "ML/2_3_2_classification_algorithms.html#classification",
    "title": "2. Machine Learning",
    "section": "2.3 Classification",
    "text": "2.3 Classification\n\n2.3.8 K-Nearest Neighbors (KNN)\nThe k-nearest neighbors (KNN) classifier is one of the simplest yet most commonly used classifiers in supervised machine learning. KNN is often considered a lazy learner; it doesn’t technically train a model to make predictions. Instead an observation is predicted to be the same class as that of the largest proportion of the k nearest observations. For example, if an observation with an unknown class is surrounded by an observation of class 1, then the observation is classified as class 1. In this chapter we will explore how to use scikit-learn to create and use a KNN classifier.\n\n2.3.8.1 Finding an Observation’s Nearest Neighbors\nUse scikit-learn’s NearestNeighbors\n\n# Load libraries\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nfeatures_standardized = standardizer.fit_transform(features)\n\n# Two nearest neighbors\nnearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n\n# Create an observation\nnew_observation = [ 1, 1, 1, 1]\n\n# Find distances and indices of the observation's nearest neighbors\ndistances, indices = nearest_neighbors.kneighbors([new_observation])\n\n# View the nearest neighbors\nfeatures_standardized[indices]\n\narray([[[1.03800476, 0.55861082, 1.10378283, 1.18556721],\n        [0.79566902, 0.32841405, 0.76275827, 1.05393502]]])\n\n\nWe used the dataset of iris flowers. We created an observation, new_observation, with some values and then found the two observations that are closest to our observation. indices contains the locations of the observations in our dataset that are closest, so X[indices] displays the values of those observations. Intuitively, distance can be thought of as a measure of similarity, so the two closest observations are the two flowers most similar to the flower we created.\nHow do we measure distance? scikit-learn offers a wide variety of distance metrics, \\(d\\), including Euclidean:\n\\[d_{\\text{Euclidean}} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\\]\nand Manhattan:\n\\[d_{\\text{Manhattan}} = \\sum_{i=1}^{n} |x_i - y_i|\\]\nBy default, NearestNeighbors uses Minkowski distance:\n\\[d_{\\text{Minkowski}}(x, y) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{1/p}\\]\nwhere \\(x_i\\) and \\(y_i\\) are the two observations we are calculating the distance between. Minkowski includes a hyperparameter, \\(p\\), where \\(p=1\\) gives Manhattan distance and \\(p=2\\) gives Euclidean distance. By default, NearestNeighbors uses Minkowski distance with \\(p=2\\).\nWe can set the distance metric using the metric parameter:\n# Find two nearest neighbors based on Euclidean distance\nnearestneighbors_euclidean = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(features_standardized)\nThe distance variable we created contains the actual distance measurement to each of the two nearest neighbors:\n\ndistances\n\narray([[0.49140089, 0.74294782]])\n\n\nIn addition, we can use kneighbors_graph to create a matrix indicating each observation’s nearest neighbors:\n\n# Find each observation's three nearest neighbors\n# based on Euclidean distance (including itself)\nnearestneighbors_euclidean = NearestNeighbors(\n    n_neighbors=3, metric=\"euclidean\").fit(features_standardized)\n\n# List of lists indicating each observation's three nearest neighbors\n# (including itself)\nnearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(\n    features_standardized).toarray()\n\n# Remove 1s marking an observation is a nearest neighbor to itself\nfor i, x in enumerate(nearest_neighbors_with_self):\n    x[i] = 0\n\n# View first observation's two nearest neighbors\nnearest_neighbors_with_self[0]\n\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n\nWhen we are finding nearest neighbors or using any learning algorithm based on distance, it is important to transform features so that they are on the same scale. This is because the distance metrics treat all features as if they were on the same scale, but if one feature is in millions of dollars and a second feature is in percentages, the distance calculated will be biased toward the former. In our solution we addressed this potential issue by standardizing the features using StandardScaler.\n\n\n2.3.8.2 Creating a KNN Classifier\nGiven an observation of unknown class, you need to predict its class based on the class of its neighbors. If the dataset is not very large, use KNeighborsClassifier:\n\n# Load libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# Load data\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Standardize features\nX_std = standardizer.fit_transform(X)\n\n# Use PCA to reduce to 2D for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_std)\n\n# Visualize the iris dataset in 2D\nfig, ax = plt.subplots(figsize=(10, 6))\nscatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=100, alpha=0.6, edgecolors='k')\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nax.set_title('Iris Dataset - 2D Projection using PCA')\ncbar = plt.colorbar(scatter, ax=ax)\ncbar.set_label('Iris Species')\ncbar.set_ticks([0, 1, 2])\ncbar.set_ticklabels(iris.target_names)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualize the effect of different k values on decision boundaries\nfig, axes = plt.subplots(2, 2, figsize=(14, 12))\nk_values = [1, 3, 5, 15]\n\nfor ax, k in zip(axes.flat, k_values):\n    # Create a mesh to plot decision boundaries\n    x_min, x_max = X_pca[:, 0].min() - 0.5, X_pca[:, 0].max() + 0.5\n    y_min, y_max = X_pca[:, 1].min() - 0.5, X_pca[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n                         np.linspace(y_min, y_max, 100))\n    \n    # Train KNN with current k value\n    knn_k = KNeighborsClassifier(n_neighbors=k).fit(X_pca, y)\n    \n    # Predict for the mesh\n    Z = knn_k.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    # Plot decision boundary and training points\n    ax.contourf(xx, yy, Z, alpha=0.4, cmap='viridis', levels=2)\n    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=100, alpha=0.8, edgecolors='k')\n    \n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n    ax.set_title(f'KNN Decision Boundary (k={k})')\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Effect of k on KNN Decision Boundaries', fontsize=16, y=1.00)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"- k=1: Very flexible, can overfit (high variance)\")\nprint(\"- k=3,5: Good balance between bias and variance\")\nprint(\"- k=15: Very smooth boundaries, may underfit (high bias)\")\n\n\n\n\n\n\n\n\n\nObservations:\n- k=1: Very flexible, can overfit (high variance)\n- k=3,5: Good balance between bias and variance\n- k=15: Very smooth boundaries, may underfit (high bias)\n\n\n\n# Train a KNN classifier with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_pca, y)\n\n# Create two observations (in PCA space for visualization)\nnew_observations_original = np.array([[ 5, 5, 0.5, 0.5], [ 1, 3, 3, 10]])\nnew_observations_std = standardizer.transform(new_observations_original)\nnew_observations = pca.transform(new_observations_std)\n\n# Predict the class of two observations\npredictions = knn.predict(new_observations)\n\n# Visualize predictions with new observations\nfig, ax = plt.subplots(figsize=(10, 6))\nscatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=100, alpha=0.6, edgecolors='k', label='Training data')\nax.scatter(new_observations[:, 0], new_observations[:, 1], c=predictions, cmap='viridis', s=300, marker='*', \n           edgecolors='red', linewidths=2, label='New observations')\nax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\nax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\nax.set_title('KNN Classification - Predicting Classes for New Observations (k=5)')\nax.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"New observations predictions: {[iris.target_names[p] for p in predictions]}\")\n\n\n\n\n\n\n\n\nNew observations predictions: [np.str_('setosa'), np.str_('virginica')]\n\n\nIn KNN, given an observation, \\(x_u\\), with an unknown target class, the algorithm first identifies the k closest observations (sometimes called \\(x_u\\)’s neighborhood) based on some distance metric (e.g., Euclidean distance), then these k observations “vote” based on their class, and the class that wins the vote is \\(x_u\\)’s predicted class. More formally, the probability \\(x_u\\) of some class \\(j\\) is:\n\\[ P(y=j|x_u) = \\frac{1}{k} \\sum_{i \\in N_k(x_u)} I(y_i = j) \\]\nwhere \\(N_k(x_u)\\) is the set of k nearest neighbors of \\(x_u\\), \\(y_i\\) is the class of observation \\(i\\), and \\(I\\) is an indicator function that is 1 if \\(y_i = j\\) and 0 otherwise. The predicted class for \\(x_u\\) is the class that maximizes this probability.\n\n# View probability that each observation is one of three classes\nknn.predict_proba(new_observations)\n\narray([[1., 0., 0.],\n       [0., 0., 1.]])\n\n\nThe class with the highest probability becomes the predicted class. For example, in the preceding output, the first observation should be class 0 \\((P = 1)\\) while the second observation should be class 2 \\((P = 1)\\), and this is just what we see:\n\nknn.predict(new_observations)\n\narray([0, 2])\n\n\nKNeighborsClassifier contains a number of important parameters to consider. First, metric sets the distance metric used. Second, n_jobs determines how many of the computer’s cores to use. Because making a prediction requires calculating the distance from a point to every single point in the data, using multiple cores is highly recommended. Third, algorithm sets the method used to calculate the nearest neighbors. While there are real differences in the algorithms, by default KNeighborsClassifier attempts to auto-select the best algorithm so you often don’t need to worry about this parameter. Fourth, by default KNeighborsClassifier works how we described previously, with each observation in the neighborhood getting one vote; however, if we set the weights parameter to distance, the closer observations’ votes are weighted more than observations farther away. Intuitively this make sense, since more similar neighbors might tell us more about an observation’s class than others.\nFinally, because distance calculations treat all features as if they are on the same scale, it is important to standardize the features prior to using a KNN classifier.\n\n\n2.3.8.3 Identifying the Best Neighborhood Size (k)\nUse model selection techniques like GridSearchCV:\n\n# Load libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\n\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create standardizer\nstandardizer = StandardScaler()\n\n# Create a KNN classifier\nknn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n\n# Create a pipeline\npipe = Pipeline([(\"standardizer\", standardizer), (\"knn\", knn)])\n\n# Create space of candidate values\nsearch_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n\n# Create grid search\nclassifier = GridSearchCV(pipe, search_space, cv=5, verbose=0).fit(features, target)\n\nThe size of \\(k\\) has real implications in KNN classifiers. In machine learning we are trying to find a balance between bias and variance, and in few places is that as explicit as the value of \\(k\\). If \\(k = n\\), where \\(n\\) is the number of observations, then we have high bias but low variance. If \\(k = 1\\), we will have low bias but high variance. The best model will come from finding the value of \\(k\\) that balances this bias-variance trade-off. In our solution, we used GridSearchCV to conduct five-fold cross-validation on KNN classifiers with different values of \\(k\\). When that is completed, we can see the k that produces the best model:\n\n# Best neighborhood size (k)\nclassifier.best_estimator_.get_params()[\"knn__n_neighbors\"]\n\n6\n\n\nNote: In KNN classification, an observation’s class is predicted from the classes of its \\(k\\) neighbors. A less common technique is classification in a radius-based nearest neighbor (RNN) classifier, where an observation’s class is predicted from the classes of all observations within a given radius \\(r\\).\nIn scikit-learn, RadiusNeighborsClassifier is very similar to KNeighborsClassifier, with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it’s best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius—which itself can be a useful tool for identifying outliers.\n# Train a radius neighbors classifier\nrnn = RadiusNeighborsClassifier(radius=.5, n_jobs=-1).fit(features_standardized, target)\n\n\n\n2.3.9 Bayesian Classification\nNaive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being useful as a quick-and-dirty baseline for a classification problem. This section will provide an intuitive explanation of how naive Bayes classifiers work, followed by a few examples of them in action on some datasets.\nNaive Bayes classifiers are built on Bayesian classification methods. These rely on Bayes’s theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities. In Bayesian classification, we’re interested in finding the probability of a label \\(y\\) given some observed features, which we can write as \\(P(y|\\text{features})\\). Bayes’s theorem tells us how to express this in terms of quantities we can compute more directly:\n\\[ P(y|x_1,...,x_j) = \\frac{P(x_1,...,x_j|y) \\cdot P(y)}{P(x_1,...,x_j)} \\]\nWhere:\n\n\\(P(y|x_1,...,x_j)\\) is called the posterior and is the probability that an observation is class \\(y\\) given the observation’s values for the \\(j\\) features, \\(x1, …, xj\\).\n\\(P(x_1,...,x_j|y)\\) is called the likelihood and is the likelihood of an observation’s values for features \\(x1, …, xj\\) given their class, \\(y\\).\n\\(P(y)\\) is called the prior and is our belief for the probability of class \\(y\\) before looking at the data.\n\\(P(x_1,...,x_j)\\) is called the marginal probability.\n\nIn naive Bayes, we compare an observation’s posterior values for each possible class. Specifically, because the marginal probability is constant across these comparisons, we compare the numerators of the posterior for each class. For each observation, the class with the greatest posterior numerator becomes the predicted class, \\(\\hat{y}\\).\nThere are two important things to note about naive Bayes classifiers. First, for each feature in the data, we have to assume the statistical distribution of the likelihood, \\(P(x_j∣y)\\). The common distributions are the normal (Gaussian), multinomial, and Bernoulli distributions. The distribution chosen is often determined by the nature of features (continuous, binary, etc.). Second, naive Bayes gets its name because we assume that each feature, and its resulting likelihood, is independent. This “naive” assumption is frequently wrong yet in practice does little to prevent building highquality classifiers.\nSuch a model is called a generative model because it specifies the hypothetical random process that generates the data.\n\n2.3.9.1 Gaussian Naive Bayes\nThe most common type of naive Bayes classifier is the Gaussian naive Bayes. In Gaussian naive Bayes, we assume that the likelihood of the feature values \\(x\\), given an observation is of class \\(y\\), follows a normal distribution:\n\\[ P(x_j|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left(-\\frac{(x_j - \\mu_y)^2}{2\\sigma_y^2}\\right) \\]\nWhere \\(\\mu_y\\) and \\(\\sigma_y^2\\) are the mean and variance of feature \\(x_j\\) for all observations in class \\(y\\). Because of the assumption of the normal distribution, Gaussian naive Bayes is best used in cases where all our features are continuous.\n\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');\n\n\n\n\n\n\n\n\nThe simplest Gaussian model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. This model can be fit by computing the mean and standard deviation of the points within each label, which is all we need to define such a distribution. The result of this naive Gaussian assumption is shown in the following figure, where we see the contours of the Gaussian distributions fit to each class in the two-dimensional dataset:\n\nfig, ax = plt.subplots()\n\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\nax.set_title('Naive Bayes Model', size=14)\n\nxlim = (-8, 8)\nylim = (-15, 5)\n\nxg = np.linspace(xlim[0], xlim[1], 60)\nyg = np.linspace(ylim[0], ylim[1], 40)\nxx, yy = np.meshgrid(xg, yg)\nXgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n\nfor label, color in enumerate(['red', 'blue']):\n    mask = (y == label)\n    mu, std = X[mask].mean(0), X[mask].std(0)\n    P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n    Pm = np.ma.masked_array(P, P &lt; 0.03)\n    ax.pcolormesh(xx, yy, Pm.reshape(xx.shape), alpha=0.5,\n                  cmap=color.title() + 's', shading='auto')\n    ax.contour(xx, yy, P.reshape(xx.shape),\n               levels=[0.01, 0.1, 0.5, 0.9],\n               colors=color, alpha=0.2)\n    \nax.set(xlim=xlim, ylim=ylim);\n\n\n\n\n\n\n\n\nThe ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses. With this generative model in place for each class, we have a simple recipe to compute the likelihood \\(P(\\text{features}|y)\\) for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point.\nThis procedure is implemented in Scikit-Learn’s sklearn.naive_bayes.GaussianNB estimator:\n\nfrom sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y)\n\nGaussianNB()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNB?Documentation for GaussianNBiFitted\n        \n            \n                Parameters\n                \n\n\n\n\npriors \nNone\n\n\n\nvar_smoothing \n1e-09\n\n\n\n\n            \n        \n    \n\n\n\nrng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)\n\nNow we can plot this new data to get an idea of where the decision boundary is (see the following figure):\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\nlim = plt.axis()\nplt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\nplt.axis(lim);\n\n\n\n\n\n\n\n\nWe see a slightly curved boundary in the classifications—in general, the boundary produced by a Gaussian naive Bayes model will be quadratic.\nA nice aspect of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the predict_proba method:\n\nyprob = model.predict_proba(Xnew)\nyprob[-8:].round(2)\n\narray([[0.89, 0.11],\n       [1.  , 0.  ],\n       [1.  , 0.  ],\n       [1.  , 0.  ],\n       [1.  , 0.  ],\n       [1.  , 0.  ],\n       [0.  , 1.  ],\n       [0.15, 0.85]])\n\n\nThe columns give the posterior probabilities of the first and second labels, respectively. If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a good place to start.\nOf course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results. Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a reliable method.\n\n\n2.3.9.2 Multinomial Naive Bayes\nThe Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label. Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution. The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\nThe idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model it with a best-fit multinomial distribution.\nExample: Classifying Text\n\nfrom sklearn.datasets import fetch_20newsgroups\n\ndata = fetch_20newsgroups()\ndata.target_names\n\n['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']\n\n\nFor simplicity here, we will select just a few of these categories and download the training and testing sets:\n\ncategories = ['talk.religion.misc', 'soc.religion.christian',\n              'sci.space', 'comp.graphics']\ntrain = fetch_20newsgroups(subset='train', categories=categories)\ntest = fetch_20newsgroups(subset='test', categories=categories)\n\nHere is a representative entry from the data:\n\nprint(train.data[5][48:])\n\nSubject: Federal Hearing\nOriginator: dmcgee@uluhe\nOrganization: School of Ocean and Earth Science and Technology\nDistribution: usa\nLines: 10\n\n\nFact or rumor....?  Madalyn Murray O'Hare an atheist who eliminated the\nuse of the bible reading and prayer in public schools 15 years ago is now\ngoing to appear before the FCC with a petition to stop the reading of the\nGospel on the airways of America.  And she is also campaigning to remove\nChristmas programs, songs, etc from the public schools.  If it is true\nthen mail to Federal Communications Commission 1919 H Street Washington DC\n20054 expressing your opposition to her request.  Reference Petition number\n\n2493.\n\n\n\nIn order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers. For this we will use the TF-IDF vectorizer, and create a pipeline that attaches it to a multinomial naive Bayes classifier:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(TfidfVectorizer(), MultinomialNB())\n\nWith this pipeline, we can apply the model to the training data and predict labels for the test data:\n\nmodel.fit(train.data, train.target)\nlabels = model.predict(test.data)\n\nNow that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, let’s take a look at the confusion matrix between the true and predicted labels for the test data (see the following figure):\n\nfrom sklearn.metrics import confusion_matrix\nmat = confusion_matrix(test.target, labels)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=train.target_names, yticklabels=train.target_names,\n            cmap='Blues')\nplt.ylabel('true label')\nplt.xlabel('predicted label');\n\n\n\n\n\n\n\n\nEvidently, even this very simple classifier can successfully separate space discussions from computer discussions, but it gets confused between discussions about religion and discussions about Christianity. This is perhaps to be expected!\nThe cool thing here is that we now have the tools to determine the category for any string, using the predict method of this pipeline. Here’s a utility function that will return the prediction for a single string:\n\ndef predict_category(s, train=train, model=model):\n    pred = model.predict([s])\n    return train.target_names[pred[0]]\n\nLet’s try it out:\n\npredict_category('sending a payload to the ISS')\n\n'sci.space'\n\n\n\npredict_category('discussing the existence of God')\n\n'soc.religion.christian'\n\n\n\npredict_category('determining the screen resolution')\n\n'comp.graphics'\n\n\nRemember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking. Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective.\n\n\n2.3.9.3 When to Use Naive Bayes Classifiers\nBecause naive Bayes classifiers make such stringent assumptions about data, they will generally not perform as well as more complicated models. That said, they have several advantages:\n\nThey are fast for both training and prediction.\nThey provide straightforward probabilistic prediction.\nThey are often easily interpretable.\nThey have few (if any) tunable parameters.\n\nThese advantages mean a naive Bayes classifier is often a good choice as an initial baseline classification. If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem. If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\nNaive Bayes classifiers tend to perform especially well in the following situations:\n\nWhen the naive assumptions actually match the data (very rare in practice)\nFor very well-separated categories, when model complexity is less important\nFor very high-dimensional data, when model complexity is less important\n\nThe last two points seem distinct, but they actually are related: as the dimensionality of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like the ones discussed here tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful.\n\n\n\n2.3.10 Support Vector Machines (SVM)\nSupport vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will explore the intuition behind SVMs and their use in classification problems.\nThey technically perform binary classification only, but Scikit-Learn enables them to do multiclass classification as well using techniques discussed in previous sections.\nScikit-Learn makes building SVMs easy with classes such as SVC (short for support vector classifier) for classification models and SVR (support vector regressor) for regression models. You can use these classes without understanding how SVMs work, but you’ll get more out of them if you do understand how they work. It’s also important to know how to tune SVMs for individual datasets and how to prepare data before you train a model. Toward the end of this chapter, we’ll build an SVM that performs facial recognition. But first, let’s look behind the scenes and discover why SVMs are often the go-to mechanism for modeling real-world datasets.\n\n2.3.10.1 How SVMs Work?\nAs part of our discussion of Bayesian classification, we learned about a simple kind of model that describes the distribution of each underlying class, and experimented with using it to probabilistically determine labels for new points. That was an example of generative classification; here we will consider instead discriminative classification. That is, rather than modeling each class, we will simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.\nAs an example of this, consider the simple case of a classification task in which the two classes of points are well separated (see the following figure):\n\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n\n\n\n\n\n\n\n\nA linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two-dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\nWe can draw some of them as follows; the following figure shows the result:\n\nxfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n\nfor m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n    plt.plot(xfit, m * xfit + b, '-k')\n\nplt.xlim(-1, 3.5);\n\n\n\n\n\n\n\n\nThese are three very different separators which, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the “X” in this plot) will be assigned a different label! Evidently our simple intuition of “drawing a line between classes” is not good enough, and we need to think a bit more deeply.\nMaximizing the margin\nSupport vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest point. Here is an example of how this might look (see the following figure):\n\nxfit = np.linspace(-1, 3.5)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n\nfor m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n    yfit = m * xfit + b\n    plt.plot(xfit, yfit, '-k')\n    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n                     color='lightgray', alpha=0.5)\n\nplt.xlim(-1, 3.5);\n\n\n\n\n\n\n\n\nThe line that maximizes this margin is the one we will choose as the optimal model.\n\n\n2.3.10.2 Fitting an SVM Model with Scikit-Learn\nLet’s see the result of an actual fit to this data: we will use Scikit-Learn’s support vector classifier (SVC) to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number (we’ll discuss the meaning of these in more depth momentarily):\n\nfrom sklearn.svm import SVC # \"Support vector classifier\"\nmodel = SVC(kernel='linear', C=1E10)\nmodel.fit(X, y)\n\nSVC(C=10000000000.0, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC \n10000000000.0\n\n\n\nkernel \n'linear'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\nTo better visualize what’s happening here, let’s create a quick convenience function that will plot SVM decision boundaries for us (see the following figure):\n\ndef plot_svc_decision_function(model, ax=None, plot_support=True):\n    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n    if ax is None:\n        ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # create grid to evaluate model\n    x = np.linspace(xlim[0], xlim[1], 30)\n    y = np.linspace(ylim[0], ylim[1], 30)\n    Y, X = np.meshgrid(y, x)\n    xy = np.vstack([X.ravel(), Y.ravel()]).T\n    P = model.decision_function(xy).reshape(X.shape)\n    \n    # plot decision boundary and margins\n    ax.contour(X, Y, P, colors='k',\n               levels=[-1, 0, 1], alpha=0.5,\n               linestyles=['--', '-', '--'])\n    \n    # plot support vectors\n    if plot_support:\n        ax.scatter(model.support_vectors_[:, 0],\n                   model.support_vectors_[:, 1],\n                   s=300, linewidth=1, edgecolors='black',\n                   facecolors='none');\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(model);\n\n\n\n\n\n\n\n\nThis is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin: they are circled in the following figure. These points are the pivotal elements of this fit; they are known as the support vectors, and give the algorithm its name. In Scikit-Learn, the identities of these points are stored in the support_vectors_ attribute of the classifier:\n\nmodel.support_vectors_\n\narray([[0.44359863, 3.11530945],\n       [2.33812285, 3.43116792],\n       [2.06156753, 1.96918596]])\n\n\nA key to this classifier’s success is that for the fit, only the positions of the support vectors matter; any points further from the margin that are on the correct side do not modify the fit. Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\nWe can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset (see the following figure):\n\ndef plot_svm(N=10, ax=None):\n    X, y = make_blobs(n_samples=200, centers=2,\n                      random_state=0, cluster_std=0.60)\n    X = X[:N]\n    y = y[:N]\n    model = SVC(kernel='linear', C=1E10)\n    model.fit(X, y)\n    \n    ax = ax or plt.gca()\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n    ax.set_xlim(-1, 4)\n    ax.set_ylim(-1, 6)\n    plot_svc_decision_function(model, ax)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\nfor axi, N in zip(ax, [60, 120]):\n    plot_svm(N, axi)\n    axi.set_title('N = {0}'.format(N))\n\n\n\n\n\n\n\n\nIn the left panel, we see the model and the support vectors for 60 training points. In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors in the left panel are the same as the support vectors in the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model.\npip install ipywidgets\n\nfrom ipywidgets import interact, Output\n\nout = Output()\n\ndef plot_svm_interactive(N=10):\n    with out:\n        out.clear_output(wait=True)\n        _, ax = plt.subplots(figsize=(8, 6))\n        plot_svm(N, ax)\n        plt.show()\n\ninteract(plot_svm_interactive, N=(10, 200, 10))\nout\n\n\n\n\n\n\n\n\n\n2.3.10.3 Kernel SVM\nWhere SVM can become quite powerful is when it is combined with kernels. We have seen a version of kernels before. There we projected our data into a higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to reduce dimensionality and fit for nonlinear relationships with a linear classifier.\nIn SVM models, we can use a version of the same idea. To motivate the need for kernels, let’s look at some data that is not linearly separable\n\nfrom sklearn.datasets import make_circles\nX, y = make_circles(100, factor=.1, noise=.1)\n\nclf = SVC(kernel='linear').fit(X, y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf, plot_support=False);\n\n\n\n\n\n\n\n\nIt is clear that no linear discrimination will ever be able to separate this data. But we can draw a lesson from the radial basis function kernel in Introduction to Data Science: Dimensionality Reduction, and think about how we might project the data into a higher dimension such that a linear separator would be sufficient. For example, one simple projection we could use would be to compute a radial basis function (RBF) centered on the middle clump:\n\nr = np.exp(-(X ** 2).sum(1))\n\nWe can visualize this extra data dimension using a three-dimensional plot, as seen in the following figure:\n\nfrom mpl_toolkits import mplot3d\n\nax = plt.subplot(projection='3d')\nax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\nax.view_init(elev=20, azim=30)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('r');\n\n\n\n\n\n\n\n\nWe can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, \\(r=0.7\\).\nIn this case we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\nOne strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation, as it is based on a similarity relationship (or kernel) between each pair of points.\nA potential problem with this strategy—projecting \\(N\\) points into \\(N\\) dimensions—is that it might become very computationally intensive as \\(N\\) grows large. However, because of a neat little procedure known as the kernel trick, a fit on kernel-transformed data can be done implicitly—that is, without ever building the full \\(N\\)-dimensional representation of the kernel projection. This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\nIn Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF kernel, using the kernel model hyperparameter:\n\nclf = SVC(kernel='rbf', C=1E6)\nclf.fit(X, y)\n\nSVC(C=1000000.0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC \n1000000.0\n\n\n\nkernel \n'rbf'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\nLet’s use our previously defined function to visualize the fit and identify the support vectors (see the following figure):\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\nplot_svc_decision_function(clf)\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n            s=300, lw=1, facecolors='none');\n\n\n\n\n\n\n\n\nUsing this kernelized support vector machine, we learn a suitable nonlinear decision boundary. This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used.\n\n\n2.3.10.4 Tuning the SVM Model: Softening Margins with C\nOur discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists. But what if your data has some amount of overlap? For example, you may have data like this (see the following figure):\n\nX, y = make_blobs(n_samples=100, centers=2,\n                  random_state=0, cluster_std=1.2)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');\n\n\n\n\n\n\n\n\nTo handle this case, the SVM implementation has a bit of a fudge factor that “softens” the margin: that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C. For a very large C, the margin is hard, and points cannot lie in it. For a smaller C, the margin is softer and can grow to encompass some points.\nThe plot shown in the following figure gives a visual picture of how a changing C affects the final fit via the softening of the margin:\n\nX, y = make_blobs(n_samples=100, centers=2,\n                  random_state=0, cluster_std=0.8)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6))\nfig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n\nfor axi, C in zip(ax, [10.0, 0.1]):\n    model = SVC(kernel='linear', C=C).fit(X, y)\n    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n    plot_svc_decision_function(model, axi)\n    axi.scatter(model.support_vectors_[:, 0],\n                model.support_vectors_[:, 1],\n                s=300, lw=1, facecolors='none');\n    axi.set_title('C = {0:.1f}'.format(C), size=14)\n\n\n\n\n\n\n\n\nIn other words, the C parameter controls how aggressively the model fits to the training data. The higher the value, the tighter the fit and the higher the risk of overfitting. The default is C=1 in Scikit, but you can specify a different value to adjust the fit.\nIt’s difficult to know what the right value of C is (that is, the value that provides the best balance between underfitting and overfitting the training data and yields the best results when the model is run with test data).\nFor the RBF and polynomial kernels, there’s another value called gamma that affects accuracy. And for polynomial kernels, the degree parameter impacts the model’s ability to learn from the training data.\nAs we’ve seen earlier, to aid in the process of hyperparameter tuning, Scikit provides a family of optimizers that includes GridSearchCV, which tries all combinations of a specified set of parameter values with built-in cross-validation to determine which combination produces the most accurate model. These optimizers prevent you from having to write code to do a brute-force search using all the unique combinations of parameter values. To be clear, they do brute-force searches themselves by training the model multiple times, each time with a different combination of values. At the end, you can retrieve the most accurate model from the best_estimator_ attribute, the parameter values that produced the most accurate model from the best_params_ attribute, and the best score from the best_score_ attribute.\nHere’s an example that uses Scikit’s SVC class to implement an SVM classifier. This uses the RBF kernel with C=1. You can specify the kernel type and values for C and gamma this way:\nfrom sklearn.svm import SVC\nmodel = SVC(kernel='rbf', C=1, gamma=0.1)\nmodel.fit(X_train, y_train)\nSuppose you wanted to try two different kernels and five values each for C and gamma to see which combination produces the best results. Rather than write a nested for loop, you could do this:\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = SVC()\nparam_grid = {\n    'kernel': ['rbf', 'poly'],\n    'C': [0.01, 0.1, 1, 10, 100],\n    'gamma': [0.01, 0.25, 0.5, 0.75, 1.0]\n}\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, verbose=2)\ngrid.fit(X_train, y_train)  # Train the model with different parameter combinations\nThe call to fit won’t return for a while. It trains the model 250 times since there are 50 different combinations of kernel, C, and gamma, and cv=5 says to use fivefold cross-validation to assess the results. Once training is complete, you retrieve the best model this way:\nbest_model  =  grid.best_estimator_\nIt is not uncommon to run a search regimen such as this one multiple times—the first time with course parameter values, and each time thereafter with narrower ranges of values centered on the values obtained from best_params_. More training time up front is the price you pay for an accurate model. To reiterate, you can almost always make an SVM more accurate by finding the optimum combination of parameters. And for better or worse, brute force is the most effective way to identify the best combination.\nNote:\nSVMs usually train better with data that is normalized to unit variance. That’s true even if the values in all the columns have similar ranges, but it’s especially true if they don’t have similar ranges. Scikit’s StandardScaler class applies unit variance to data. Unit variance is achieved by dividing the values in a column by the mean of all the values in the column and dividing by the standard deviation. Scikit’s make_pipeline function enables you to combine transformers such as StandardScaler and classifiers such as SVC into one logical unit to ensure that data passed to fit and predict undergoes the same transformations.\n\n\n\nUnnormalized data plotted with equally scaled axes\n\n\n\n\n\nData normalized with MinMaxScaler\n\n\n\n\n2.3.10.5 Face recognition Example\nAs an example of support vector machines in action, let’s take a look at the facial recognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn:\n\nfrom sklearn.datasets import fetch_lfw_people\nfaces = fetch_lfw_people(min_faces_per_person=60)\nprint(faces.target_names)\nprint(faces.images.shape)\n\n['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'\n 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair']\n(1348, 62, 47)\n\n\nLet’s plot a few of these faces to see what we’re working with (see the following figure):\n\nfig, ax = plt.subplots(3, 5, figsize=(8, 6))\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(faces.images[i], cmap='bone')\n    axi.set(xticks=[], yticks=[],\n            xlabel=faces.target_names[faces.target[i]])\n\n\n\n\n\n\n\n\nEach image contains \\(62 \\times 47\\), or around \\(3,000\\), pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use principal component analysis (see PCA in Dimensionality Reduction) to extract \\(150\\) fundamental components to feed into our support vector machine classifier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:\n\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n\npca = PCA(n_components=150, whiten=True,\n          svd_solver='randomized', random_state=42)\nsvc = SVC(kernel='rbf', class_weight='balanced')\nmodel = make_pipeline(pca, svc)\n\nFor the sake of testing our classifier output, we will split the data into a training set and a testing set:\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n                                                random_state=42)\n\nFinally, we can use grid search cross-validation to explore combinations of parameters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model:\n\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'svc__C': [1, 5, 10, 50],\n              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\ngrid = GridSearchCV(model, param_grid)\n\n%time grid.fit(Xtrain, ytrain)\nprint(grid.best_params_)\n\nCPU times: total: 1min 1s\nWall time: 22.2 s\n{'svc__C': 5, 'svc__gamma': 0.001}\n\n\nThe optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\nNow with this cross-validated model we can predict the labels for the test data, which the model has not yet seen:\n\nmodel = grid.best_estimator_\nyfit = model.predict(Xtest)\n\nLet’s take a look at a few of the test images along with their predicted values (see the following figure):\n\nfig, ax = plt.subplots(4, 6)\nfor i, axi in enumerate(ax.flat):\n    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n    axi.set(xticks=[], yticks=[])\n    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n                   color='black' if yfit[i] == ytest[i] else 'red')\nfig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);\n\n\n\n\n\n\n\n\nOut of this small sample, our optimal estimator mislabeled only two faces. We can get a better sense of our estimator’s performance using the classification report, which lists recovery statistics label by label:\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(ytest, yfit,\n                            target_names=faces.target_names))\n\n                   precision    recall  f1-score   support\n\n     Ariel Sharon       0.65      0.87      0.74        15\n     Colin Powell       0.83      0.88      0.86        68\n  Donald Rumsfeld       0.70      0.84      0.76        31\n    George W Bush       0.97      0.80      0.88       126\nGerhard Schroeder       0.76      0.83      0.79        23\n      Hugo Chavez       0.93      0.70      0.80        20\nJunichiro Koizumi       0.86      1.00      0.92        12\n       Tony Blair       0.82      0.98      0.89        42\n\n         accuracy                           0.85       337\n        macro avg       0.82      0.86      0.83       337\n     weighted avg       0.86      0.85      0.85       337\n\n\n\nWe might also display the confusion matrix between these classes (see the following figure):\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nmat = confusion_matrix(ytest, yfit)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d',\n            cbar=False, cmap='Blues',\n            xticklabels=faces.target_names,\n            yticklabels=faces.target_names)\nplt.xlabel('true label')\nplt.ylabel('predicted label');\n\n\n\n\n\n\n\n\nThis helps us get a sense of which labels are likely to be confused by the estimator.\nFor a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation. For this kind of application, one good option is to make use of OpenCV, which, among other things, includes pretrained implementations of state-of-the-art feature extraction tools for images in general and faces in particular.\n\n\n2.3.10.6 Summmary\nThis has been a brief intuitive introduction to the principles behind support vector machines. These models are a powerful classification method for a number of reasons:\n\nTheir dependence on relatively few support vectors means that they are compact and take up very little memory.\nOnce the model is trained, the prediction phase is very fast.\nBecause they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is challenging for other algorithms.\nTheir integration with kernel methods makes them very versatile, able to adapt to many types of data.\n\nHowever, SVMs have several disadvantages as well: * The scaling with the number of samples \\(N\\) is \\(O(N^3)\\) at worst, or \\(O(N^2)\\) for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive. * The results are strongly dependent on a suitable choice for the softening parameter C. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size. * The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the probability parameter of SVC), but this extra estimation is costly.\n\nReferences:\nDisclaimer: Some of the material in this notebook is adapted from other sources. These references are provided for further reading and to acknowledge the original authors.\n\nChapter 41, 43 Python Data Science Handbook, 2nd edition\nChapter 15, 18 Machine Learning with Python Cookbook, 2nd edition"
  }
]